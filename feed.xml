<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.0">Jekyll</generator><link href="https://mattcasmith.net/feed.xml" rel="self" type="application/atom+xml" /><link href="https://mattcasmith.net/" rel="alternate" type="text/html" /><updated>2021-05-01T20:26:46+01:00</updated><id>https://mattcasmith.net/feed.xml</id><title type="html">MattCASmith</title><subtitle>A blog about cyber security and technology</subtitle><entry><title type="html">Installing Splunk Free in a virtual machine for log analysis</title><link href="https://mattcasmith.net/2021/05/01/installing-splunk-free-virtual-machine-log-analysis" rel="alternate" type="text/html" title="Installing Splunk Free in a virtual machine for log analysis" /><published>2021-05-01T01:00:00+01:00</published><updated>2021-05-01T01:00:00+01:00</updated><id>https://mattcasmith.net/2021/05/01/running-splunk-virtual-machine</id><content type="html" xml:base="https://mattcasmith.net/2021/05/01/installing-splunk-free-virtual-machine-log-analysis">&lt;p&gt;Splunk is considered the gold standard for analysis of event logs and other data, but unless you’re lucky enough to work for an organisation that pays for it, it can be difficult to get practical experience in how to run searches, build dashboards, and otherwise dissect data using its query language.&lt;/p&gt;

&lt;p&gt;Recently, when I wanted to take a look at some &lt;a href=&quot;/2021/01/01/backutil-windows-backup-utility&quot;&gt;Backutil&lt;/a&gt; logs to see how recent development had affected performance, I stumbled across &lt;a href=&quot;https://docs.splunk.com/Documentation/Splunk/8.1.3/Admin/MoreaboutSplunkFree&quot; target=&quot;_blank&quot;&gt;Splunk Free&lt;/a&gt; - a limited version of the logging platform - and discovered just how easy it is to install (and how surprisingly well it runs) in a virtual machine. Here’s how to do it…&lt;/p&gt;

&lt;h3 id=&quot;setting-up-a-virtual-machine&quot;&gt;Setting up a virtual machine&lt;/h3&gt;

&lt;p&gt;For the purposes of this post, I’ll assume you’re familiar with the basics of setting up a virtual machine in VirtualBox or VMware. I won’t go over the process of creating the machine, but I will point out some key steps to take to make sure Splunk works and is accessible from your host system.&lt;/p&gt;

&lt;p&gt;First, choose your Linux distro. I went for &lt;a href=&quot;https://www.centos.org/&quot; target=&quot;_blank&quot;&gt;CentOS&lt;/a&gt;, which is usually a safe bet to run a stable server with less of the fluff that comes with some other versions, with 8GB of RAM. Like most modern operating systems, installation is as easy as booting from the ISO and following the on-screen instructions.&lt;/p&gt;

&lt;p&gt;One extra (but critically important) step: You’ll want to check the virtual machine’s network connection is set to Bridged Adapter. When you boot it up, configure an IP address on the same subnet as your host machine. With these settings, the guest system will behave as though it is a completely separate machine on the network as your computer. This way, once we’ve made some firewall changes, we’ll be able to access Splunk in our browser at the virtual machine’s IP address.&lt;/p&gt;

&lt;h3 id=&quot;downloading-and-installing-splunk&quot;&gt;Downloading and installing Splunk&lt;/h3&gt;

&lt;p&gt;With our virtual machine set up, it’s time to install Splunk. Download Splunk Free for Linux (you can find it by clicking Free Splunk in the top right corner of &lt;a href=&quot;https://www.splunk.com/&quot; target=&quot;_blank&quot;&gt;the Splunk homepage&lt;/a&gt; and selecting Software Download). Either do this in your virtual machine, or do it on your host system and copy the file to your Linux machine.&lt;/p&gt;

&lt;p&gt;To install Splunk, we need to extract &lt;code&gt;splunk-x.x.x-x-Linux-x86_64.tgz&lt;/code&gt; to the &lt;code&gt;/opt&lt;/code&gt; directory. The easiest way to do this is to copy it to the correct directory and run the following command.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tar&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xvf&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;splunk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;8.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;63079&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c59e632&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linux&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x86_64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tgz&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You should now have a &lt;code&gt;/opt/splunk&lt;/code&gt; folder containing the archive’s contents, which are essentially a fully-fledged Splunk installation. There’s just one more command to run to get Splunk up and running.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;bin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splunk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accept&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;license&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The first time you run this file, it will launch the Splunk installer, which will give you an opportunity to set an administrator username and password before your instance is live. At this point, you should be able to access Splunk at &lt;code&gt;127.0.0.1:8000&lt;/code&gt; in your virtual machine’s web browser.&lt;/p&gt;

&lt;h3 id=&quot;configuring-the-linux-firewall&quot;&gt;Configuring the Linux firewall&lt;/h3&gt;

&lt;p&gt;But wouldn’t it be much simpler if we could access Splunk from our host system? To do that, we need to allow connections to our virtual machine on port 8000. The exact command you’ll need to run will depend on which Linux distro you chose for your virtual machine, but in CentOS it looks like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;firewall&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;permanent&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p style=&quot;float: left; width: 90%; padding: 5px; padding-left: 15px; padding-right: 5%; margin: 30px 0 10px 0; font-style: italic; border-left: 5px solid red; background: #ff9999; color: #333;&quot;&gt;Note that this firewall rule will open your virtual machine to all connections on TCP port 8000. If you're working with more sensitive data or in a less secure/private environment, you'll probably want to be more specific - for example, by specifying a source IP address.&lt;/p&gt;

&lt;p&gt;Now that this firewall rule has been added as a permanent rule, we need to use the &lt;code&gt;reload&lt;/code&gt; command to refresh the live instance of the firewall with the latest ruleset.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;firewall&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reload&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;That should do the trick, but if you want to make sure that you’ve done everything correctly, use the &lt;code&gt;list-all&lt;/code&gt; option to check that your new rule is present and correct.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;firewall&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Your output should look something like this - the important bit is that &lt;code&gt;8000/tcp&lt;/code&gt; is under &lt;code&gt;ports&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/05/splunk_vm_firewall.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If your firewall rule has been added successfully, you’re ready to start using Splunk!&lt;/p&gt;

&lt;h3 id=&quot;accessing-your-splunk-instance&quot;&gt;Accessing your Splunk instance&lt;/h3&gt;

&lt;p&gt;Open a web browser on your host machine and type in the IP address of your virtual machine followed by &lt;code&gt;:8000&lt;/code&gt;. With any luck, you’ll presented with a Splunk login page. Sign in with the administrator credentials you set a little earlier and there you have it - your own Splunk instance to tinker and experiment with.&lt;/p&gt;

&lt;p&gt;At this point, it’s probably a good idea to take a virtual machine snapshot before you start importing data or changing settings. That way, if you make any mistakes, or want to start fresh for any other reason, you can simply revert to the snapshot without doing all the setup again.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/05/splunk_vm_data.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And we’re done! Obviously Splunk won’t run quite as well on a virtual machine with 8GB of RAM as it does in the cloud or on a dedicated server cluster (and it will show warning messages to remind you of this), but I’ve found it surprisingly stable when analysing small datasets and learning its features.&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">Splunk is considered the gold standard for analysis of event logs and other data, but unless you’re lucky enough to work for an organisation that pays for it, it can be difficult to get practical experience in how to run searches, build dashboards, and otherwise dissect data using its query language. Recently, when I wanted to take a look at some Backutil logs to see how recent development had affected performance, I stumbled across Splunk Free - a limited version of the logging platform - and discovered just how easy it is to install (and how surprisingly well it runs) in a virtual machine. Here’s how to do it… Setting up a virtual machine For the purposes of this post, I’ll assume you’re familiar with the basics of setting up a virtual machine in VirtualBox or VMware. I won’t go over the process of creating the machine, but I will point out some key steps to take to make sure Splunk works and is accessible from your host system. First, choose your Linux distro. I went for CentOS, which is usually a safe bet to run a stable server with less of the fluff that comes with some other versions, with 8GB of RAM. Like most modern operating systems, installation is as easy as booting from the ISO and following the on-screen instructions. One extra (but critically important) step: You’ll want to check the virtual machine’s network connection is set to Bridged Adapter. When you boot it up, configure an IP address on the same subnet as your host machine. With these settings, the guest system will behave as though it is a completely separate machine on the network as your computer. This way, once we’ve made some firewall changes, we’ll be able to access Splunk in our browser at the virtual machine’s IP address. Downloading and installing Splunk With our virtual machine set up, it’s time to install Splunk. Download Splunk Free for Linux (you can find it by clicking Free Splunk in the top right corner of the Splunk homepage and selecting Software Download). Either do this in your virtual machine, or do it on your host system and copy the file to your Linux machine. To install Splunk, we need to extract splunk-x.x.x-x-Linux-x86_64.tgz to the /opt directory. The easiest way to do this is to copy it to the correct directory and run the following command. sudo tar xvf splunk-8.1.3-63079c59e632-Linux-x86_64.tgz You should now have a /opt/splunk folder containing the archive’s contents, which are essentially a fully-fledged Splunk installation. There’s just one more command to run to get Splunk up and running. sudo /opt/bin/splunk/start --accept-license The first time you run this file, it will launch the Splunk installer, which will give you an opportunity to set an administrator username and password before your instance is live. At this point, you should be able to access Splunk at 127.0.0.1:8000 in your virtual machine’s web browser. Configuring the Linux firewall But wouldn’t it be much simpler if we could access Splunk from our host system? To do that, we need to allow connections to our virtual machine on port 8000. The exact command you’ll need to run will depend on which Linux distro you chose for your virtual machine, but in CentOS it looks like this: sudo firewall-cmd --zone=public --add-port=8000/tcp --permanent Note that this firewall rule will open your virtual machine to all connections on TCP port 8000. If you're working with more sensitive data or in a less secure/private environment, you'll probably want to be more specific - for example, by specifying a source IP address. Now that this firewall rule has been added as a permanent rule, we need to use the reload command to refresh the live instance of the firewall with the latest ruleset. sudo firewall-cmd --reload That should do the trick, but if you want to make sure that you’ve done everything correctly, use the list-all option to check that your new rule is present and correct. sudo firewall-cmd --list-all Your output should look something like this - the important bit is that 8000/tcp is under ports. If your firewall rule has been added successfully, you’re ready to start using Splunk! Accessing your Splunk instance Open a web browser on your host machine and type in the IP address of your virtual machine followed by :8000. With any luck, you’ll presented with a Splunk login page. Sign in with the administrator credentials you set a little earlier and there you have it - your own Splunk instance to tinker and experiment with. At this point, it’s probably a good idea to take a virtual machine snapshot before you start importing data or changing settings. That way, if you make any mistakes, or want to start fresh for any other reason, you can simply revert to the snapshot without doing all the setup again. And we’re done! Obviously Splunk won’t run quite as well on a virtual machine with 8GB of RAM as it does in the cloud or on a dedicated server cluster (and it will show warning messages to remind you of this), but I’ve found it surprisingly stable when analysing small datasets and learning its features.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mattcasmith.net/wp-content/uploads/2021/05/splunk_vm_data.png" /><media:content medium="image" url="https://mattcasmith.net/wp-content/uploads/2021/05/splunk_vm_data.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">File carving: Recovering a deleted file from a Windows disk image</title><link href="https://mattcasmith.net/2021/04/02/file-carving-recovering-deleted-file-disk-image" rel="alternate" type="text/html" title="File carving&amp;#58; Recovering a deleted file from a Windows disk image" /><published>2021-04-02T01:00:00+01:00</published><updated>2021-04-02T01:00:00+01:00</updated><id>https://mattcasmith.net/2021/04/02/file-carving-recovering-deleted-file-disk-image</id><content type="html" xml:base="https://mattcasmith.net/2021/04/02/file-carving-recovering-deleted-file-disk-image">&lt;p&gt;Most computer users assume that when they delete a file and empty the Recycle Bin, it’s gone forever. After all, if Windows doesn’t show us a file, it doesn’t exist anymore, right? Wrong. With the right tools and knowledge, forensics experts can find fragments - or even complete versions - of deleted files that remain on the hard disk long after they disappear from Windows Explorer.&lt;/p&gt;

&lt;p&gt;In this post, I’ll run through how to take a disk image and recover deleted files using FTK Imager and a hex editor, explain why this works, and show how certain utilities let you overwrite this remnant data.&lt;/p&gt;

&lt;h3 id=&quot;what-happens-when-you-delete-a-file&quot;&gt;What happens when you delete a file?&lt;/h3&gt;

&lt;p&gt;To give myself something to find, I created the text file shown below on a 2GB hard drive partition. I saved it, then closed it, deleted it, and emptied the Recycle Bin to render it inaccessible to any normal Windows user. But what is actually happening in the background when you do this?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/04/deletedfile_file.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At the top of your hard drive is what’s known as the master file table (MFT). This is essentially an index - it contains filenames along with the addresses on the hard drive that these files can be found at. So when I create &lt;code&gt;test_file.txt&lt;/code&gt;, an MFT record is created containing that filename and the address where the data (i.e. the file contents) is stored - in this case that’s &lt;code&gt;40009D50h&lt;/code&gt;, but we don’t know that yet.&lt;/p&gt;

&lt;p&gt;When you delete a file, none of this information is removed from the disk. All deletion does is add a “deleted” flag to the MFT record, telling the computer that the information at the specified address is no longer needed and can be overwritten with new data if necessary - but nothing is actually deleted at this point and the file data is still present on the disk. You can probably guess where this is going…&lt;/p&gt;

&lt;h3 id=&quot;creating-a-disk-image-with-ftk-imager&quot;&gt;Creating a disk image with FTK Imager&lt;/h3&gt;

&lt;p&gt;With the right tools, we can access the remnant data. First, we need a physical disk image to work with. &lt;a href=&quot;https://accessdata.com/product-download&quot; target=&quot;_blank&quot;&gt;FTK Imager&lt;/a&gt; is a free tool that allows us to create one. We choose a few simple options (I’m generating an image in the &lt;code&gt;E01&lt;/code&gt; format) and set it to work. Depending on the size of the disk and your computer’s speed, it will take a little while to generate the image, which is an exact replica of the hard drive.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/04/deletedfile_ftk.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One thing to note is that the image cannot be generated on the same physical hard drive that is being imaged. For this reason, unless the system has multiple hard drives, it is necessary to choose to save the image to a directory on a removable drive or a network share, rather than on the subject computer itself.&lt;/p&gt;

&lt;h3 id=&quot;recovering-a-deleted-file-with-ftk-imager&quot;&gt;Recovering a deleted file with FTK Imager&lt;/h3&gt;

&lt;p&gt;FTK Imager can also be useful for the next step in our process - it can actually do the hard work for us and recover the deleted file. If we mount the physical image we created and open the correct partition, we can navigate through the file system to find deleted files, which will be marked with a red cross.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/04/deletedfile_ftkrecover.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As the last place the file was present was the Recycle Bin (read my post about &lt;a href=&quot;/2018/12/15/python-windows-forensics-recycle-bin-deleted-files/&quot;&gt;Python forensics for the Recycle Bin&lt;/a&gt; for more information about the file structure), its MFT entry points to that directory. Here, we can see that the file &lt;code&gt;$RHEYFYA.txt&lt;/code&gt; is our test file, and FTK Imager helpfully displays the contents.&lt;/p&gt;

&lt;h3 id=&quot;recovering-a-deleted-file-with-a-hex-editor&quot;&gt;Recovering a deleted file with a hex editor&lt;/h3&gt;

&lt;p&gt;But things won’t always be that easy, and we shouldn’t rely on tools to do our forensics for us without knowing how they work. We can find the same data manually by mounting the disk image and opening it in a hex editor - in this example, &lt;a href=&quot;https://www.sweetscape.com/010editor/&quot; target=&quot;_blank&quot;&gt;010 Editor&lt;/a&gt;. This displays the entire contents of the hard drive in hex format, and has a helpful feature that will search for strings amongst the data in the image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/04/deletedfile_hex.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, I was able to find the text file’s contents by searching for strings. If we were looking for a different type of file, we might search according to its file signature, which indicates what format the file is in (for example, JPEGs start with &lt;code&gt;FF D8 FF E0&lt;/code&gt;). Text files don’t normally have signatures, and in this case we can see the file data starts immediately at the address &lt;code&gt;40009D50h&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;overwriting-traces-of-deleted-files&quot;&gt;Overwriting traces of deleted files&lt;/h3&gt;

&lt;p&gt;So we’ve seen that it’s possible to recover deleted files, but is there any way to remove them  from the disk completely? In short: yes, there is. Tools like &lt;a href=&quot;https://www.ccleaner.com/&quot; target=&quot;_blank&quot;&gt;CCleaner&lt;/a&gt; can overwrite the free space on a hard drive with nonsense data. If I run CCleaner’s Drive Wiper feature on our partition and repeat the above process, we can see that the address formerly occupied by &lt;code&gt;test_file.txt&lt;/code&gt; is now full of zeroes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/04/deletedfile_ccleaner.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While this prevents a forensic investigator from recovering a file’s contents if done correctly, using some of these tools will arouse suspicion in itself. Some wipers will fill free space with ones, Zs, or random data, and this does not normally occur naturally. If this is observed on a hard drive then questions will be asked over whether the user was trying to hide something and a more thorough search may be conducted.&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">Most computer users assume that when they delete a file and empty the Recycle Bin, it’s gone forever. After all, if Windows doesn’t show us a file, it doesn’t exist anymore, right? Wrong. With the right tools and knowledge, forensics experts can find fragments - or even complete versions - of deleted files that remain on the hard disk long after they disappear from Windows Explorer. In this post, I’ll run through how to take a disk image and recover deleted files using FTK Imager and a hex editor, explain why this works, and show how certain utilities let you overwrite this remnant data. What happens when you delete a file? To give myself something to find, I created the text file shown below on a 2GB hard drive partition. I saved it, then closed it, deleted it, and emptied the Recycle Bin to render it inaccessible to any normal Windows user. But what is actually happening in the background when you do this? At the top of your hard drive is what’s known as the master file table (MFT). This is essentially an index - it contains filenames along with the addresses on the hard drive that these files can be found at. So when I create test_file.txt, an MFT record is created containing that filename and the address where the data (i.e. the file contents) is stored - in this case that’s 40009D50h, but we don’t know that yet. When you delete a file, none of this information is removed from the disk. All deletion does is add a “deleted” flag to the MFT record, telling the computer that the information at the specified address is no longer needed and can be overwritten with new data if necessary - but nothing is actually deleted at this point and the file data is still present on the disk. You can probably guess where this is going… Creating a disk image with FTK Imager With the right tools, we can access the remnant data. First, we need a physical disk image to work with. FTK Imager is a free tool that allows us to create one. We choose a few simple options (I’m generating an image in the E01 format) and set it to work. Depending on the size of the disk and your computer’s speed, it will take a little while to generate the image, which is an exact replica of the hard drive. One thing to note is that the image cannot be generated on the same physical hard drive that is being imaged. For this reason, unless the system has multiple hard drives, it is necessary to choose to save the image to a directory on a removable drive or a network share, rather than on the subject computer itself. Recovering a deleted file with FTK Imager FTK Imager can also be useful for the next step in our process - it can actually do the hard work for us and recover the deleted file. If we mount the physical image we created and open the correct partition, we can navigate through the file system to find deleted files, which will be marked with a red cross. As the last place the file was present was the Recycle Bin (read my post about Python forensics for the Recycle Bin for more information about the file structure), its MFT entry points to that directory. Here, we can see that the file $RHEYFYA.txt is our test file, and FTK Imager helpfully displays the contents. Recovering a deleted file with a hex editor But things won’t always be that easy, and we shouldn’t rely on tools to do our forensics for us without knowing how they work. We can find the same data manually by mounting the disk image and opening it in a hex editor - in this example, 010 Editor. This displays the entire contents of the hard drive in hex format, and has a helpful feature that will search for strings amongst the data in the image. As you can see, I was able to find the text file’s contents by searching for strings. If we were looking for a different type of file, we might search according to its file signature, which indicates what format the file is in (for example, JPEGs start with FF D8 FF E0). Text files don’t normally have signatures, and in this case we can see the file data starts immediately at the address 40009D50h. Overwriting traces of deleted files So we’ve seen that it’s possible to recover deleted files, but is there any way to remove them from the disk completely? In short: yes, there is. Tools like CCleaner can overwrite the free space on a hard drive with nonsense data. If I run CCleaner’s Drive Wiper feature on our partition and repeat the above process, we can see that the address formerly occupied by test_file.txt is now full of zeroes. While this prevents a forensic investigator from recovering a file’s contents if done correctly, using some of these tools will arouse suspicion in itself. Some wipers will fill free space with ones, Zs, or random data, and this does not normally occur naturally. If this is observed on a hard drive then questions will be asked over whether the user was trying to hide something and a more thorough search may be conducted.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mattcasmith.net/wp-content/uploads/2021/04/deletedfile_hex.png" /><media:content medium="image" url="https://mattcasmith.net/wp-content/uploads/2021/04/deletedfile_hex.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Backutil development: Building and JOINing SQLite databases in Python</title><link href="https://mattcasmith.net/2021/03/26/sqlite-databases-python" rel="alternate" type="text/html" title="Backutil development&amp;#58; Building and JOINing SQLite databases in Python" /><published>2021-03-26T00:00:00+00:00</published><updated>2021-03-26T00:00:00+00:00</updated><id>https://mattcasmith.net/2021/03/26/using-sqlite-python-protecting-sql-injection</id><content type="html" xml:base="https://mattcasmith.net/2021/03/26/sqlite-databases-python">&lt;p&gt;This post is something of a development diary for Backutil - &lt;a href=&quot;http://127.0.0.1:4000/2021/01/01/backutil-windows-backup-utility&quot;&gt;my Python-based utility for backing up files from Windows systems&lt;/a&gt;. I published the first version of Backutil (v0.51) at the beginning of 2021, and pushed a small update (v0.52) to fix some minor issues in February.&lt;/p&gt;

&lt;p&gt;As of v0.52, the utility still relied on text files full of file hashes to remember what it had backed up, and in-memory Pandas dataframes to compare the files currently on the system to the previous file hashes and work out what to back up. The full process looked something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/03/sqlite_before.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For v0.61, I wanted to streamline this somewhat, and hopefully speed up the process (this will be boosted by a multithreading implementation in a future update). The solution? SQLite - as the name would suggest, a lighter version of SQL that can be used to store databases either in memory or as files via the Python &lt;code&gt;sqlite3&lt;/code&gt; library. This enabled me to use &lt;a href=&quot;/2018/10/12/basic-sql-queries-select-from-where-operators/&quot;&gt;SQL functions like &lt;code&gt;INSERT&lt;/code&gt;, &lt;code&gt;DELETE&lt;/code&gt; and &lt;code&gt;JOIN&lt;/code&gt;&lt;/a&gt; to more efficiently manipulate the backup data. The new process looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/03/sqlite_after.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To demonstrate this, I’ll run through a few examples of how SQLite is used in the latest version.&lt;/p&gt;

&lt;h3 id=&quot;managing-the-database-connection&quot;&gt;Managing the database connection&lt;/h3&gt;

&lt;p&gt;Before we can use a SQLite database, we must connect to it. As this would happen several times through the code, I defined a function that takes the &lt;code&gt;config&lt;/code&gt; class and &lt;code&gt;action&lt;/code&gt; variable. If the &lt;code&gt;action&lt;/code&gt; is &lt;code&gt;open&lt;/code&gt;, we connect to the database and create the &lt;code&gt;backutil_previous&lt;/code&gt; table if it does not already exist.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;open&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db_conn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqlite3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db_cursor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db_conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db_cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;CREATE TABLE IF NOT EXISTS backutil_previous(date TEXT, hash TEXT);&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If the &lt;code&gt;action&lt;/code&gt; supplied to the function is &lt;code&gt;close&lt;/code&gt;, we close the connection to the database.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;close&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db_conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;With this function defined and actions taken according to the contents of the &lt;code&gt;action&lt;/code&gt; variable, we can open and close the database using a single line elsewhere in the code.&lt;/p&gt;

&lt;h3 id=&quot;getting-previous-backup-dates&quot;&gt;Getting previous backup dates&lt;/h3&gt;

&lt;p&gt;In order to work out whether the backups need to be rotated, Backutil previously counted the number of &lt;code&gt;.back&lt;/code&gt; text files in its directory. Now, however, we can use &lt;code&gt;SELECT DISTINCT&lt;/code&gt; to get a list of unique timestamps of previous backups from the SQLite database stored on disk.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;n&quot;&gt;backup_dates&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;db_dates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;previous_db_cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;SELECT DISTINCT date FROM backutil_previous ORDER BY date ASC;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;date&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db_dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;backup_dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This code iterates through the returned dates and adds them to a Python list named &lt;code&gt;backup_dates&lt;/code&gt;, the length of which is subsequently counted to determine whether there are more backups than required.&lt;/p&gt;

&lt;h3 id=&quot;deleting-data-from-old-backups&quot;&gt;Deleting data from old backups&lt;/h3&gt;

&lt;p&gt;If there are too many backups, the oldest backup must be deleted, along with its entries in the previous backups database. We can take the oldest date from &lt;code&gt;backup_dates&lt;/code&gt; and use the &lt;code&gt;DELETE&lt;/code&gt; operation. Using question marks in &lt;code&gt;sqlite3&lt;/code&gt; inserts items from the supplied variable - in this case &lt;code&gt;query_data&lt;/code&gt; - and is best practice to protect against SQL injection by interpreting strings only as literal values.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backup_dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backups_retained&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backups_rotated&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;True&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;query_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backup_dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;previous_db_cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;DELETE FROM backutil_previous WHERE date = ?;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;previous_db_conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;commit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Another important note here is that after an operation is executed that would change a SQLite database, it must be committed. If we did not commit our changes to the database, it would not be written to the file on disk. Once the changes are committed, we have deleted the oldest backup hash entries for good.&lt;/p&gt;

&lt;h3 id=&quot;checking-old-backup-hashes-against-current-files&quot;&gt;Checking old backup hashes against current files&lt;/h3&gt;

&lt;p&gt;Meanwhile, a separate database named &lt;code&gt;backutil_tracker&lt;/code&gt; has been built in memory, containing the hashes of all the files that currently reside in the backup directories. To work out which files are new and need to be backed up, we must compare them to the hashes in &lt;code&gt;backutil_previous&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We can use &lt;code&gt;ATTACH&lt;/code&gt; to enable cross-database operations, then perform a &lt;code&gt;LEFT JOIN&lt;/code&gt; (see &lt;a href=&quot;/2018/12/21/sql-joins-inner-left-right-outer/&quot;&gt;my previous post on SQL &lt;code&gt;JOIN&lt;/code&gt;s&lt;/a&gt; for all the details) to combine the data from the two tables based on the file hashes.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;n&quot;&gt;query_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;previous_db_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tracker_db_cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ATTACH ? as backutil_previous&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tracker_db_cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;SELECT backutil_tracker.file, backutil_tracker.hash, backutil_previous.date FROM backutil_tracker LEFT JOIN backutil_previous ON backutil_tracker.hash=backutil_previous.hash;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;files_to_back_up&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If a hash in &lt;code&gt;backutil_tracker&lt;/code&gt; matches a hash in &lt;code&gt;backutil_previous&lt;/code&gt;, the date that file was backed up is copied to the table. Once all hashes have been checked, we know that rows with &lt;code&gt;None&lt;/code&gt; in the date column do not exist in previous backups. These are copied to a Python list that will be supplied to the backup loop. This way, only new files are added to the new backup.&lt;/p&gt;

&lt;h3 id=&quot;writing-backed-up-files-to-the-database&quot;&gt;Writing backed up files to the database&lt;/h3&gt;

&lt;p&gt;As Backutil successfully copies files, it appends their rows to the &lt;code&gt;files_backed_up&lt;/code&gt; list. Once all files have been safely copied, compressed, and encrypted, we can &lt;code&gt;INSERT&lt;/code&gt; the contents of this list into the &lt;code&gt;backutil_previous&lt;/code&gt; table, which is stored on disk for future reference.&lt;/p&gt;

&lt;p&gt;We open the database connection again, execute the &lt;code&gt;INSERT&lt;/code&gt; operation, then commit the changes. The new hashes will be written to the database with the timestamp of the new backup.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;        &lt;span class=&quot;n&quot;&gt;manage_previous_db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;open&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backed_up_file&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;files_backed_up&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;query_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backup_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backed_up_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;previous_db_cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;INSERT INTO backutil_previous (date, hash) VALUES (?, ?);&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;previous_db_conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;commit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;manage_previous_db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;close&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;With our files backed up and the new hashes written to &lt;code&gt;backutil_previous&lt;/code&gt;, we can close our database connection. The database will remain on disk until it is called upon during the next backup, at which time Backutil will not copy the same files again because their hashes are now in the database.&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">This post is something of a development diary for Backutil - my Python-based utility for backing up files from Windows systems. I published the first version of Backutil (v0.51) at the beginning of 2021, and pushed a small update (v0.52) to fix some minor issues in February. As of v0.52, the utility still relied on text files full of file hashes to remember what it had backed up, and in-memory Pandas dataframes to compare the files currently on the system to the previous file hashes and work out what to back up. The full process looked something like this: For v0.61, I wanted to streamline this somewhat, and hopefully speed up the process (this will be boosted by a multithreading implementation in a future update). The solution? SQLite - as the name would suggest, a lighter version of SQL that can be used to store databases either in memory or as files via the Python sqlite3 library. This enabled me to use SQL functions like INSERT, DELETE and JOIN to more efficiently manipulate the backup data. The new process looks like this: To demonstrate this, I’ll run through a few examples of how SQLite is used in the latest version. Managing the database connection Before we can use a SQLite database, we must connect to it. As this would happen several times through the code, I defined a function that takes the config class and action variable. If the action is open, we connect to the database and create the backutil_previous table if it does not already exist. if action == &quot;open&quot;: config.db_conn = sqlite3.connect(config.db_name) config.db_cursor = config.db_conn.cursor() config.db_cursor.execute(&quot;CREATE TABLE IF NOT EXISTS backutil_previous(date TEXT, hash TEXT);&quot;) If the action supplied to the function is close, we close the connection to the database. if action == &quot;close&quot;: config.db_conn.close() With this function defined and actions taken according to the contents of the action variable, we can open and close the database using a single line elsewhere in the code. Getting previous backup dates In order to work out whether the backups need to be rotated, Backutil previously counted the number of .back text files in its directory. Now, however, we can use SELECT DISTINCT to get a list of unique timestamps of previous backups from the SQLite database stored on disk. backup_dates=[] db_dates = config.previous_db_cursor.execute(&quot;SELECT DISTINCT date FROM backutil_previous ORDER BY date ASC;&quot;) for date in db_dates: backup_dates.append(date[0]) This code iterates through the returned dates and adds them to a Python list named backup_dates, the length of which is subsequently counted to determine whether there are more backups than required. Deleting data from old backups If there are too many backups, the oldest backup must be deleted, along with its entries in the previous backups database. We can take the oldest date from backup_dates and use the DELETE operation. Using question marks in sqlite3 inserts items from the supplied variable - in this case query_data - and is best practice to protect against SQL injection by interpreting strings only as literal values. if len(backup_dates) &amp;gt; (config.backups_retained - 1) and (config.backups_rotated == &quot;True&quot;): query_data = (str(backup_dates[0]),) config.previous_db_cursor.execute(&quot;DELETE FROM backutil_previous WHERE date = ?;&quot;, query_data) config.previous_db_conn.commit() Another important note here is that after an operation is executed that would change a SQLite database, it must be committed. If we did not commit our changes to the database, it would not be written to the file on disk. Once the changes are committed, we have deleted the oldest backup hash entries for good. Checking old backup hashes against current files Meanwhile, a separate database named backutil_tracker has been built in memory, containing the hashes of all the files that currently reside in the backup directories. To work out which files are new and need to be backed up, we must compare them to the hashes in backutil_previous. We can use ATTACH to enable cross-database operations, then perform a LEFT JOIN (see my previous post on SQL JOINs for all the details) to combine the data from the two tables based on the file hashes. query_data = (config.previous_db_name,) config.tracker_db_cursor.execute(&quot;ATTACH ? as backutil_previous&quot;, query_data) results = config.tracker_db_cursor.execute(&quot;SELECT backutil_tracker.file, backutil_tracker.hash, backutil_previous.date FROM backutil_tracker LEFT JOIN backutil_previous ON backutil_tracker.hash=backutil_previous.hash;&quot;) for line in results: if line[2] == None: config.files_to_back_up.append(line) If a hash in backutil_tracker matches a hash in backutil_previous, the date that file was backed up is copied to the table. Once all hashes have been checked, we know that rows with None in the date column do not exist in previous backups. These are copied to a Python list that will be supplied to the backup loop. This way, only new files are added to the new backup. Writing backed up files to the database As Backutil successfully copies files, it appends their rows to the files_backed_up list. Once all files have been safely copied, compressed, and encrypted, we can INSERT the contents of this list into the backutil_previous table, which is stored on disk for future reference. We open the database connection again, execute the INSERT operation, then commit the changes. The new hashes will be written to the database with the timestamp of the new backup. manage_previous_db(config, &quot;open&quot;) for backed_up_file in config.files_backed_up: query_data = (config.backup_time, backed_up_file) config.previous_db_cursor.execute(&quot;INSERT INTO backutil_previous (date, hash) VALUES (?, ?);&quot;, query_data) config.previous_db_conn.commit() manage_previous_db(config, &quot;close&quot;) With our files backed up and the new hashes written to backutil_previous, we can close our database connection. The database will remain on disk until it is called upon during the next backup, at which time Backutil will not copy the same files again because their hashes are now in the database.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mattcasmith.net/wp-content/uploads/2021/03/sqlite_after.png" /><media:content medium="image" url="https://mattcasmith.net/wp-content/uploads/2021/03/sqlite_after.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">PES 5 Master League retrospective ‐ balancing realism and gameplay</title><link href="https://mattcasmith.net/2021/01/16/pro-evolution-soccer-5-master-league" rel="alternate" type="text/html" title="PES 5 Master League retrospective &amp;#8208; balancing realism and gameplay" /><published>2021-01-16T00:00:00+00:00</published><updated>2021-01-16T00:00:00+00:00</updated><id>https://mattcasmith.net/2021/01/16/pro-evolution-soccer-5-master-league</id><content type="html" xml:base="https://mattcasmith.net/2021/01/16/pro-evolution-soccer-5-master-league">&lt;p&gt;I have a rare off-topic post for you this month – something of an essay about the structure of the classic &lt;em&gt;Pro Evolution Soccer&lt;/em&gt; Master League, the differences in its modern-day equivalents, and a discussion about the delicate balance between realism and gameplay in video games more widely. Design in football games doesn’t get much attention, as the focus is normally on comparisons to the real game, so I thought this would be an interesting area to explore in a little more detail than I’ve seen before.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Pro Evolution Soccer 5&lt;/em&gt; has always been an all-time favourite of mine, but it is only now, 15 years after its initial release, that I have found the inspiration to do it justice in words. Of course, there are many reviews out there from 2005 that cover its gameplay, features, and so on, but in this post I have attempted to capture what made it so perfectly balanced and special to many fans. I hope that at the very least this will stir up some nostalgia that inspire others to dig out their copies for a match or two.&lt;/p&gt;

&lt;p style=&quot;float: left; width: 90%; padding: 5px; padding-left: 15px; padding-right: 5%; margin: 30px 0 10px 0; font-style: italic; border-left: 5px solid #ffff00; background: #ffffb9; color: #333;&quot;&gt;This isn’t something I’ve ever done before on this blog – but don’t worry, I won’t be doing this all the time and my focus will still be &lt;a href=&quot;/category/cyber-security.html&quot;&gt;cyber security&lt;/a&gt;! I just might dig up a retro game or two from time to time for and write a bit of a deep dive when the inspiration strikes me.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/01/pes-5-1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Like everyone else, I didn’t have a normal Christmas holiday this year. In my efforts to stave off boredom in the gap between Christmas and the new year, I installed an emulator and grabbed a copy of &lt;em&gt;Pro Evolution Soccer 5&lt;/em&gt; to play. I wasted a huge number of hours playing this game as a teenager, and wanted to see how it held up 15 years and three console generations later. To my surprise, I once again became addicted, and played Master League matches on a daily basis for the rest of the break.&lt;/p&gt;

&lt;p&gt;There is some nostalgia involved here, of course – both for the game and the 2005/06 football season – but there are so many things that should have driven me away: the graphics are rough round the edges, the game mechanics are unrefined, and the emulator stutters and occasionally refuses to let me run right until I reconnect my controller. With all these inconveniences, what kept me playing? And why didn’t I fire up my PlayStation 4 or PC and play one of the football games from 2020 instead?&lt;/p&gt;

&lt;p&gt;I believe the answer says a lot about design, both in football games and video games in general. I’d always considered &lt;em&gt;PES 5&lt;/em&gt;, or more generally the period between &lt;em&gt;Pro Evolution Soccer 4&lt;/em&gt; and &lt;em&gt;Pro Evolution Soccer 6&lt;/em&gt; on PlayStation 2 and Xbox, to be a high tide mark for football games. But even with this in mind when playing more modern games, I could never really pinpoint why. Returning to &lt;em&gt;PES 5&lt;/em&gt; in 2020 with fresh eyes has given me the perspective I needed to put this X factor into words.&lt;/p&gt;

&lt;h3 id=&quot;part-i-the-master-league&quot;&gt;Part I: The Master League&lt;/h3&gt;

&lt;p&gt;The reasons for &lt;em&gt;PES 5&lt;/em&gt;’s superiority lie both on and off the pitch. I’ll begin with the latter, which I believe is a masterstroke that is key to the game’s overall addictiveness. The Master League is Konami’s answer to FIFA’s Career Mode – a multi-season campaign that puts the player in charge of a club with financial responsibilities and the freedom to make transfers. However, there are some important differences that lean away from realism and integrate systems more often seen in other video game genres.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/01/pes-5-2.png&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;a-sense-of-ownership&quot;&gt;A sense of ownership&lt;/h4&gt;

&lt;p&gt;The first important difference is that the Master League’s focus is on the club, not the manager’s career. The player can either select or create a team at the beginning of the Master League, and cannot switch to another team at any point. If they choose the latter option, they choose the team’s name, design kits (which integrate with the game world better than similar features in other games due to the range of design options and general lack of licences), and select other attributes like a stadium and crowd colours.&lt;/p&gt;

&lt;p&gt;The “real” way to play the Master League is to begin with the default squad, rather than the team’s real-life players. This team, which includes fictional players like Castolo, Minanda, Stremer, and Ivarov, is largely useless in terms of ability. They take touches metres ahead of themselves, hit shots wildly off-target unless directly facing the goal at walking pace, and are outmuscled by even the smallest opponents. This means there is a natural siege mentality for the first couple of seasons, and scraping a point feels like a victory. Unlike &lt;em&gt;FIFA&lt;/em&gt;, where selecting Chelsea will mean Champions League matches in your first year in charge, it is not even possible to reach continental competition in the Master League until at least the third season, and even that is unlikely given the players initially at your disposal. This adds to the sense of progression as the player – and by extension their club – earns their place in the footballing world.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/01/pes-5-3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;All of this means the player has a far greater sense of ownership than they do in other games’ career modes, where they can take over a successful team of competent players assembled by a real-world manager. This is your club, and as the seasons progress and you slowly bring in talent during the transfer windows, each player in the squad reflects a decision you have made. It’s less realistic, but makes for a far more engaging game – something that will become a recurring theme through this post.&lt;/p&gt;

&lt;h4 id=&quot;a-football-rpg&quot;&gt;A football RPG&lt;/h4&gt;

&lt;p&gt;This culminates in a mode structured more like a football role-playing game (RPG) than a sports game. The squad is akin the player’s party, the matches are battles, and the time between is spent managing resources and organising the party in a way that increases the success rate, including keeping track of players’ development and signing new personnel where required. To help them on their way, &lt;em&gt;PES 5&lt;/em&gt; tracks and makes available quite a bit more data than even the most recent football games.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/01/pes-5-4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;An important aspect of this is that transfers are straightforward, and are far more about affordability than negotiation, which is rarely done well in games. In this respect, the player’s focus is always on their points balance (more on that later) and earning enough to upgrade. They aren’t forced to sit through lengthy office cut scenes, playing a guessing game with an inconsistent AI to agree on a price, and being unable to sign the best players even in the cash-rich end game because of their value to their current clubs.&lt;/p&gt;

&lt;p&gt;Scouting players to sign is also more rewarding than in modern &lt;em&gt;FIFA&lt;/em&gt; or &lt;em&gt;PES&lt;/em&gt;. Players in both current games have an Overall rating, which often reduces comparison to a single statistic and makes choosing between two players, whether for a transfer or a starting position, a one-dimensional affair. &lt;em&gt;PES 5&lt;/em&gt; does not have this statistic, and the player must instead scan each player’s full attributes to gain an impression of their ability. Overlapping star diagrams like the one pictured above allow the player to directly compare six attributes when switching players, sometimes forcing an informed decision – for example, do I choose a full-back who is pacey and attacking, or powerful and defensive? It also helps that the full list of attributes is smaller than in modern games, making it easier to review them in full.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/01/pes-5-5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The exaggerated speed and visibility with which these attributes improve also adds to the RPG-like sense of narrative and progression. After each match, the player is taken to a screen where progression points for each attribute stack up for the players that took to the pitch, and it’s hugely satisfying to see your signings’ attributes increase. It’s difficult to compare them exactly, but I believe the progression in &lt;em&gt;PES 5&lt;/em&gt; is also quicker than in games like the modern &lt;em&gt;FIFA&lt;/em&gt; series, where a player will join with an Overall rating in the 50s and take a decade to reach full potential. Once again, this is a compromise on realism for the sake of gameplay, increasing the investment that the player has in their squad and club.&lt;/p&gt;

&lt;h4 id=&quot;extreme-focus&quot;&gt;Extreme focus&lt;/h4&gt;

&lt;p&gt;However, the most important part of the Master League, and I believe has been lost in every current career mode, is the fact that it keeps the player extremely focused on a simple task: survival and development. There is no fan sentiment to monitor, no board waiting to sack you, and no possibility of a move to another club. It’s you and your created team against the world, and if they fail, you may be forced to start another game (something that once happened to me, when I foolishly signed Steven Pienaar and saved myself into a corner, ensuring bankruptcy). The risk level is comparable to that of classic arcade games.&lt;/p&gt;

&lt;p&gt;Points are the currency in PES 5, and the only number that matters is your bank account balance. You earn points by getting results: 1,000 for a win and 500 for a draw, with bonus points for each goal scored. Your players are paid an annual salary, and if you don’t have enough points to cover the wage bill at the end of the season, it’s game over. Any leftover funds can be used to sign new players, leading to something of a gamble on a mental risk-reward calculation when signing players during the mid-season transfer window – will the added talent be enough to secure the results necessary to pay the wages at the end of the season? Or will your ambition be the cause of your downfall?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/01/pes-5-6.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The focus on points and the wage bill adds real tension to the Master League, which is something that isn’t recreated in career modes with separate wage budgets. The stakes of each individual match are raised – especially towards the end of a subpar season – and while the board won’t be on your back for being knocked out of the cup, the number of matches and therefore your chance of winning points is reduced. This can lead to an intense scramble to scrape results in the last few fixtures.&lt;/p&gt;

&lt;p&gt;The points system also has the added effect of tying Master League difficulty directly to match difficulty. A greater challenge on the pitch means a lower chance of amassing points and signing star players. As a result, the focus is always on the football and never on dialogue trees, objectives, or working out how to please unseen and unpredictable parties that could cut your tenure short. This makes sense in the big picture, because no &lt;em&gt;PES&lt;/em&gt; or &lt;em&gt;FIFA&lt;/em&gt; game will ever be &lt;a href=&quot;/2019/01/25/football-manager-addictive-spreadsheet/&quot;&gt;&lt;em&gt;Football Manager&lt;/em&gt;&lt;/a&gt;, so they shouldn’t try to be. Perhaps the Master League economy is a byproduct of development limitations, but accidental or not, it is a beautifully concise system that could be discussed alongside some of history’s best gameplay loops.&lt;/p&gt;

&lt;h3 id=&quot;part-ii-core-gameplay&quot;&gt;Part II: Core gameplay&lt;/h3&gt;

&lt;p&gt;But what of the gameplay on the pitch, where the player spends most of their time? I remember marvelling at the realism of &lt;em&gt;PES 5&lt;/em&gt; back in 2005, but in retrospect it is nothing of the sort. Quick passes fly about effortlessly, players’ movements follow robotic paths, and pinball-like deflections between bodies produce some truly comedic spells of play. These issues may have been the due to the hardware of the time – there’s only so much Konami could do with physics and AI on a PlayStation 2, after all - but these compromises in the representation of real-life football also serve to make the game more fun.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/01/pes-5-7.png&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;the-beautiful-game&quot;&gt;The beautiful game&lt;/h4&gt;

&lt;p&gt;No football game before &lt;em&gt;PES 5&lt;/em&gt; – and arguably no game since – flows as fantastically as Konami’s masterpiece. Its gameplay mechanics are designed for fast, end-to-end action, with passes pinging between players with pinpoint accuracy, but ball control that is heavy enough that any attempt to break into a packed penalty area carries a real risk of losing possession without creating an opportunity. This means shooting from distance is just as viable an option, and it is possible to score some real 30-yard screamers with the more talented players of the time like Frank Lampard and Steven Gerrard.&lt;/p&gt;

&lt;p&gt;The game’s attacking balance is unmatched – you’re just as likely to score or miss with a long shot, a run through the middle, or a cross into the box – and this variety is catalysed by the aforementioned element of randomness. All those different shot types, deflections, mistakes, and rebounds add an organic unpredictability that means that even after hundreds of hours of play, players still see new situations emerge. This stops attacking and defending, even against the AI, from falling into the predictable patterns seen in many football games (like some &lt;em&gt;FIFA&lt;/em&gt; editions’ six-yard box square-pass-and-shoot goals).&lt;/p&gt;

&lt;h4 id=&quot;ai-and-momentum&quot;&gt;AI and momentum&lt;/h4&gt;

&lt;p&gt;It’s also worth taking a moment to discuss the AI, which you’ll be seeing a lot of if you choose to play the Master League mode. Simply put, &lt;em&gt;PES 5&lt;/em&gt;’s AI is a bigger challenge than any modern football game’s equivalent. Three seasons into my new campaign, I was just about winning consistently on four stars, and that still left me with the five-star mode (and the six-star mode, unlockable via the PES Shop) to take on.&lt;/p&gt;

&lt;p&gt;I believe a lack of predictability is again what makes the AI work so well – it seems to have a random element, and also encounters all the scraps and deflections that you do, which stops the player from learning that opponents will always pass in a certain direction or pattern. Perhaps this generation of football games was also slightly easier to develop AI for, in that the game engine itself was simpler, and there were fewer mechanics for the AI to have to interpret and choose between.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PES 5&lt;/em&gt; matches also benefit from being left to run their course naturally. In 2021, football game forums are rife with complaints about alleged “momentum” mechanics – the idea that the game “wants” a certain result for entertainment value, and makes a losing team more likely to score a last-minute equaliser or pits a dominant player’s championship challengers against a team fighting relegation who seem to gain superhuman powers if the player has won too many matches recently. While I remember reading some initial murmurings about momentum at the time, &lt;em&gt;PES 5&lt;/em&gt; seems to largely let the game flow, free from artficial drama. The bulk of the “momentum” you’ll find here is in the commentary (particularly from Trevor Brooking, who seems obsessed with the word, especially following goals).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/01/pes-5-10.png&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;party-on-the-pitch&quot;&gt;Party on the pitch&lt;/h4&gt;

&lt;p&gt;Earlier, I discussed the Master League mode’s RPG-like elements, and these are also present as you take to the pitch, with the game adjusting noticeably to players’ attributes and engineering situations for them to take effect. Starting a player with less fatigue and better form is often more beneficial than starting a jaded superstar, and some abilities like the famous lofted through ball are essentially locked off until you have skilled enough players. These limitations initially led me to believe the game was much more basic than I remembered when I first picked it up, but as I acquired more talented players and began to see the effects on the pitch, I began to realise that this was just another facet of its complexity.&lt;/p&gt;

&lt;p&gt;In a similar way, a game of &lt;em&gt;PES 5&lt;/em&gt; includes several subtle, and sometimes slightly frustrating, gameplay features that digress from real football in the name of playability. Low-skill players take huge touches when controlling even the simplest pass, for example, and players almost always stumble when tackled, making it impossible to instantly win the ball back. This also seems to be the reason for one of the most often criticised occurrences, when the ball will occasionally pass through a player’s leg. This was criticised and branded unrealistic at the time – particularly by fans of the rival &lt;em&gt;FIFA&lt;/em&gt; series – but it’s really Konami rolling the D20 once more, and the attacker’s ability and luck totalling more than the defender’s (or vice versa). It can be visually jarring, but it’s the kind of nuanced feature that keeps the game flowing.&lt;/p&gt;

&lt;h4 id=&quot;straightforward-set-pieces&quot;&gt;Straightforward set pieces&lt;/h4&gt;

&lt;p&gt;Whenever a new iteration of a football game is at the drawing board, the developers need to make a decision on set pieces, and in the 2000s we saw a huge variety of systems employed. In my opinion, &lt;em&gt;PES 5&lt;/em&gt; strikes the balance better than any, again bringing player attributes to the forefront. Rather than a complicated system of giant arrows, ball contact diagrams, and &lt;a href=&quot;https://www.youtube.com/watch?v=cGd5qVouoCY&quot; target=&quot;_blank&quot;&gt;stop-the-slider mini games&lt;/a&gt;, Konami gave us three means of influencing free kicks: the initial angle, a power bar, and the application of curl with the D-pad. The result leaves the player feeling far more connected to the action – free kicks are fast, direct, and satisfying when they hit the top corner, and (as is the case with all good game design) when the player fails, they always feel as though they might do better next time with a slight change of technique.&lt;/p&gt;

&lt;p&gt;Penalties benefit from being even more straightforward. Both the player and the goalkeeper select one of nine directions – that’s it. No power bar, no stuttering run-ups or star jumps on the line, and no fancy chips down the centre. What happens next is a combination of luck and player attributes – even if the goalkeeper dives the right way, they might not keep the shot out if their skills don’t match the striker’s, and the taker always stands a chance of hitting the post or missing entirely. The way the outcome is calculated feels fair, balanced, and in sync with the rest of the match engine.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/01/pes-5-9.png&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;an-imperfect-masterpiece&quot;&gt;An imperfect masterpiece&lt;/h4&gt;

&lt;p&gt;All of this isn’t to say that &lt;em&gt;PES 5&lt;/em&gt;’s interpretation of football is perfect. Referees in particular could have used some refinement. After noticing them constantly blowing for soft fouls, sometimes nowhere near the ball, I initially held back on tackling, fearing that I’d gather bookings just as easily. But despite being massively strict on minor infringements, they seem to have no sense of whether a foul was committed by the last man. A deliberate but minor foul on a player through on goal will usually yield no card – the AI does it to me, I do it to the AI, and it essentially becomes another fair gameplay tactic.&lt;/p&gt;

&lt;p&gt;The advantage rule is also massively inconsistent, and is something modern football games undoubtedly do better. The main issue here is that the period that the referee plays advantage for tends to be very short. If an opposing player fouls my attacker and the ball happens to fall to his strike partner, the advantage icon disappears almost as soon as he controls it, meaning no free kick is awarded even if he is being tightly marked and is immediately dispossessed. There are also many times where the referee stops play when playing on would have given the attacking team a chance on goal. The advantage rule was a feature introduced in &lt;em&gt;PES 3&lt;/em&gt;, but even two editions later it was in desperate need of improvement.&lt;/p&gt;

&lt;p&gt;Finally, despite the flowing gameplay and organic deflections, the players themselves aren’t always the quickest to react to unexpected passages of play. Generally this isn’t an issue, but it can be frustrating to see a deflected clearance apparently fall into your star forward’s path, only for him to refuse to approach the ball, letting a more distant defender get there first and clean up the mess.&lt;/p&gt;

&lt;h3 id=&quot;part-iii-the-future-of-gaming&quot;&gt;Part III: The future of gaming&lt;/h3&gt;

&lt;p&gt;So, what does this analysis of a 15-year-old football game tell us about video games more broadly? I think the key message is that a compromise between realism and gameplay, which used to be forced upon developers by the PlayStation 2’s limited hardware, is essential to creating a more playable game. The conversation around sports games always seems to be around which title more closely resembles the real game, but perhaps the focus would be better placed on which is more fun instead.&lt;/p&gt;

&lt;p&gt;While this post has examined &lt;em&gt;PES 5&lt;/em&gt;, the same is true of games in other genres. Looking at another Konami series, &lt;em&gt;Metal Gear Solid&lt;/em&gt; evolved in a similar way. &lt;em&gt;Metal Gear Solid 2&lt;/em&gt; and &lt;em&gt;3&lt;/em&gt; were arguably the series’ peak, when gameplay was constrained to small areas that presented controlled, isolated challenges, and a large amount of effort was spent adding colour, story, and character. Fast forward to &lt;em&gt;Metal Gear Solid 5&lt;/em&gt; and emphasis is squarely on open-world gameplay, with few cut scenes and little memorable dialogue. Anybody from the mid-2000s would be stunned at the gameplay features, but the game loses its focus, both thematically and in terms of the gameplay – there are any number of situations the developers did not account for where the player can engineer situations that &lt;a href=&quot;https://www.youtube.com/watch?v=uxjnMR2CupA&quot; target=&quot;_blank&quot;&gt;confuse the AI&lt;/a&gt;, either exposing bugs (and breaking immersion) or lowering the difficulty. It is a game in desperate need of focus, both in terms of the player experience and where development time was allocated.&lt;/p&gt;

&lt;h4 id=&quot;gameplay-over-realism&quot;&gt;Gameplay over realism&lt;/h4&gt;

&lt;p&gt;A niche more commonly explored by mid-level and indie developers is the game that has fairly simple mechanics (or at least features that have been relatively stable for years) and modern graphics. This section of the gaming market is wide-ranging, but I would point to examples like &lt;em&gt;Football Manager&lt;/em&gt;, the rebooted &lt;em&gt;Hitman series&lt;/em&gt;, and the modern &lt;em&gt;Wolfenstein&lt;/em&gt; and &lt;em&gt;Doom&lt;/em&gt; games. These games follow almost the same core gameplay formula that similar titles did 15 years ago, but the developers have used the extra power at their disposal to make them prettier, refine mechanics, and improve their AI, rather than adding additional complexity unnecessarily or targeting hyperrealism at the expense of player enjoyment.&lt;/p&gt;

&lt;p&gt;In the football genre in particular, I suspect there are many disenfranchised players out there who like me would jump at the chance to play a game similar to the classic &lt;em&gt;PES&lt;/em&gt; titles, but with minor improvements (for example, updated graphics, better referees, and more consistent player control) rather than additional features that increase realism but make the game more of a chore to play (slower, heavier gameplay; player complaints and conversations, press conferences and negotiation cut scenes).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PES&lt;/em&gt; has improved somewhat during the PS4/Xbox One generation, but it definitely isn’t as fast or addictive as it was previously, and the Master League has been diluted massively in pursuit of &lt;em&gt;FIFA&lt;/em&gt;’s Career Mode. With the focus on cash-generating online services like FIFA Ultimate Team and MyClub, gameplay has remained relatively consistent for a while now, but perhaps there’s a small developer out there that could make take a chance on a simpler, more arcadey title that resembles a refined classic &lt;em&gt;PES&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Until then, I’m hooked once more on the classic &lt;em&gt;Pro Evolution Soccer&lt;/em&gt; formula, and at least for the foreseeable future, &lt;em&gt;PES 5&lt;/em&gt; is going to remain my football game of choice.&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">I have a rare off-topic post for you this month – something of an essay about the structure of the classic Pro Evolution Soccer Master League, the differences in its modern-day equivalents, and a discussion about the delicate balance between realism and gameplay in video games more widely. Design in football games doesn’t get much attention, as the focus is normally on comparisons to the real game, so I thought this would be an interesting area to explore in a little more detail than I’ve seen before. Pro Evolution Soccer 5 has always been an all-time favourite of mine, but it is only now, 15 years after its initial release, that I have found the inspiration to do it justice in words. Of course, there are many reviews out there from 2005 that cover its gameplay, features, and so on, but in this post I have attempted to capture what made it so perfectly balanced and special to many fans. I hope that at the very least this will stir up some nostalgia that inspire others to dig out their copies for a match or two. This isn’t something I’ve ever done before on this blog – but don’t worry, I won’t be doing this all the time and my focus will still be cyber security! I just might dig up a retro game or two from time to time for and write a bit of a deep dive when the inspiration strikes me. Like everyone else, I didn’t have a normal Christmas holiday this year. In my efforts to stave off boredom in the gap between Christmas and the new year, I installed an emulator and grabbed a copy of Pro Evolution Soccer 5 to play. I wasted a huge number of hours playing this game as a teenager, and wanted to see how it held up 15 years and three console generations later. To my surprise, I once again became addicted, and played Master League matches on a daily basis for the rest of the break. There is some nostalgia involved here, of course – both for the game and the 2005/06 football season – but there are so many things that should have driven me away: the graphics are rough round the edges, the game mechanics are unrefined, and the emulator stutters and occasionally refuses to let me run right until I reconnect my controller. With all these inconveniences, what kept me playing? And why didn’t I fire up my PlayStation 4 or PC and play one of the football games from 2020 instead? I believe the answer says a lot about design, both in football games and video games in general. I’d always considered PES 5, or more generally the period between Pro Evolution Soccer 4 and Pro Evolution Soccer 6 on PlayStation 2 and Xbox, to be a high tide mark for football games. But even with this in mind when playing more modern games, I could never really pinpoint why. Returning to PES 5 in 2020 with fresh eyes has given me the perspective I needed to put this X factor into words. Part I: The Master League The reasons for PES 5’s superiority lie both on and off the pitch. I’ll begin with the latter, which I believe is a masterstroke that is key to the game’s overall addictiveness. The Master League is Konami’s answer to FIFA’s Career Mode – a multi-season campaign that puts the player in charge of a club with financial responsibilities and the freedom to make transfers. However, there are some important differences that lean away from realism and integrate systems more often seen in other video game genres. A sense of ownership The first important difference is that the Master League’s focus is on the club, not the manager’s career. The player can either select or create a team at the beginning of the Master League, and cannot switch to another team at any point. If they choose the latter option, they choose the team’s name, design kits (which integrate with the game world better than similar features in other games due to the range of design options and general lack of licences), and select other attributes like a stadium and crowd colours. The “real” way to play the Master League is to begin with the default squad, rather than the team’s real-life players. This team, which includes fictional players like Castolo, Minanda, Stremer, and Ivarov, is largely useless in terms of ability. They take touches metres ahead of themselves, hit shots wildly off-target unless directly facing the goal at walking pace, and are outmuscled by even the smallest opponents. This means there is a natural siege mentality for the first couple of seasons, and scraping a point feels like a victory. Unlike FIFA, where selecting Chelsea will mean Champions League matches in your first year in charge, it is not even possible to reach continental competition in the Master League until at least the third season, and even that is unlikely given the players initially at your disposal. This adds to the sense of progression as the player – and by extension their club – earns their place in the footballing world. All of this means the player has a far greater sense of ownership than they do in other games’ career modes, where they can take over a successful team of competent players assembled by a real-world manager. This is your club, and as the seasons progress and you slowly bring in talent during the transfer windows, each player in the squad reflects a decision you have made. It’s less realistic, but makes for a far more engaging game – something that will become a recurring theme through this post. A football RPG This culminates in a mode structured more like a football role-playing game (RPG) than a sports game. The squad is akin the player’s party, the matches are battles, and the time between is spent managing resources and organising the party in a way that increases the success rate, including keeping track of players’ development and signing new personnel where required. To help them on their way, PES 5 tracks and makes available quite a bit more data than even the most recent football games. An important aspect of this is that transfers are straightforward, and are far more about affordability than negotiation, which is rarely done well in games. In this respect, the player’s focus is always on their points balance (more on that later) and earning enough to upgrade. They aren’t forced to sit through lengthy office cut scenes, playing a guessing game with an inconsistent AI to agree on a price, and being unable to sign the best players even in the cash-rich end game because of their value to their current clubs. Scouting players to sign is also more rewarding than in modern FIFA or PES. Players in both current games have an Overall rating, which often reduces comparison to a single statistic and makes choosing between two players, whether for a transfer or a starting position, a one-dimensional affair. PES 5 does not have this statistic, and the player must instead scan each player’s full attributes to gain an impression of their ability. Overlapping star diagrams like the one pictured above allow the player to directly compare six attributes when switching players, sometimes forcing an informed decision – for example, do I choose a full-back who is pacey and attacking, or powerful and defensive? It also helps that the full list of attributes is smaller than in modern games, making it easier to review them in full. The exaggerated speed and visibility with which these attributes improve also adds to the RPG-like sense of narrative and progression. After each match, the player is taken to a screen where progression points for each attribute stack up for the players that took to the pitch, and it’s hugely satisfying to see your signings’ attributes increase. It’s difficult to compare them exactly, but I believe the progression in PES 5 is also quicker than in games like the modern FIFA series, where a player will join with an Overall rating in the 50s and take a decade to reach full potential. Once again, this is a compromise on realism for the sake of gameplay, increasing the investment that the player has in their squad and club. Extreme focus However, the most important part of the Master League, and I believe has been lost in every current career mode, is the fact that it keeps the player extremely focused on a simple task: survival and development. There is no fan sentiment to monitor, no board waiting to sack you, and no possibility of a move to another club. It’s you and your created team against the world, and if they fail, you may be forced to start another game (something that once happened to me, when I foolishly signed Steven Pienaar and saved myself into a corner, ensuring bankruptcy). The risk level is comparable to that of classic arcade games. Points are the currency in PES 5, and the only number that matters is your bank account balance. You earn points by getting results: 1,000 for a win and 500 for a draw, with bonus points for each goal scored. Your players are paid an annual salary, and if you don’t have enough points to cover the wage bill at the end of the season, it’s game over. Any leftover funds can be used to sign new players, leading to something of a gamble on a mental risk-reward calculation when signing players during the mid-season transfer window – will the added talent be enough to secure the results necessary to pay the wages at the end of the season? Or will your ambition be the cause of your downfall? The focus on points and the wage bill adds real tension to the Master League, which is something that isn’t recreated in career modes with separate wage budgets. The stakes of each individual match are raised – especially towards the end of a subpar season – and while the board won’t be on your back for being knocked out of the cup, the number of matches and therefore your chance of winning points is reduced. This can lead to an intense scramble to scrape results in the last few fixtures. The points system also has the added effect of tying Master League difficulty directly to match difficulty. A greater challenge on the pitch means a lower chance of amassing points and signing star players. As a result, the focus is always on the football and never on dialogue trees, objectives, or working out how to please unseen and unpredictable parties that could cut your tenure short. This makes sense in the big picture, because no PES or FIFA game will ever be Football Manager, so they shouldn’t try to be. Perhaps the Master League economy is a byproduct of development limitations, but accidental or not, it is a beautifully concise system that could be discussed alongside some of history’s best gameplay loops. Part II: Core gameplay But what of the gameplay on the pitch, where the player spends most of their time? I remember marvelling at the realism of PES 5 back in 2005, but in retrospect it is nothing of the sort. Quick passes fly about effortlessly, players’ movements follow robotic paths, and pinball-like deflections between bodies produce some truly comedic spells of play. These issues may have been the due to the hardware of the time – there’s only so much Konami could do with physics and AI on a PlayStation 2, after all - but these compromises in the representation of real-life football also serve to make the game more fun. The beautiful game No football game before PES 5 – and arguably no game since – flows as fantastically as Konami’s masterpiece. Its gameplay mechanics are designed for fast, end-to-end action, with passes pinging between players with pinpoint accuracy, but ball control that is heavy enough that any attempt to break into a packed penalty area carries a real risk of losing possession without creating an opportunity. This means shooting from distance is just as viable an option, and it is possible to score some real 30-yard screamers with the more talented players of the time like Frank Lampard and Steven Gerrard. The game’s attacking balance is unmatched – you’re just as likely to score or miss with a long shot, a run through the middle, or a cross into the box – and this variety is catalysed by the aforementioned element of randomness. All those different shot types, deflections, mistakes, and rebounds add an organic unpredictability that means that even after hundreds of hours of play, players still see new situations emerge. This stops attacking and defending, even against the AI, from falling into the predictable patterns seen in many football games (like some FIFA editions’ six-yard box square-pass-and-shoot goals). AI and momentum It’s also worth taking a moment to discuss the AI, which you’ll be seeing a lot of if you choose to play the Master League mode. Simply put, PES 5’s AI is a bigger challenge than any modern football game’s equivalent. Three seasons into my new campaign, I was just about winning consistently on four stars, and that still left me with the five-star mode (and the six-star mode, unlockable via the PES Shop) to take on. I believe a lack of predictability is again what makes the AI work so well – it seems to have a random element, and also encounters all the scraps and deflections that you do, which stops the player from learning that opponents will always pass in a certain direction or pattern. Perhaps this generation of football games was also slightly easier to develop AI for, in that the game engine itself was simpler, and there were fewer mechanics for the AI to have to interpret and choose between. PES 5 matches also benefit from being left to run their course naturally. In 2021, football game forums are rife with complaints about alleged “momentum” mechanics – the idea that the game “wants” a certain result for entertainment value, and makes a losing team more likely to score a last-minute equaliser or pits a dominant player’s championship challengers against a team fighting relegation who seem to gain superhuman powers if the player has won too many matches recently. While I remember reading some initial murmurings about momentum at the time, PES 5 seems to largely let the game flow, free from artficial drama. The bulk of the “momentum” you’ll find here is in the commentary (particularly from Trevor Brooking, who seems obsessed with the word, especially following goals). Party on the pitch Earlier, I discussed the Master League mode’s RPG-like elements, and these are also present as you take to the pitch, with the game adjusting noticeably to players’ attributes and engineering situations for them to take effect. Starting a player with less fatigue and better form is often more beneficial than starting a jaded superstar, and some abilities like the famous lofted through ball are essentially locked off until you have skilled enough players. These limitations initially led me to believe the game was much more basic than I remembered when I first picked it up, but as I acquired more talented players and began to see the effects on the pitch, I began to realise that this was just another facet of its complexity. In a similar way, a game of PES 5 includes several subtle, and sometimes slightly frustrating, gameplay features that digress from real football in the name of playability. Low-skill players take huge touches when controlling even the simplest pass, for example, and players almost always stumble when tackled, making it impossible to instantly win the ball back. This also seems to be the reason for one of the most often criticised occurrences, when the ball will occasionally pass through a player’s leg. This was criticised and branded unrealistic at the time – particularly by fans of the rival FIFA series – but it’s really Konami rolling the D20 once more, and the attacker’s ability and luck totalling more than the defender’s (or vice versa). It can be visually jarring, but it’s the kind of nuanced feature that keeps the game flowing. Straightforward set pieces Whenever a new iteration of a football game is at the drawing board, the developers need to make a decision on set pieces, and in the 2000s we saw a huge variety of systems employed. In my opinion, PES 5 strikes the balance better than any, again bringing player attributes to the forefront. Rather than a complicated system of giant arrows, ball contact diagrams, and stop-the-slider mini games, Konami gave us three means of influencing free kicks: the initial angle, a power bar, and the application of curl with the D-pad. The result leaves the player feeling far more connected to the action – free kicks are fast, direct, and satisfying when they hit the top corner, and (as is the case with all good game design) when the player fails, they always feel as though they might do better next time with a slight change of technique. Penalties benefit from being even more straightforward. Both the player and the goalkeeper select one of nine directions – that’s it. No power bar, no stuttering run-ups or star jumps on the line, and no fancy chips down the centre. What happens next is a combination of luck and player attributes – even if the goalkeeper dives the right way, they might not keep the shot out if their skills don’t match the striker’s, and the taker always stands a chance of hitting the post or missing entirely. The way the outcome is calculated feels fair, balanced, and in sync with the rest of the match engine. An imperfect masterpiece All of this isn’t to say that PES 5’s interpretation of football is perfect. Referees in particular could have used some refinement. After noticing them constantly blowing for soft fouls, sometimes nowhere near the ball, I initially held back on tackling, fearing that I’d gather bookings just as easily. But despite being massively strict on minor infringements, they seem to have no sense of whether a foul was committed by the last man. A deliberate but minor foul on a player through on goal will usually yield no card – the AI does it to me, I do it to the AI, and it essentially becomes another fair gameplay tactic. The advantage rule is also massively inconsistent, and is something modern football games undoubtedly do better. The main issue here is that the period that the referee plays advantage for tends to be very short. If an opposing player fouls my attacker and the ball happens to fall to his strike partner, the advantage icon disappears almost as soon as he controls it, meaning no free kick is awarded even if he is being tightly marked and is immediately dispossessed. There are also many times where the referee stops play when playing on would have given the attacking team a chance on goal. The advantage rule was a feature introduced in PES 3, but even two editions later it was in desperate need of improvement. Finally, despite the flowing gameplay and organic deflections, the players themselves aren’t always the quickest to react to unexpected passages of play. Generally this isn’t an issue, but it can be frustrating to see a deflected clearance apparently fall into your star forward’s path, only for him to refuse to approach the ball, letting a more distant defender get there first and clean up the mess. Part III: The future of gaming So, what does this analysis of a 15-year-old football game tell us about video games more broadly? I think the key message is that a compromise between realism and gameplay, which used to be forced upon developers by the PlayStation 2’s limited hardware, is essential to creating a more playable game. The conversation around sports games always seems to be around which title more closely resembles the real game, but perhaps the focus would be better placed on which is more fun instead. While this post has examined PES 5, the same is true of games in other genres. Looking at another Konami series, Metal Gear Solid evolved in a similar way. Metal Gear Solid 2 and 3 were arguably the series’ peak, when gameplay was constrained to small areas that presented controlled, isolated challenges, and a large amount of effort was spent adding colour, story, and character. Fast forward to Metal Gear Solid 5 and emphasis is squarely on open-world gameplay, with few cut scenes and little memorable dialogue. Anybody from the mid-2000s would be stunned at the gameplay features, but the game loses its focus, both thematically and in terms of the gameplay – there are any number of situations the developers did not account for where the player can engineer situations that confuse the AI, either exposing bugs (and breaking immersion) or lowering the difficulty. It is a game in desperate need of focus, both in terms of the player experience and where development time was allocated. Gameplay over realism A niche more commonly explored by mid-level and indie developers is the game that has fairly simple mechanics (or at least features that have been relatively stable for years) and modern graphics. This section of the gaming market is wide-ranging, but I would point to examples like Football Manager, the rebooted Hitman series, and the modern Wolfenstein and Doom games. These games follow almost the same core gameplay formula that similar titles did 15 years ago, but the developers have used the extra power at their disposal to make them prettier, refine mechanics, and improve their AI, rather than adding additional complexity unnecessarily or targeting hyperrealism at the expense of player enjoyment. In the football genre in particular, I suspect there are many disenfranchised players out there who like me would jump at the chance to play a game similar to the classic PES titles, but with minor improvements (for example, updated graphics, better referees, and more consistent player control) rather than additional features that increase realism but make the game more of a chore to play (slower, heavier gameplay; player complaints and conversations, press conferences and negotiation cut scenes). PES has improved somewhat during the PS4/Xbox One generation, but it definitely isn’t as fast or addictive as it was previously, and the Master League has been diluted massively in pursuit of FIFA’s Career Mode. With the focus on cash-generating online services like FIFA Ultimate Team and MyClub, gameplay has remained relatively consistent for a while now, but perhaps there’s a small developer out there that could make take a chance on a simpler, more arcadey title that resembles a refined classic PES. Until then, I’m hooked once more on the classic Pro Evolution Soccer formula, and at least for the foreseeable future, PES 5 is going to remain my football game of choice.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mattcasmith.net/wp-content/uploads/2021/01/pes-5-1.png" /><media:content medium="image" url="https://mattcasmith.net/wp-content/uploads/2021/01/pes-5-1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introducing Backutil: A Python‐based Windows backup utility</title><link href="https://mattcasmith.net/2021/01/01/backutil-windows-backup-utility" rel="alternate" type="text/html" title="Introducing Backutil&amp;#58; A Python&amp;#8208;based Windows backup utility" /><published>2021-01-01T00:00:00+00:00</published><updated>2021-01-01T00:00:00+00:00</updated><id>https://mattcasmith.net/2021/01/01/backutil-python-windows-backup-utility</id><content type="html" xml:base="https://mattcasmith.net/2021/01/01/backutil-windows-backup-utility">&lt;p&gt;Back in the spring, I decided that 2020 would be the year I would finally see a coding project through to completion. A recent work project shone a light on backup and recovery, and I realised that I should probably be a bit more consistent with my own backups from my personal PC. Wanting to avoid paying &lt;em&gt;another&lt;/em&gt; annual subscription, I decided to write a script myself. Thus Backutil was born - and the project only grew from there as I added more features along the way.&lt;/p&gt;

&lt;p&gt;I’m still not quite at the point when I’m ready to release a v1.0, but I told myself a few months ago that I wanted to put together a minimum viable product by the new year - so here it is! It has a few bugs and is missing a couple of features, but Backutil is now a functioning Python-based utility for backing up files on Windows systems, complete with options for incremental backups and backup rotation.&lt;/p&gt;

&lt;h3 id=&quot;contents&quot;&gt;Contents&lt;/h3&gt;

&lt;p&gt;1. &lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;br /&gt;
          a. &lt;a href=&quot;#testing-and-limitations&quot;&gt;Testing and limitations&lt;/a&gt;&lt;br /&gt;
2. &lt;a href=&quot;#configuration&quot;&gt;Configuration&lt;/a&gt;&lt;br /&gt;
          a. &lt;a href=&quot;#configuration-file&quot;&gt;Configuration file&lt;/a&gt;&lt;br /&gt;
          b. &lt;a href=&quot;#backup-list-file&quot;&gt;Backup list file&lt;/a&gt;&lt;br /&gt;
          a. &lt;a href=&quot;#command-line-options&quot;&gt;Command line options&lt;/a&gt;&lt;br /&gt;
3. &lt;a href=&quot;#download&quot;&gt;Download&lt;/a&gt;&lt;br /&gt;
4. &lt;a href=&quot;#changelog&quot;&gt;Changelog&lt;/a&gt;&lt;br /&gt;
5. &lt;a href=&quot;#future-development&quot;&gt;Future development&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Backutil is a simple, Python-based utility for backing up files from Windows systems to compressed, password-protected local archives. It has features for performing incremental backups and automatically rotating backup files. This is achieved using &lt;code&gt;robocopy&lt;/code&gt; and 7-Zip, which must be installed.&lt;/p&gt;

&lt;p style=&quot;float: left; width: 90%; padding: 5px; padding-left: 15px; padding-right: 5%; margin: 30px 0 10px 0; font-style: italic; border-left: 5px solid red; background: #ff9999; color: #333;&quot;&gt;Backutil is a learning/hobby project and some aspects of its code may not follow best practices. While you're welcome to use it, you do so at your own risk. Make sure you take a manual backup of your files before trying it out, and don't go relying on it to back up your production servers.&lt;/p&gt;

&lt;p&gt;To back up your files, simply ensure you have configured Backutil (see the sections below) and run &lt;code&gt;backutil.exe&lt;/code&gt; from the Command Prompt or PowerShell. The utility will report on its progress until the backup is successfully completed. More detail can also be found in &lt;code&gt;backutil_log.csv&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2020/12/backutil-1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When the utility is finished, you should find your complete backup files in your designated backup folder. The number and size of these backup files can be configured using the incremental backup and rotation settings, which are set in the configuration file or as command line options.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2020/12/backutil-2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As Backutil automatically manages your backup files, it can be configured to run automatically at the desired interval using the Windows Task Scheduler. Backutil’s features can be used to generate rolling full or incremental backups as required by your backup objectives and disk size.&lt;/p&gt;

&lt;h4 id=&quot;testing-and-limitations&quot;&gt;Testing and limitations&lt;/h4&gt;

&lt;p&gt;Aside from all the testing that comes naturally during the development process, I have been using Backutil to back up my personal files for the last few months using a Windows scheduled task to run the utility on a weekly basis. My configuration performs incremental backups on a five-file rotation and so far has worked without a hitch, to a level where I occasionally even forgot it was running.&lt;/p&gt;

&lt;p&gt;One slight limitation, which will be improved with &lt;a href=&quot;#future-development&quot;&gt;future development&lt;/a&gt;, is the speed of the backup process. My current backups include around 83GB of data (about 50GB once compressed), and the initial “big” backup can take a couple of hours to run. For this reason, I recommend using Backutil to back up a focused set of directories rather than your whole hard drive, at least for the moment.&lt;/p&gt;

&lt;h3 id=&quot;configuration&quot;&gt;Configuration&lt;/h3&gt;

&lt;p&gt;Backutil can be configured via three main means: a configuration file, a file containing a list of directories to be backed up, and a series of command line options that override other settings. If a configuration file and backup list file are present, Backutil can be run using the following simple command.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;backutil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exe&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In terms of an installation directory, I put the executable and configuration files in &lt;code&gt;C:\backutil\bin\&lt;/code&gt; and use &lt;code&gt;C:\backutil\&lt;/code&gt; as the staging folder for the temporary files and records. However, you can put these files wherever you like as long as your settings are configured accordingly.&lt;/p&gt;

&lt;h4 id=&quot;configuration-file&quot;&gt;Configuration file&lt;/h4&gt;

&lt;p&gt;Backutil automatically loads settings from a file named &lt;code&gt;config.ini&lt;/code&gt;, including the location of the list of directories to back up, folders for backups and temporary files, and incremental backup and rotation options. The configuration file should be located in the same folder as &lt;code&gt;backutil.exe&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The contents of an example configuration file are shown below.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LOCAL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;computer_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pc&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;backup_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;txt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;staging_folder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;backutil&lt;/span&gt;\
&lt;span class=&quot;n&quot;&gt;archive_pass&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;supersecretpassword&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;incremental&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rotation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;retained&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SERVER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;server_directory&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;backups&lt;/span&gt;\&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The table below sets out what each option in the &lt;code&gt;config.ini&lt;/code&gt; configuration file does. Note that all directories supplied via the configuration file must include the trailing backslash.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Section&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Key&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Purpose&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCAL&lt;/td&gt;
      &lt;td&gt;computer_name&lt;/td&gt;
      &lt;td&gt;Sets backup folder/record name&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCAL&lt;/td&gt;
      &lt;td&gt;backup_list&lt;/td&gt;
      &lt;td&gt;Sets the backup list filename&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCAL&lt;/td&gt;
      &lt;td&gt;staging_folder&lt;/td&gt;
      &lt;td&gt;Sets folder for temporary file storage&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCAL&lt;/td&gt;
      &lt;td&gt;archive_pass&lt;/td&gt;
      &lt;td&gt;Sets 7-Zip backup file password&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCAL&lt;/td&gt;
      &lt;td&gt;incremental&lt;/td&gt;
      &lt;td&gt;Turns incremental backups on/off (True/False)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCAL&lt;/td&gt;
      &lt;td&gt;rotation&lt;/td&gt;
      &lt;td&gt;Turns backup rotation on/off (True/False)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCAL&lt;/td&gt;
      &lt;td&gt;retained&lt;/td&gt;
      &lt;td&gt;Sets number of backups to retain if rotation is on&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SERVER&lt;/td&gt;
      &lt;td&gt;server_directory&lt;/td&gt;
      &lt;td&gt;Sets folder for backup storage&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;backup-list-file&quot;&gt;Backup list file&lt;/h4&gt;

&lt;p&gt;The backup list file is a text file containing a list of directories. When Backutil is run, it will automatically generate a list of files to back up by scanning the contents of these directories and all subdirectories. The format of the backup list file should look something like the example below.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Desktop&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Downloads&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Music&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iTunes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iTunes&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Media&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Music&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pictures&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Videos&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;command-line-options&quot;&gt;Command line options&lt;/h4&gt;

&lt;p&gt;Backutil also supports several options if you wish to set certain configuration parameters manually from the Command Prompt or PowerShell. Note that any parameters set via command line options will override the respective parameters in the &lt;code&gt;config.ini&lt;/code&gt; configuration file.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Short&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Long&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Purpose&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-h&lt;/td&gt;
      &lt;td&gt;--help&lt;/td&gt;
      &lt;td&gt;Displays the help file&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-n &amp;lt;name&amp;gt;&lt;/td&gt;
      &lt;td&gt;--name &amp;lt;name&amp;gt;&lt;/td&gt;
      &lt;td&gt;Manually sets the backup folder/record name&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-l &amp;lt;file&amp;gt;&lt;/td&gt;
      &lt;td&gt;--list &amp;lt;file&amp;gt;&lt;/td&gt;
      &lt;td&gt;Manually sets the backup list file&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-i&lt;/td&gt;
      &lt;td&gt;--incremental&lt;/td&gt;
      &lt;td&gt;Manually turns on incremental backups&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-r &amp;lt;no&amp;gt;&lt;/td&gt;
      &lt;td&gt;--rotate &amp;lt;no&amp;gt;&lt;/td&gt;
      &lt;td&gt;Manually turns on backup rotation and sets number of backups&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The following command shows an example of how the command line options may be used.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;backutil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exe&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;locations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;txt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Running Backutil with the options above will save backup files to a folder called &lt;code&gt;matts-pc&lt;/code&gt; (note that this folder name is also how previous backups are tracked). The list of directories to back up files from will be retrieved from &lt;code&gt;locations.txt&lt;/code&gt;. Backups will be incremental (only changed files will be backed up each time Backutil runs) and five previous backups will be retained.&lt;/p&gt;

&lt;h3 id=&quot;download&quot;&gt;Download&lt;/h3&gt;

&lt;p&gt;Use the link below to download Backutil. You’re free to run it for personal use - just please let me know if you encounter any bugs so I can work on fixing them in future realeases!&lt;/p&gt;

&lt;p style=&quot;float: left; width: 90%; padding: 5px; padding-left: 15px; padding-right: 5%; margin: 30px 0 10px 0; font-style: italic; border-left: 5px solid red; background: #ff9999; color: #333;&quot;&gt;Backutil is a learning/hobby project and some aspects of its code may not follow best practices. While you're welcome to use it, you do so at your own risk. Make sure you take a manual backup of your files before trying it out, and don't go relying on it to back up your production servers.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/mattcasmith/backutil/raw/main/dist/backutil_v0.61.zip&quot;&gt;&lt;img src=&quot;/assets/images/download.png&quot; style=&quot;width: 50px&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/mattcasmith/backutil/raw/main/dist/backutil_v0.61.zip&quot;&gt;Download Backutil v0.61&lt;/a&gt;&lt;br /&gt;7.5MB, ZIP&lt;/td&gt;
      &lt;td&gt;The downloadable archive contains &lt;code&gt;backutil.exe&lt;/code&gt; and an example &lt;code&gt;config.ini&lt;/code&gt; file. Interested in the source code? The full Python script is available in &lt;a target=&quot;_blank&quot; href=&quot;https://github.com/mattcasmith/backutil&quot;&gt;the Backutil GitHub repository&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;changelog&quot;&gt;Changelog&lt;/h3&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Date&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Version&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Changes&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;26/03/2021&lt;/td&gt;
      &lt;td&gt;v0.61&lt;/td&gt;
      &lt;td&gt;Implemented SQLite and other speed improvements:&lt;br /&gt;- All data processed using SQLite&lt;br /&gt;- Hashes generated using bigger file chunks&lt;br /&gt;- File size cut by 80 per cent due to Pandas removal&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;19/02/2021&lt;/td&gt;
      &lt;td&gt;v0.52&lt;/td&gt;
      &lt;td&gt;Small bug fixes and improvements from v0.51:&lt;br /&gt;- 7-Zip file now generated directly in destination folder&lt;br /&gt;- Hash file now only generated after successful backup&lt;br /&gt;- Blank line at end of backup list file no longer required&lt;br /&gt;- Help page consistent with online documentation&lt;br /&gt;- Fixed –help and –incremental arguments&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;future-development&quot;&gt;Future development&lt;/h3&gt;

&lt;p&gt;My determination to build a minimum viable product before the end of 2020 means that I have a backlog of bug fixes and new features to add during 2021. These include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Speed/efficiency improvements&lt;/strong&gt; - As it stands, Backutil generates hashes and copies files via some fairly simple logic. As a next step I hope to implement a multithreading solution to process multiple files at once and reduce the time taken to perform each backup.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Remote backups&lt;/strong&gt; - You’ll notice that some parts of Backutil use terminology associated with remote backups (for example, the Server section in the configuration file). This is because Backutil could originally be configured to use WinSCP to send backup files to a remote server. This has been removed for the initial release, but I hope to reinstate it in a future version.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Graphical user interface (GUI)&lt;/strong&gt; - I’ve played around with Python GUIs a couple of times before, but have never had a script worth implementing one for. Depending on time limitations, I might develop a GUI for Backutil to increase ease of use for less experienced users.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Inconsistencies and bug fixes&lt;/strong&gt; - The more time you spend with a piece of code, the more flaws you find in it. I’m sure I’ll spot plenty to fix along the way.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Have you got more ideas for new Backutil features? Or have you found bugs that I haven’t? Please &lt;a href=&quot;mailto:mattcasmith@protonmail.com&quot;&gt;send me an email&lt;/a&gt; to let me know so I can add them to the development backlog.&lt;/p&gt;

&lt;p&gt;If you’re interested in the project, check back regularly for new releases. I’ll also announce any updates on &lt;a target=&quot;_blank&quot; href=&quot;https://twitter.com/mattcasmith&quot;&gt;my Twitter account&lt;/a&gt;, and may add some form of banner to &lt;a href=&quot;https://mattcasmith.net&quot;&gt;my site’s homepage&lt;/a&gt;.&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">Back in the spring, I decided that 2020 would be the year I would finally see a coding project through to completion. A recent work project shone a light on backup and recovery, and I realised that I should probably be a bit more consistent with my own backups from my personal PC. Wanting to avoid paying another annual subscription, I decided to write a script myself. Thus Backutil was born - and the project only grew from there as I added more features along the way. I’m still not quite at the point when I’m ready to release a v1.0, but I told myself a few months ago that I wanted to put together a minimum viable product by the new year - so here it is! It has a few bugs and is missing a couple of features, but Backutil is now a functioning Python-based utility for backing up files on Windows systems, complete with options for incremental backups and backup rotation. Contents 1. Introduction           a. Testing and limitations 2. Configuration           a. Configuration file           b. Backup list file           a. Command line options 3. Download 4. Changelog 5. Future development Introduction Backutil is a simple, Python-based utility for backing up files from Windows systems to compressed, password-protected local archives. It has features for performing incremental backups and automatically rotating backup files. This is achieved using robocopy and 7-Zip, which must be installed. Backutil is a learning/hobby project and some aspects of its code may not follow best practices. While you're welcome to use it, you do so at your own risk. Make sure you take a manual backup of your files before trying it out, and don't go relying on it to back up your production servers. To back up your files, simply ensure you have configured Backutil (see the sections below) and run backutil.exe from the Command Prompt or PowerShell. The utility will report on its progress until the backup is successfully completed. More detail can also be found in backutil_log.csv. When the utility is finished, you should find your complete backup files in your designated backup folder. The number and size of these backup files can be configured using the incremental backup and rotation settings, which are set in the configuration file or as command line options. As Backutil automatically manages your backup files, it can be configured to run automatically at the desired interval using the Windows Task Scheduler. Backutil’s features can be used to generate rolling full or incremental backups as required by your backup objectives and disk size. Testing and limitations Aside from all the testing that comes naturally during the development process, I have been using Backutil to back up my personal files for the last few months using a Windows scheduled task to run the utility on a weekly basis. My configuration performs incremental backups on a five-file rotation and so far has worked without a hitch, to a level where I occasionally even forgot it was running. One slight limitation, which will be improved with future development, is the speed of the backup process. My current backups include around 83GB of data (about 50GB once compressed), and the initial “big” backup can take a couple of hours to run. For this reason, I recommend using Backutil to back up a focused set of directories rather than your whole hard drive, at least for the moment. Configuration Backutil can be configured via three main means: a configuration file, a file containing a list of directories to be backed up, and a series of command line options that override other settings. If a configuration file and backup list file are present, Backutil can be run using the following simple command. .\backutil.exe In terms of an installation directory, I put the executable and configuration files in C:\backutil\bin\ and use C:\backutil\ as the staging folder for the temporary files and records. However, you can put these files wherever you like as long as your settings are configured accordingly. Configuration file Backutil automatically loads settings from a file named config.ini, including the location of the list of directories to back up, folders for backups and temporary files, and incremental backup and rotation options. The configuration file should be located in the same folder as backutil.exe. The contents of an example configuration file are shown below. [LOCAL] computer_name = matts-pc backup_list = backup-list.txt staging_folder = C:\backutil\ archive_pass = supersecretpassword incremental = True rotation = True retained = 5 [SERVER] server_directory = D:\backups\ The table below sets out what each option in the config.ini configuration file does. Note that all directories supplied via the configuration file must include the trailing backslash. Section Key Purpose LOCAL computer_name Sets backup folder/record name LOCAL backup_list Sets the backup list filename LOCAL staging_folder Sets folder for temporary file storage LOCAL archive_pass Sets 7-Zip backup file password LOCAL incremental Turns incremental backups on/off (True/False) LOCAL rotation Turns backup rotation on/off (True/False) LOCAL retained Sets number of backups to retain if rotation is on SERVER server_directory Sets folder for backup storage Backup list file The backup list file is a text file containing a list of directories. When Backutil is run, it will automatically generate a list of files to back up by scanning the contents of these directories and all subdirectories. The format of the backup list file should look something like the example below. C:/Users/Matt/Desktop C:/Users/Matt/Downloads C:/Users/Matt/Music/iTunes/iTunes Media/Music C:/Users/Matt/Pictures C:/Users/Matt/Videos Command line options Backutil also supports several options if you wish to set certain configuration parameters manually from the Command Prompt or PowerShell. Note that any parameters set via command line options will override the respective parameters in the config.ini configuration file. Short Long Purpose -h --help Displays the help file -n &amp;lt;name&amp;gt; --name &amp;lt;name&amp;gt; Manually sets the backup folder/record name -l &amp;lt;file&amp;gt; --list &amp;lt;file&amp;gt; Manually sets the backup list file -i --incremental Manually turns on incremental backups -r &amp;lt;no&amp;gt; --rotate &amp;lt;no&amp;gt; Manually turns on backup rotation and sets number of backups The following command shows an example of how the command line options may be used. .\backutil.exe -n matts-pc -l locations.txt -i -r 5 Running Backutil with the options above will save backup files to a folder called matts-pc (note that this folder name is also how previous backups are tracked). The list of directories to back up files from will be retrieved from locations.txt. Backups will be incremental (only changed files will be backed up each time Backutil runs) and five previous backups will be retained. Download Use the link below to download Backutil. You’re free to run it for personal use - just please let me know if you encounter any bugs so I can work on fixing them in future realeases! Backutil is a learning/hobby project and some aspects of its code may not follow best practices. While you're welcome to use it, you do so at your own risk. Make sure you take a manual backup of your files before trying it out, and don't go relying on it to back up your production servers. Download Backutil v0.617.5MB, ZIP The downloadable archive contains backutil.exe and an example config.ini file. Interested in the source code? The full Python script is available in the Backutil GitHub repository. Changelog Date Version Changes 26/03/2021 v0.61 Implemented SQLite and other speed improvements:- All data processed using SQLite- Hashes generated using bigger file chunks- File size cut by 80 per cent due to Pandas removal 19/02/2021 v0.52 Small bug fixes and improvements from v0.51:- 7-Zip file now generated directly in destination folder- Hash file now only generated after successful backup- Blank line at end of backup list file no longer required- Help page consistent with online documentation- Fixed –help and –incremental arguments Future development My determination to build a minimum viable product before the end of 2020 means that I have a backlog of bug fixes and new features to add during 2021. These include: Speed/efficiency improvements - As it stands, Backutil generates hashes and copies files via some fairly simple logic. As a next step I hope to implement a multithreading solution to process multiple files at once and reduce the time taken to perform each backup. Remote backups - You’ll notice that some parts of Backutil use terminology associated with remote backups (for example, the Server section in the configuration file). This is because Backutil could originally be configured to use WinSCP to send backup files to a remote server. This has been removed for the initial release, but I hope to reinstate it in a future version. Graphical user interface (GUI) - I’ve played around with Python GUIs a couple of times before, but have never had a script worth implementing one for. Depending on time limitations, I might develop a GUI for Backutil to increase ease of use for less experienced users. Inconsistencies and bug fixes - The more time you spend with a piece of code, the more flaws you find in it. I’m sure I’ll spot plenty to fix along the way. Have you got more ideas for new Backutil features? Or have you found bugs that I haven’t? Please send me an email to let me know so I can add them to the development backlog. If you’re interested in the project, check back regularly for new releases. I’ll also announce any updates on my Twitter account, and may add some form of banner to my site’s homepage.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mattcasmith.net/wp-content/uploads/2020/12/backutil-1.png" /><media:content medium="image" url="https://mattcasmith.net/wp-content/uploads/2020/12/backutil-1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The best cyber security and technology books I read during 2020</title><link href="https://mattcasmith.net/2020/12/22/best-cyber-security-tech-books-2020" rel="alternate" type="text/html" title="The best cyber security and technology books I read during 2020" /><published>2020-12-22T00:00:00+00:00</published><updated>2020-12-22T00:00:00+00:00</updated><id>https://mattcasmith.net/2020/12/22/best-cyber-security-tech-books-2020</id><content type="html" xml:base="https://mattcasmith.net/2020/12/22/best-cyber-security-tech-books-2020">&lt;p&gt;One of the few upsides of the whole 2020 situation is that I’ve had a lot more time to read. Periods that I would usually have spent commuting, out with friends, or cramming in chores between getting home and going to bed became downtime that I could devote to good books. It was a small silver lining to a year that became something of an endurance test in staying at home and finding ways to amuse myself.&lt;/p&gt;

&lt;p&gt;I read more books in 2020 than I have since I was at college or university, so naturally, some of them were more notable, enjoyable, or informative than others. With that in mind, I thought I’d close off the year by sharing some of my favourite cyber security and tech titles. Also, it doesn’t look like my schedule will change any time soon, so if you have any suggestions for my 2021 reading list, please let me know!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2020/12/cybersecuritytechbooks2020.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;ghost-in-the-wires&quot;&gt;Ghost in the Wires&lt;/h3&gt;

&lt;p&gt;I’ll start with a book that should require little introduction for many of you, but had somehow passed me by until now. &lt;em&gt;Ghost in the Wires&lt;/em&gt; is the memoir of Kevin Mitnick, possibly the most famous hacker of all time. It tells the story of his time hacking various people and corporations for the sheer thrill of it and evading the American authorities attempting to track him down. This had been on my list for years, and I narrowly missed out on picking up a signed copy &lt;a href=&quot;https://mattcasmith.net/2019/08/26/im-back-def-con-inspired-hacking/&quot;&gt;at Black Hat last year&lt;/a&gt; (I actually passed Mitnick in a corridor shortly afterwards), but I finally got my hands on a copy in the summer.&lt;/p&gt;

&lt;p&gt;You’re unlikely to learn much from the book in a technical sense, partly because that’s not the point and partly because Mitnick’s tale takes place in the late 1980s and early 1990s, so many of the technologies and attacks mentioned are outdated. But &lt;em&gt;Ghost in the Wires&lt;/em&gt; is a real page-turner akin to many a fictional action thriller. It is rightly held as a classic of the genre, and should serve as a powerful reminder of the damage a skilled social engineer can cause to an unprepared organisation.&lt;/p&gt;

&lt;h3 id=&quot;chaos-monkeys&quot;&gt;Chaos Monkeys&lt;/h3&gt;

&lt;p&gt;Antonio Garcia Martinez’s humourous and often shocking account of his time on the Silicon Valley start-up scene is far less cyber-focused, but no less thrilling. It would be easy to compare this book to the HBO sitcom &lt;em&gt;Silicon Valley&lt;/em&gt;, but the difference is that everything in &lt;em&gt;Chaos Monkeys&lt;/em&gt; is true. It also includes rare insights into personal dealings with some of the most famous faces in tech.&lt;/p&gt;

&lt;p&gt;Martinez covers his journey from Goldman Sachs to his own start-up and eventually to Facebook, with many intriguing details along the way. Despite his stories of some of the more scandalous behaviour in the California bubble, it’s nearly impossible to read this without becoming inspired by the big dreams and hard work of tech founders, and &lt;a href=&quot;https://mattcasmith.net/2020/06/30/new-website-new-philosophy/&quot;&gt;it was even the indirect inspiration for the latest iteration of this blog&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;countdown-to-zero-day&quot;&gt;Countdown to Zero Day&lt;/h3&gt;

&lt;p&gt;As a former cyber security journalist, I know just how hard it is to combine ground-level technical detail, organisational and industry fallout, and developments in the global geopolitical landscape into a compelling story. That’s why what Kim Zetter has done here is an even bigger triumph. She tells the story of Stuxnet in a way that will satisfy industry veterans and casual observers alike, framing the campaign with detail that provides valuable context for subsequent events such as the NotPetya incident.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Countdown to Zero Day&lt;/em&gt; does a fantastic job of providing an account of the Stuxnet campaign at all levels, from the corridors of the US government and Department of Defense to the Iranian nuclear facilities it targeted and the researchers at companies like Symantec racing to decipher how the malware worked. While it doesn’t touch the low-level technical details (understandably - that would fill a book all to itself), Zetter’s account breaks down the core functionality of Stuxnet - much of which also applies more broadly to other malware variants - in a way that both techies and non-techies will understand.&lt;/p&gt;

&lt;h3 id=&quot;code-the-hidden-language-of-computer-hardware-and-software&quot;&gt;Code: The Hidden Language of Computer Hardware and Software&lt;/h3&gt;

&lt;p&gt;Have you ever wondered how computers actually use all those ones and zeroes to allow us to send emails, edit Word documents, and write new programs in C++? Charles Petzold’s book covers more ground than just about any I’ve ever read, beginning with relatively simple systems like Morse code and, over the course of fewer than 400 pages, building a theoretical telegraph system, processor, RAM, and eventually a computer akin to those we use every day. It really is quite staggering to think about.&lt;/p&gt;

&lt;p&gt;While the detail presented in &lt;em&gt;Code&lt;/em&gt; isn’t something most of us will consider in the course of our daily lives, it does help to dispel the “magic box” effect surrounding PCs and smartphones. For my colleagues in the cyber security industry, it also provides a great overview of the relationship between binary and machine language, assembly, and high- and low-level programming languages, as well as where data is stored in CPU registers and RAM when code is run - fundamentals for malware analysts and bug hunters.&lt;/p&gt;

&lt;h3 id=&quot;practical-packet-analysis&quot;&gt;Practical Packet Analysis&lt;/h3&gt;

&lt;p&gt;No Starch Press is always a good bet for solid technology and cyber security books, and &lt;a href=&quot;https://nostarch.com/&quot; target=&quot;_blank&quot;&gt;the publisher’s website&lt;/a&gt; often serves as my starting point when I’m looking for something technical to read. I read a couple of their titles this year, but the highlight has to be &lt;em&gt;Practical Packet Analysis&lt;/em&gt;, which I worked through during some leave in the summer. At face value, the book is billed as a guide to using Wireshark to solve network problems, but it actually covers a lot more than that.&lt;/p&gt;

&lt;p&gt;For anybody new to networking, I think the opening chapters provide about the clearest explanation of the OSI model, TCP/IP, and common protocols that you’ll find anywhere. This is followed by an extensive rundown of Wireshark’s interface and features, including a guide on where and how to capture network traffic, as well as a series of scenarios where packet analysis helps to diagnose network problems, complete with downloadable PCAP files to follow along and get some practice in.&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">One of the few upsides of the whole 2020 situation is that I’ve had a lot more time to read. Periods that I would usually have spent commuting, out with friends, or cramming in chores between getting home and going to bed became downtime that I could devote to good books. It was a small silver lining to a year that became something of an endurance test in staying at home and finding ways to amuse myself. I read more books in 2020 than I have since I was at college or university, so naturally, some of them were more notable, enjoyable, or informative than others. With that in mind, I thought I’d close off the year by sharing some of my favourite cyber security and tech titles. Also, it doesn’t look like my schedule will change any time soon, so if you have any suggestions for my 2021 reading list, please let me know! Ghost in the Wires I’ll start with a book that should require little introduction for many of you, but had somehow passed me by until now. Ghost in the Wires is the memoir of Kevin Mitnick, possibly the most famous hacker of all time. It tells the story of his time hacking various people and corporations for the sheer thrill of it and evading the American authorities attempting to track him down. This had been on my list for years, and I narrowly missed out on picking up a signed copy at Black Hat last year (I actually passed Mitnick in a corridor shortly afterwards), but I finally got my hands on a copy in the summer. You’re unlikely to learn much from the book in a technical sense, partly because that’s not the point and partly because Mitnick’s tale takes place in the late 1980s and early 1990s, so many of the technologies and attacks mentioned are outdated. But Ghost in the Wires is a real page-turner akin to many a fictional action thriller. It is rightly held as a classic of the genre, and should serve as a powerful reminder of the damage a skilled social engineer can cause to an unprepared organisation. Chaos Monkeys Antonio Garcia Martinez’s humourous and often shocking account of his time on the Silicon Valley start-up scene is far less cyber-focused, but no less thrilling. It would be easy to compare this book to the HBO sitcom Silicon Valley, but the difference is that everything in Chaos Monkeys is true. It also includes rare insights into personal dealings with some of the most famous faces in tech. Martinez covers his journey from Goldman Sachs to his own start-up and eventually to Facebook, with many intriguing details along the way. Despite his stories of some of the more scandalous behaviour in the California bubble, it’s nearly impossible to read this without becoming inspired by the big dreams and hard work of tech founders, and it was even the indirect inspiration for the latest iteration of this blog. Countdown to Zero Day As a former cyber security journalist, I know just how hard it is to combine ground-level technical detail, organisational and industry fallout, and developments in the global geopolitical landscape into a compelling story. That’s why what Kim Zetter has done here is an even bigger triumph. She tells the story of Stuxnet in a way that will satisfy industry veterans and casual observers alike, framing the campaign with detail that provides valuable context for subsequent events such as the NotPetya incident. Countdown to Zero Day does a fantastic job of providing an account of the Stuxnet campaign at all levels, from the corridors of the US government and Department of Defense to the Iranian nuclear facilities it targeted and the researchers at companies like Symantec racing to decipher how the malware worked. While it doesn’t touch the low-level technical details (understandably - that would fill a book all to itself), Zetter’s account breaks down the core functionality of Stuxnet - much of which also applies more broadly to other malware variants - in a way that both techies and non-techies will understand. Code: The Hidden Language of Computer Hardware and Software Have you ever wondered how computers actually use all those ones and zeroes to allow us to send emails, edit Word documents, and write new programs in C++? Charles Petzold’s book covers more ground than just about any I’ve ever read, beginning with relatively simple systems like Morse code and, over the course of fewer than 400 pages, building a theoretical telegraph system, processor, RAM, and eventually a computer akin to those we use every day. It really is quite staggering to think about. While the detail presented in Code isn’t something most of us will consider in the course of our daily lives, it does help to dispel the “magic box” effect surrounding PCs and smartphones. For my colleagues in the cyber security industry, it also provides a great overview of the relationship between binary and machine language, assembly, and high- and low-level programming languages, as well as where data is stored in CPU registers and RAM when code is run - fundamentals for malware analysts and bug hunters. Practical Packet Analysis No Starch Press is always a good bet for solid technology and cyber security books, and the publisher’s website often serves as my starting point when I’m looking for something technical to read. I read a couple of their titles this year, but the highlight has to be Practical Packet Analysis, which I worked through during some leave in the summer. At face value, the book is billed as a guide to using Wireshark to solve network problems, but it actually covers a lot more than that. For anybody new to networking, I think the opening chapters provide about the clearest explanation of the OSI model, TCP/IP, and common protocols that you’ll find anywhere. This is followed by an extensive rundown of Wireshark’s interface and features, including a guide on where and how to capture network traffic, as well as a series of scenarios where packet analysis helps to diagnose network problems, complete with downloadable PCAP files to follow along and get some practice in.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mattcasmith.net/wp-content/uploads/2020/12/cybersecuritytechbooks2020.jpg" /><media:content medium="image" url="https://mattcasmith.net/wp-content/uploads/2020/12/cybersecuritytechbooks2020.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AWS: Deploying and connecting to a SQL database in the cloud</title><link href="https://mattcasmith.net/2020/11/15/aws-deploy-connect-sql-database-cloud" rel="alternate" type="text/html" title="AWS&amp;#58; Deploying and connecting to a SQL database in the cloud" /><published>2020-11-20T00:00:00+00:00</published><updated>2020-11-20T00:00:00+00:00</updated><id>https://mattcasmith.net/2020/11/15/aws-deploy-connecting-sql-database-cloud</id><content type="html" xml:base="https://mattcasmith.net/2020/11/15/aws-deploy-connect-sql-database-cloud">&lt;p&gt;My first Amazon Web Services (AWS) basics post covered the process of &lt;a href=&quot;https://mattcasmith.net/2020/11/15/aws-deploying-virtual-network-server-cloud&quot;&gt;setting up a Virtual Private Cloud (VPC) and a Windows Server 2019 EC2 instance&lt;/a&gt;. This time we’re going to build on this simple setup by deploying a Amazon Aurora SQL database and ensuring we can access it from our server.&lt;/p&gt;

&lt;h3 id=&quot;aws-basics-series&quot;&gt;AWS basics series&lt;/h3&gt;

&lt;p&gt;1. &lt;a href=&quot;https://mattcasmith.net/2020/11/15/aws-deploying-virtual-network-server-cloud&quot;&gt;Deploying a virtual network and server&lt;/a&gt;&lt;br /&gt;
2. Deploying and connecting to a SQL database&lt;/p&gt;

&lt;h3 id=&quot;databases-in-aws&quot;&gt;Databases in AWS&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/products/databases/&quot; target=&quot;_blank&quot;&gt;Databases in AWS&lt;/a&gt; generally come in three different flavours, which are all designed for different use cases, data volumes, and availability requirements. These are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Relational Database Service (RDS)&lt;/strong&gt; - Relational databases are typical, structured, table-based databases. AWS gives you the option to run MySQL, Oracle SQL, and Microsoft SQL Server among other established names, but also offers its own database engine called &lt;a href=&quot;https://aws.amazon.com/rds/aurora/&quot; target=&quot;_blank&quot;&gt;Amazon Aurora&lt;/a&gt;, which is optimised with a few extra features designed for the cloud.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;DynamoDB&lt;/strong&gt; - DynamoDB is a NoSQL database consisting of key-value pairs for less structured data. This can be a good option if the speed of queries is the most important factor.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Redshift&lt;/strong&gt; - Redshift is the AWS service for data warehouses. This is the best solution if you have petabytes of data to store, and is optimised for handling these large datasets.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/products/databases/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;\wp-content\uploads\2020\11\aws-sql-dbs.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To keep things simple, we’ll assume our data will fit nicely in an RDS database that needs to be accessed from the Windows Server 2019 EC2 instance we deployed in the last post. Deploying an RDS database will take three steps: creating the database, ensuring our server has the correct access permissions, and connecting to the database to use it. Let’s get started with the setup.&lt;/p&gt;

&lt;h3 id=&quot;deploying-an-aws-rds-database&quot;&gt;Deploying an AWS RDS database&lt;/h3&gt;

&lt;p&gt;Navigate to the RDS dashboard and look for the Create Database section, where there is also a Create Database button. Clicking this will take you to a form where you can choose the configuration of your new database, from the engine to which VPC it sits in. Click the Standard Create option to continue.&lt;/p&gt;

&lt;p&gt;Now let’s select an engine for our SQL database. You may instinctively reach for the familiar names like MySQL and Microsoft SQL Server, but I’m going to use Amazon Aurora (selecting the edition with MySQL compatibility). This is Amazon’s own database engine, which is optimised for use in the AWS cloud and can support higher throughput, auto-scaling, and replication across availability zones.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\wp-content\uploads\2020\11\aws-sql-engine.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ll choose Provisioned capacity, which means we manage the server instance the database sits on, and select the Dev/Test template to avoid any extra charges for the high availability and resilience aspects of the Production template. Next we can create a username and password for the master user - make a note of these details as we’ll need them to access the database later on.&lt;/p&gt;

&lt;p&gt;Scroll down and you have the opportunity to choose the VPC (basically the network) your database should sit in. We’ll choose the &lt;code&gt;server-deployment&lt;/code&gt; VPC we configured during the first blog post. This will make it easier to reach the database from the server and will do for our simple example, but it is not recommended to deploy production databases in VPCs with internet access - it would be best to tuck them away in their own VPC and configure rules for access from another.&lt;/p&gt;

&lt;p&gt;Let’s also create a new Security Group to allow access to the database. We’ll call it &lt;code&gt;database-access&lt;/code&gt; and configure it a bit later. Click Create Database and you’ll be taken back to the Databases Dashboard, where you’ll be able to see your new Aurora MySQL database is now being created.&lt;/p&gt;

&lt;h3 id=&quot;granting-access-to-the-ec2-server-instance&quot;&gt;Granting access to the EC2 server instance&lt;/h3&gt;

&lt;p&gt;But if we were to try to connect from our EC2 server to the database now, our connection would fail. Why? Because we haven’t configured the Security Group to allow the connection. We can rectify this by navigating to the VPC Dashboard and clicking on Security Groups. From there we can add a rule that allows access on port 3306 from the subnet our server sits in (&lt;code&gt;10.0.0.0/24&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\wp-content\uploads\2020\11\aws-sql-security-group.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That’s it - we’ve deployed a SQL database and given our server access to it. The next step is to log in and manipulate some data, just to prove that the database and connection both work.&lt;/p&gt;

&lt;h3 id=&quot;connecting-to-the-aws-rds-database&quot;&gt;Connecting to the AWS RDS database&lt;/h3&gt;

&lt;p&gt;If we RDP to our server, we can now connect to our SQL database - but first, you’ll need to download a client. I used Oracle’s &lt;a href=&quot;https://dev.mysql.com/downloads/&quot; target=&quot;_blank&quot;&gt;MySQL Shell&lt;/a&gt;, with which you can establish a connection with this command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;mysqlsh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exe&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endpoint&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3306&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;After entering our password, we are connected to the database and can interact with its contents using standard SQL commands to add, remove, merge, and filter data. Looks like it’s good to go! If you need any tips on how to manipulate data with SQL, see my previous posts on &lt;a href=&quot;https://mattcasmith.net/2018/10/12/basic-sql-queries-select-from-where-operators/&quot;&gt;basic SQL commands&lt;/a&gt;, &lt;a href=&quot;https://mattcasmith.net/2018/12/21/sql-joins-inner-left-right-outer/&quot;&gt;SQL JOINs&lt;/a&gt;, and &lt;a href=&quot;https://mattcasmith.net/2019/02/01/sql-alter-table-add-modify-drop-columns/&quot;&gt;SQL TABLE commands&lt;/a&gt;, which should help you to get started.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\wp-content\uploads\2020\11\aws-sql-mysql.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again, make sure you go back to the EC2 and RDS Dashboards when you’re finished and stop your EC2 instance and RDS database. This will help you to avoid any unexpected AWS charges.&lt;/p&gt;

&lt;p&gt;So now we have &lt;a href=&quot;https://mattcasmith.net/2020/11/15/aws-deploying-virtual-network-server-cloud&quot;&gt;a VPC and a Windows Server 2019 EC2 instance&lt;/a&gt;, and have deployed a simple RDS Aurora database that we can access from it. In my next and final AWS basics post, I’ll run through how to set up an S3 bucket to store some files in the cloud (and avoid sharing them with everyone).&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">My first Amazon Web Services (AWS) basics post covered the process of setting up a Virtual Private Cloud (VPC) and a Windows Server 2019 EC2 instance. This time we’re going to build on this simple setup by deploying a Amazon Aurora SQL database and ensuring we can access it from our server. AWS basics series 1. Deploying a virtual network and server 2. Deploying and connecting to a SQL database Databases in AWS Databases in AWS generally come in three different flavours, which are all designed for different use cases, data volumes, and availability requirements. These are: Relational Database Service (RDS) - Relational databases are typical, structured, table-based databases. AWS gives you the option to run MySQL, Oracle SQL, and Microsoft SQL Server among other established names, but also offers its own database engine called Amazon Aurora, which is optimised with a few extra features designed for the cloud. DynamoDB - DynamoDB is a NoSQL database consisting of key-value pairs for less structured data. This can be a good option if the speed of queries is the most important factor. Redshift - Redshift is the AWS service for data warehouses. This is the best solution if you have petabytes of data to store, and is optimised for handling these large datasets. To keep things simple, we’ll assume our data will fit nicely in an RDS database that needs to be accessed from the Windows Server 2019 EC2 instance we deployed in the last post. Deploying an RDS database will take three steps: creating the database, ensuring our server has the correct access permissions, and connecting to the database to use it. Let’s get started with the setup. Deploying an AWS RDS database Navigate to the RDS dashboard and look for the Create Database section, where there is also a Create Database button. Clicking this will take you to a form where you can choose the configuration of your new database, from the engine to which VPC it sits in. Click the Standard Create option to continue. Now let’s select an engine for our SQL database. You may instinctively reach for the familiar names like MySQL and Microsoft SQL Server, but I’m going to use Amazon Aurora (selecting the edition with MySQL compatibility). This is Amazon’s own database engine, which is optimised for use in the AWS cloud and can support higher throughput, auto-scaling, and replication across availability zones. We’ll choose Provisioned capacity, which means we manage the server instance the database sits on, and select the Dev/Test template to avoid any extra charges for the high availability and resilience aspects of the Production template. Next we can create a username and password for the master user - make a note of these details as we’ll need them to access the database later on. Scroll down and you have the opportunity to choose the VPC (basically the network) your database should sit in. We’ll choose the server-deployment VPC we configured during the first blog post. This will make it easier to reach the database from the server and will do for our simple example, but it is not recommended to deploy production databases in VPCs with internet access - it would be best to tuck them away in their own VPC and configure rules for access from another. Let’s also create a new Security Group to allow access to the database. We’ll call it database-access and configure it a bit later. Click Create Database and you’ll be taken back to the Databases Dashboard, where you’ll be able to see your new Aurora MySQL database is now being created. Granting access to the EC2 server instance But if we were to try to connect from our EC2 server to the database now, our connection would fail. Why? Because we haven’t configured the Security Group to allow the connection. We can rectify this by navigating to the VPC Dashboard and clicking on Security Groups. From there we can add a rule that allows access on port 3306 from the subnet our server sits in (10.0.0.0/24). That’s it - we’ve deployed a SQL database and given our server access to it. The next step is to log in and manipulate some data, just to prove that the database and connection both work. Connecting to the AWS RDS database If we RDP to our server, we can now connect to our SQL database - but first, you’ll need to download a client. I used Oracle’s MySQL Shell, with which you can establish a connection with this command: .\mysqlsh.exe -h &amp;lt;insert database endpoint address&amp;gt; -P 3306 -u &amp;lt;username&amp;gt; -p After entering our password, we are connected to the database and can interact with its contents using standard SQL commands to add, remove, merge, and filter data. Looks like it’s good to go! If you need any tips on how to manipulate data with SQL, see my previous posts on basic SQL commands, SQL JOINs, and SQL TABLE commands, which should help you to get started. Again, make sure you go back to the EC2 and RDS Dashboards when you’re finished and stop your EC2 instance and RDS database. This will help you to avoid any unexpected AWS charges. So now we have a VPC and a Windows Server 2019 EC2 instance, and have deployed a simple RDS Aurora database that we can access from it. In my next and final AWS basics post, I’ll run through how to set up an S3 bucket to store some files in the cloud (and avoid sharing them with everyone).</summary></entry><entry><title type="html">AWS: Deploying a virtual network and server in the cloud</title><link href="https://mattcasmith.net/2020/11/15/aws-deploying-virtual-network-server-cloud" rel="alternate" type="text/html" title="AWS&amp;#58; Deploying a virtual network and server in the cloud" /><published>2020-11-15T00:00:00+00:00</published><updated>2020-11-15T00:00:00+00:00</updated><id>https://mattcasmith.net/2020/11/15/aws-deploying-virtual-server-cloud</id><content type="html" xml:base="https://mattcasmith.net/2020/11/15/aws-deploying-virtual-network-server-cloud">&lt;p&gt;Having worked on serveral projects involving Amazon Web Services (AWS) recently, but always at arm’s length, I decided to get a bit more hands-on. At worst this would give me a more practical grounding in managing cloud instances, and at best it would give me a useful resource for future &lt;a href=&quot;https://mattcasmith.net/category/programming.html&quot;&gt;coding projects&lt;/a&gt;. So I got to work and set up an account to play around with some of Amazon’s services.&lt;/p&gt;

&lt;p&gt;The nice thing about the platform is the &lt;a href=&quot;https://aws.amazon.com/free/&quot; target=&quot;_blank&quot;&gt;AWS Free Tier&lt;/a&gt; offering. This is automatically applied to all new users’ accounts and gives free access to many services, at least for the first 12 months. The ins and outs of what’s included and what’s not are a bit complex, but the headline is that unless you deploy anything particularly computationally expensive, you’re not likely to incur any costs within your first year.&lt;/p&gt;

&lt;p&gt;To help my learnings sink in, I’m going to write a few posts detailing how to perform basic tasks, beginning with setting up an EC2 instance (more on what that is later). But before you get started, make sure you have the basics covered - namely registering for an account, choosing a strong, unique password, and activating two-factor authentication, as you should for all your accounts. Once you’ve completed all of that, log in to the AWS Management Console to get started.&lt;/p&gt;

&lt;h3 id=&quot;aws-basics-series&quot;&gt;AWS basics series&lt;/h3&gt;

&lt;p&gt;1. Deploying a virtual network and server&lt;br /&gt;
2. &lt;a href=&quot;https://mattcasmith.net/2020/11/15/aws-deploy-connect-sql-database-cloud&quot;&gt;Deploying and connecting to a SQL database&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;configuring-a-virtual-private-cloud-vpc&quot;&gt;Configuring a Virtual Private Cloud (VPC)&lt;/h3&gt;

&lt;p&gt;Virtual Private Clouds (VPCs) are Amazon’s way of organising your cloud resources. They are essentially internal IP address ranges where servers and other entities can be deployed, with settings for various levels of internal and external communication via firewall-esque rules and the deployment of subnets. Before we deploy our server, we need to create a VPC for it to sit in.&lt;/p&gt;

&lt;p&gt;AWS provides a VPC by default, but for the sake of experience I’ll set up a new VPC for our server by clicking Create VPC, giving it the name &lt;code&gt;server-deployment&lt;/code&gt;, and assigning the internal IP address range &lt;code&gt;10.0.0.0/24&lt;/code&gt;. It’s as simple as that - click Create VPC and AWS will instantly deploy it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\wp-content\uploads\2020\11\aws-server-vpc.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Servers must sit on subnets within VPCs, so head back to the VPC Dashboard and click Subnets, then Create Subnet. I’ll call mine &lt;code&gt;subnet-a&lt;/code&gt;, put it in the &lt;code&gt;server-deployment&lt;/code&gt; VPC, and assign the IP address range &lt;code&gt;10.0.0.0/24&lt;/code&gt; to the subnet. This will assign the whole VPC IP address range to a single subnet, but that’s not an issue for this example as we’ll only be deploying a single server.&lt;/p&gt;

&lt;p&gt;Now we have a VPC and a subnet, we need to create a path from the subnet to the internet so we can access our server. Click on Internet Gateways in the VPC Dashboard sidebar, create a new Internet Gateway (mine is called &lt;code&gt;server-internet-gateway&lt;/code&gt;) and attach it to your VPC. Then click Route Tables in the sidebar, select the Route Table for your VPC, click on the Routes tab, and select Edit Routes. A route that sends all traffic with the destination &lt;code&gt;0.0.0.0/0&lt;/code&gt; to the new Internet Gateway will allow the server to reach the internet, which is not always desirable but is necessary for direct access in this case.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\wp-content\uploads\2020\11\aws-server-vpc-routes.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;setting-up-an-elastic-compute-cloud-ec2-instance&quot;&gt;Setting up an Elastic Compute Cloud (EC2) instance&lt;/h3&gt;

&lt;p&gt;Now let’s create a Windows server within our VPC, and configure it in such a way that we can access it from outside of AWS. Servers within AWS are called Elastic Compute Cloud (EC2) instances, so to deploy one we’ll need to navigate to the EC2 Dashboard and click Launch Instance.&lt;/p&gt;

&lt;p&gt;Here we’ll be presented with a list of what AWS calls Amazon Machine Images (AMIs). Each AMI is essentially the equivalent of a gold image in an on-premise environment. We can select from a variety of images (if you’re using AWS Free Tier, those that are included free of charge are indicated), but for the purposes of this example I’ll select the Microsoft Windows Server 2019 Base AMI.&lt;/p&gt;

&lt;p&gt;Next we’re asked to choose an instance type. This screen essentially shows a huge list of options for the number of CPUs, amount of RAM, and network performance that our server should have. We only have one option within AWS Free Tier - the &lt;code&gt;t2.micro&lt;/code&gt; instance type - so we’ll go with that.&lt;/p&gt;

&lt;p&gt;Then we’re given the opportunity to choose some settings for our instance. The important thing here is that the Network setting is set to &lt;code&gt;server-deployment&lt;/code&gt; - the VPC we created in the last section. We also need to choose a subnet - that will be &lt;code&gt;subnet-a&lt;/code&gt; (&lt;code&gt;10.0.0.0/24&lt;/code&gt;) in my case. Be sure to enable public IP assignment if you need to be able to access the server from the internet. We can also set up network interfaces and assign IP addresses here, but I’ll stick with the default and let AWS handle it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\wp-content\uploads\2020\11\aws-server-ec2-instance-details.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The next screen allocates storage to the server in the form of an Elastic Block Store (EBS), which essentially functions as a hard drive volume. Again, our options under AWS Free Tier are limited to a 30GB General Purpose SSD. It’s also a good idea to encrypt our volume for an additional layer of protection.&lt;/p&gt;

&lt;p&gt;Next up are tags, which allow us to categorise assets to help to manage costs, billing, and so on. As I’m deploying a lone server and will soon be terminating it, I won’t add any tags.&lt;/p&gt;

&lt;p&gt;The next step is crucially important. Security Groups essentially function as firewalls for EC2 instances. As any good firewall configuration should, they default to &lt;code&gt;deny all&lt;/code&gt;. We’ll need to use Remote Desktop Protocol (RDP) on TCP port 3389 to access our server, so I’ll create a new Security Group called &lt;code&gt;rdp-admin&lt;/code&gt; and allow incoming connections on that port - but we don’t want this to be exposed to the whole internet, so I’ll restrict access to my IP address (which is handily a preset AWS option).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\wp-content\uploads\2020\11\aws-server-ec2-security-group.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Click Review and Launch and you’ll have one last chance to check everything is in order before pulling the trigger and deploying your server. When you click Launch, you’ll be given the opportunity to create a new key pair, which you’ll use to decrypt the password to access the server. Name your key pair (I’ll call mine &lt;code&gt;win-server-admin&lt;/code&gt;, download the &lt;code&gt;.pem&lt;/code&gt; private key file, and keep it somewhere safe.&lt;/p&gt;

&lt;p&gt;Congratulations - you’ve deployed Windows Server 2019 in the AWS cloud!&lt;/p&gt;

&lt;h3 id=&quot;connecting-to-the-windows-server&quot;&gt;Connecting to the Windows server&lt;/h3&gt;

&lt;p&gt;If you return to the EC2 dashboard and click on Instances, you’ll now be able to see your server running in AWS. That’s great - but it’s just a number in a management dashboard. How do we actually use it?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\wp-content\uploads\2020\11\aws-server-ec2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As I previously mentioned, the answer lies with RDP - a protocol that allows us to access another Windows computer remotely and interact with it as though we’re sat in front of it using the mouse and keyboard. To prepare to connect, open the Remote Desktop Connection app in your Windows PC (click Start and type &lt;code&gt;RDP&lt;/code&gt; to find it) and enter your EC2 instance’s public IP address in the Computer field.&lt;/p&gt;

&lt;p&gt;Back on your EC2 instance’s page on the AWS dashboard, click Connect and then switch to the RDP Client tab. Then select Get Password and upload the &lt;code&gt;.pem&lt;/code&gt; private key file you downloaded earlier to decrypt the password. When you click Connect to start the session, the default username is &lt;code&gt;Administrator&lt;/code&gt; and you’ll be able to copy and paste the password to authenticate and connect to the server.&lt;/p&gt;

&lt;p&gt;Once you’re finished and the RDP client has done its thing, you should see your Windows Server 2019 desktop. You can now use your cloud server as though it was a physical computer on your desk.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\wp-content\uploads\2020\11\aws-server-ec2-rdp.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can now use your virtual server for whatever your heart desires - just make sure you remember to stop it on the EC2 Dashboard whenever you’re finished using it so you don’t incur any extra costs.&lt;/p&gt;

&lt;p&gt;This was, of course, a very simple deployment. Setting up an enterprise-scale AWS environment would require much more planning and architecture design to ensure efficiency and security. However, a simple VPC and server is a good starting point, and this is something I’ll build on in a few more posts that will cover the deployment of commonly used AWS services like databases and S3 buckets.&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">Having worked on serveral projects involving Amazon Web Services (AWS) recently, but always at arm’s length, I decided to get a bit more hands-on. At worst this would give me a more practical grounding in managing cloud instances, and at best it would give me a useful resource for future coding projects. So I got to work and set up an account to play around with some of Amazon’s services. The nice thing about the platform is the AWS Free Tier offering. This is automatically applied to all new users’ accounts and gives free access to many services, at least for the first 12 months. The ins and outs of what’s included and what’s not are a bit complex, but the headline is that unless you deploy anything particularly computationally expensive, you’re not likely to incur any costs within your first year. To help my learnings sink in, I’m going to write a few posts detailing how to perform basic tasks, beginning with setting up an EC2 instance (more on what that is later). But before you get started, make sure you have the basics covered - namely registering for an account, choosing a strong, unique password, and activating two-factor authentication, as you should for all your accounts. Once you’ve completed all of that, log in to the AWS Management Console to get started. AWS basics series 1. Deploying a virtual network and server 2. Deploying and connecting to a SQL database Configuring a Virtual Private Cloud (VPC) Virtual Private Clouds (VPCs) are Amazon’s way of organising your cloud resources. They are essentially internal IP address ranges where servers and other entities can be deployed, with settings for various levels of internal and external communication via firewall-esque rules and the deployment of subnets. Before we deploy our server, we need to create a VPC for it to sit in. AWS provides a VPC by default, but for the sake of experience I’ll set up a new VPC for our server by clicking Create VPC, giving it the name server-deployment, and assigning the internal IP address range 10.0.0.0/24. It’s as simple as that - click Create VPC and AWS will instantly deploy it. Servers must sit on subnets within VPCs, so head back to the VPC Dashboard and click Subnets, then Create Subnet. I’ll call mine subnet-a, put it in the server-deployment VPC, and assign the IP address range 10.0.0.0/24 to the subnet. This will assign the whole VPC IP address range to a single subnet, but that’s not an issue for this example as we’ll only be deploying a single server. Now we have a VPC and a subnet, we need to create a path from the subnet to the internet so we can access our server. Click on Internet Gateways in the VPC Dashboard sidebar, create a new Internet Gateway (mine is called server-internet-gateway) and attach it to your VPC. Then click Route Tables in the sidebar, select the Route Table for your VPC, click on the Routes tab, and select Edit Routes. A route that sends all traffic with the destination 0.0.0.0/0 to the new Internet Gateway will allow the server to reach the internet, which is not always desirable but is necessary for direct access in this case. Setting up an Elastic Compute Cloud (EC2) instance Now let’s create a Windows server within our VPC, and configure it in such a way that we can access it from outside of AWS. Servers within AWS are called Elastic Compute Cloud (EC2) instances, so to deploy one we’ll need to navigate to the EC2 Dashboard and click Launch Instance. Here we’ll be presented with a list of what AWS calls Amazon Machine Images (AMIs). Each AMI is essentially the equivalent of a gold image in an on-premise environment. We can select from a variety of images (if you’re using AWS Free Tier, those that are included free of charge are indicated), but for the purposes of this example I’ll select the Microsoft Windows Server 2019 Base AMI. Next we’re asked to choose an instance type. This screen essentially shows a huge list of options for the number of CPUs, amount of RAM, and network performance that our server should have. We only have one option within AWS Free Tier - the t2.micro instance type - so we’ll go with that. Then we’re given the opportunity to choose some settings for our instance. The important thing here is that the Network setting is set to server-deployment - the VPC we created in the last section. We also need to choose a subnet - that will be subnet-a (10.0.0.0/24) in my case. Be sure to enable public IP assignment if you need to be able to access the server from the internet. We can also set up network interfaces and assign IP addresses here, but I’ll stick with the default and let AWS handle it. The next screen allocates storage to the server in the form of an Elastic Block Store (EBS), which essentially functions as a hard drive volume. Again, our options under AWS Free Tier are limited to a 30GB General Purpose SSD. It’s also a good idea to encrypt our volume for an additional layer of protection. Next up are tags, which allow us to categorise assets to help to manage costs, billing, and so on. As I’m deploying a lone server and will soon be terminating it, I won’t add any tags. The next step is crucially important. Security Groups essentially function as firewalls for EC2 instances. As any good firewall configuration should, they default to deny all. We’ll need to use Remote Desktop Protocol (RDP) on TCP port 3389 to access our server, so I’ll create a new Security Group called rdp-admin and allow incoming connections on that port - but we don’t want this to be exposed to the whole internet, so I’ll restrict access to my IP address (which is handily a preset AWS option). Click Review and Launch and you’ll have one last chance to check everything is in order before pulling the trigger and deploying your server. When you click Launch, you’ll be given the opportunity to create a new key pair, which you’ll use to decrypt the password to access the server. Name your key pair (I’ll call mine win-server-admin, download the .pem private key file, and keep it somewhere safe. Congratulations - you’ve deployed Windows Server 2019 in the AWS cloud! Connecting to the Windows server If you return to the EC2 dashboard and click on Instances, you’ll now be able to see your server running in AWS. That’s great - but it’s just a number in a management dashboard. How do we actually use it? As I previously mentioned, the answer lies with RDP - a protocol that allows us to access another Windows computer remotely and interact with it as though we’re sat in front of it using the mouse and keyboard. To prepare to connect, open the Remote Desktop Connection app in your Windows PC (click Start and type RDP to find it) and enter your EC2 instance’s public IP address in the Computer field. Back on your EC2 instance’s page on the AWS dashboard, click Connect and then switch to the RDP Client tab. Then select Get Password and upload the .pem private key file you downloaded earlier to decrypt the password. When you click Connect to start the session, the default username is Administrator and you’ll be able to copy and paste the password to authenticate and connect to the server. Once you’re finished and the RDP client has done its thing, you should see your Windows Server 2019 desktop. You can now use your cloud server as though it was a physical computer on your desk. You can now use your virtual server for whatever your heart desires - just make sure you remember to stop it on the EC2 Dashboard whenever you’re finished using it so you don’t incur any extra costs. This was, of course, a very simple deployment. Setting up an enterprise-scale AWS environment would require much more planning and architecture design to ensure efficiency and security. However, a simple VPC and server is a good starting point, and this is something I’ll build on in a few more posts that will cover the deployment of commonly used AWS services like databases and S3 buckets.</summary></entry><entry><title type="html">Cracking a password-protected ZIP file with fcrackzip</title><link href="https://mattcasmith.net/2020/09/12/cracking-password-protected-zip-file-fcrackzip" rel="alternate" type="text/html" title="Cracking a password-protected ZIP file with fcrackzip" /><published>2020-09-12T01:00:00+01:00</published><updated>2020-09-12T01:00:00+01:00</updated><id>https://mattcasmith.net/2020/09/12/cracking-password-protected-zip-fcrackzip</id><content type="html" xml:base="https://mattcasmith.net/2020/09/12/cracking-password-protected-zip-file-fcrackzip">&lt;p&gt;I recently took part in a DFIR capture the flag with some colleagues. Participants were provided with a system disk image and asked to mount it and complete a number of challenges to discover various flags hidden within the data. Exercises like this are always both a lot of fun and a good way to share knowledge and learn - after all, there’s no better time to pick up new techniques than in the heat of competition.&lt;/p&gt;

&lt;p&gt;I got most of the answers and finished joint second. One of the questions I didn’t have time for, which I deprioritised as I would have needed to look up the methodology, involved discovering the password to an encrypted ZIP file to access the flag inside. To make sure I can complete similar challenges in future CTFs (or live scenarios), I decided to do some digging, crack a ZIP, and document my method.&lt;/p&gt;

&lt;h3 id=&quot;creating-a-password-protected-zip-archive&quot;&gt;Creating a password-protected ZIP archive&lt;/h3&gt;

&lt;p&gt;To emulate the conditions of the CTF, I needed to create a ZIP archive containing a text file with the would-be flag. Creating the text file is a simple as using &lt;code&gt;echo&lt;/code&gt; to write some content to a file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;This is a secret file.&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;secretfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;txt&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We can then use 7-Zip to create a password-protected ZIP archive. The &lt;code&gt;a&lt;/code&gt; option tells 7-Zip that we’re adding files to an archive. We then specify the name of the archive and the file we wish to add. Finally, the &lt;code&gt;-p&lt;/code&gt; option allows us to add a password - in this case, the very secure &lt;code&gt;thisisapassword&lt;/code&gt;. Also note that the lack of a space isn’t a typo - that’s just the way 7-Zip wants the information to be provided.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;archive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;secretfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;txt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pthisisapassword&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now let’s check that the password was properly applied to the archive using the GUI. We can see that if we open &lt;code&gt;archive.zip&lt;/code&gt;, we can see the file listing inside, but if we double-click to open any of the files we receive a prompt and cannot see the contents without entering the correct password.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2020/09/fcrackzip_1.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;cracking-the-password-protected-zip-archive-with-fcrackzip&quot;&gt;Cracking the password-protected ZIP archive with fcrackzip&lt;/h3&gt;

&lt;p&gt;The tool we’ll use for discovering the password to our ZIP file is fcrackzip. We’ll use the &lt;code&gt;-v&lt;/code&gt; option to generate verbose output and keep track of what it’s doing, and &lt;code&gt;-u&lt;/code&gt;, which tells fcrackzip to attempt to use the guessed password to unzip the file to verify that it is correct and reduce the risk of false positives.&lt;/p&gt;

&lt;p&gt;We have two options as to how fcrackzip will attempt to guess the password. Firstly, we can use the &lt;code&gt;-b&lt;/code&gt; option to perform a brute force attack, guessing every possible combination of characters. This is useful for cracking long, unique passwords, but not the quickest method if you think the password may actually just be a simple combination of words, numbers, and symbols.&lt;/p&gt;

&lt;p&gt;For these scenarios, we’d be better off performing a dictionary attack with &lt;code&gt;-D&lt;/code&gt;. This guesses the password by trying every entry in a list of passwords. As such, fcrackzip requires you to provide a dictionary to use with &lt;code&gt;-p&lt;/code&gt;. We’ll use &lt;a href=&quot;https://github.com/brannondorsey/naive-hashcat/releases/download/data/rockyou.txt&quot; taget=&quot;_blank&quot;&gt;the popular &lt;code&gt;rockyou.txt&lt;/code&gt; dictionary&lt;/a&gt; - a mainstay of the cyber security profession.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;fcrackzip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rockyou&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;txt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;archive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;When we hit return, fcrackzip will begin trying every password in the dictionary. When one of them successfully unzips the archive, it will stop and tell us what it is. The duration of this process depends on the dictionary, your computer’s processing power, and the password’s complexity, but in our case the password is so simple that we see results almost instantly.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2020/09/fcrackzip_2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To view the contents of the archive, all we would have to do is copy the password from the output, open the ZIP file in the Linux GUI, and paste it into the password field when prompted. Then voilà - we’d have our flag, attacker-encrypted data, or any other files we were trying to access.&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">I recently took part in a DFIR capture the flag with some colleagues. Participants were provided with a system disk image and asked to mount it and complete a number of challenges to discover various flags hidden within the data. Exercises like this are always both a lot of fun and a good way to share knowledge and learn - after all, there’s no better time to pick up new techniques than in the heat of competition. I got most of the answers and finished joint second. One of the questions I didn’t have time for, which I deprioritised as I would have needed to look up the methodology, involved discovering the password to an encrypted ZIP file to access the flag inside. To make sure I can complete similar challenges in future CTFs (or live scenarios), I decided to do some digging, crack a ZIP, and document my method. Creating a password-protected ZIP archive To emulate the conditions of the CTF, I needed to create a ZIP archive containing a text file with the would-be flag. Creating the text file is a simple as using echo to write some content to a file. echo &quot;This is a secret file.&quot; &amp;gt; secretfile.txt We can then use 7-Zip to create a password-protected ZIP archive. The a option tells 7-Zip that we’re adding files to an archive. We then specify the name of the archive and the file we wish to add. Finally, the -p option allows us to add a password - in this case, the very secure thisisapassword. Also note that the lack of a space isn’t a typo - that’s just the way 7-Zip wants the information to be provided. 7z a archive.zip secretfile.txt -pthisisapassword Now let’s check that the password was properly applied to the archive using the GUI. We can see that if we open archive.zip, we can see the file listing inside, but if we double-click to open any of the files we receive a prompt and cannot see the contents without entering the correct password. Cracking the password-protected ZIP archive with fcrackzip The tool we’ll use for discovering the password to our ZIP file is fcrackzip. We’ll use the -v option to generate verbose output and keep track of what it’s doing, and -u, which tells fcrackzip to attempt to use the guessed password to unzip the file to verify that it is correct and reduce the risk of false positives. We have two options as to how fcrackzip will attempt to guess the password. Firstly, we can use the -b option to perform a brute force attack, guessing every possible combination of characters. This is useful for cracking long, unique passwords, but not the quickest method if you think the password may actually just be a simple combination of words, numbers, and symbols. For these scenarios, we’d be better off performing a dictionary attack with -D. This guesses the password by trying every entry in a list of passwords. As such, fcrackzip requires you to provide a dictionary to use with -p. We’ll use the popular rockyou.txt dictionary - a mainstay of the cyber security profession. fcrackzip -D -p rockyou.txt -v -u archive.zip When we hit return, fcrackzip will begin trying every password in the dictionary. When one of them successfully unzips the archive, it will stop and tell us what it is. The duration of this process depends on the dictionary, your computer’s processing power, and the password’s complexity, but in our case the password is so simple that we see results almost instantly. To view the contents of the archive, all we would have to do is copy the password from the output, open the ZIP file in the Linux GUI, and paste it into the password field when prompted. Then voilà - we’d have our flag, attacker-encrypted data, or any other files we were trying to access.</summary></entry><entry><title type="html">Network connections and packet crafting on the Linux command line</title><link href="https://mattcasmith.net/2020/08/27/network-connections-packet-crafting-linux-command-line" rel="alternate" type="text/html" title="Network connections and packet crafting on the Linux command line" /><published>2020-09-02T01:00:00+01:00</published><updated>2020-09-02T01:00:00+01:00</updated><id>https://mattcasmith.net/2020/08/27/network-connections-packet-crafting-linux-command-line</id><content type="html" xml:base="https://mattcasmith.net/2020/08/27/network-connections-packet-crafting-linux-command-line">&lt;p&gt;The problem with taking leave during a pandemic is that there are very few places you can go that don’t present an unnecessary risk. For me at least, the thought of taking a plane abroad wasn’t appealing, and neither were the Tube journeys that would be necessary to go out and about in London. To save myself from three weeks of &lt;a href=&quot;http://mattcasmith.net/2019/01/25/football-manager-addictive-spreadsheet/&quot;&gt;&lt;em&gt;Football Manager&lt;/em&gt;&lt;/a&gt; and repeats of &lt;em&gt;The Office&lt;/em&gt;, I stocked up on cyber security books.&lt;/p&gt;

&lt;p&gt;I finally completed my first runthrough of &lt;a href=&quot;https://www.amazon.co.uk/CISSP-Certified-Information-Security-Professional/dp/1119475937/&quot; target=&quot;_blank&quot;&gt;the mammoth CISSP study guide&lt;/a&gt;, brushed up on my PowerShell with &lt;a href=&quot;https://nostarch.com/powershellsysadmins&quot; target=&quot;_blank&quot;&gt;&lt;em&gt;PowerShell for Sysadmins&lt;/em&gt;&lt;/a&gt;, and took some time to improve my Wireshark skills with &lt;a href=&quot;https://nostarch.com/packetanalysis3&quot; target=&quot;_blank&quot;&gt;&lt;em&gt;Practical Packet Analysis&lt;/em&gt;&lt;/a&gt;. The latter was certainly my favourite - Chris Sanders’ book serves as both a very practical exploration of packet analysis and just about the best introduction to networking fundamentals I’ve come across. It’s certainly one I’ll be recommending to my colleagues when I return to the (virtual) office.&lt;/p&gt;

&lt;p&gt;When I finished &lt;em&gt;Practical Packet Analysis&lt;/em&gt;, I decided I wanted to increase my familiarity not only with Wireshark and analysis techniques, but also how the packets are generated in the first place. It’s easy enough to run &lt;code&gt;ping&lt;/code&gt; or &lt;code&gt;nslookup&lt;/code&gt; and observe the results, but when it came to manually crafting packets I was only vaguely aware of where to start, and it seemed like a useful area to explore. So I fired up a couple of Linux virtual machines, opened Google, and got searching for tools and tutorials to help me.&lt;/p&gt;

&lt;p&gt;Below is a brief overview of some of the tools I experimented with. Some of them allow you to create packets, others TCP connections - but all of them are worth spending some time with to send some packets, generate some PCAPs, and learn more about how your network functions.&lt;/p&gt;

&lt;h3 id=&quot;contents&quot;&gt;Contents&lt;/h3&gt;
&lt;p&gt;1. &lt;a href=&quot;#netcat&quot;&gt;netcat&lt;/a&gt;&lt;br /&gt;
2. &lt;a href=&quot;#bash&quot;&gt;Bash&lt;/a&gt;&lt;br /&gt;
3. &lt;a href=&quot;#scapy&quot;&gt;Scapy&lt;/a&gt;&lt;br /&gt;
4. &lt;a href=&quot;#sendip&quot;&gt;SendIp&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;netcat&quot;&gt;netcat&lt;/h3&gt;

&lt;p&gt;Let’s start with a classic. A favourite among cyber security professionals, &lt;code&gt;netcat&lt;/code&gt; is a tool that can be used to send and receive data between systems, either via TCP or via UDP with the &lt;code&gt;-u&lt;/code&gt; flag. A listener is set up using the &lt;code&gt;-l&lt;/code&gt; flag - for example, the command below would start a listener on UDP port 372.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lu&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;372&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;With this system listening for UDP datagrams on port 372, we can now use &lt;code&gt;netcat&lt;/code&gt; to send data from a second system. The command to begin sending data is quite similar to the one for setting up the listener - just this time we remove the &lt;code&gt;-l&lt;/code&gt; flag and specify the IP address we want to transmit the data to.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;192.168&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;234.128&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;372&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Any text that is sent over the new connection is displayed almost immediately in the terminal of the receiving machine, which is fine if you’re sitting watching the Linux terminal and waiting for quite a short message, as in the example communication between the two VMs below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2020/09/netcat_1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The beauty of Linux, however, is that we can easily chain together commands to achieve more than they could individually. Let’s say we want to send an image file via &lt;code&gt;netcat&lt;/code&gt;, for example. We can use &lt;code&gt;&amp;gt;&lt;/code&gt; and &lt;code&gt;|&lt;/code&gt; to read the contents of the file on the sending machine and save it on the receiving machine. To receive the image via a TCP connection on port 555, the recipient would use the following command.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;555&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;received_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;png&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The sender would do almost the exact opposite, piping the contents of the original image file to the &lt;code&gt;netcat&lt;/code&gt; connection in order to transmit it over the network to the recipient’s system.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_to_send&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;png&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nc&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;192.168&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;234.128&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;555&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;code&gt;cat&lt;/code&gt; reads the contents of the image file, which are piped to our &lt;code&gt;netcat&lt;/code&gt; connection. The receiving system sends the data received over the connection to its own image file, and when we check the file itself via the GUI we can see that the image data was successfully transmitted and saved to disk.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2020/09/netcat_2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These examples only scratch the surface of what is possible with &lt;code&gt;netcat&lt;/code&gt;, and I’d encourage you to play around with it to see what it is capable of. SANS has &lt;a taget=&quot;_blank&quot; href=&quot;https://www.sans.org/security-resources/sec560/netcat_cheat_sheet_v1.pdf&quot;&gt;a particularly handy cheat sheet&lt;/a&gt; with common commands and accepted flags and options to help you get started.&lt;/p&gt;

&lt;h3 id=&quot;bash&quot;&gt;Bash&lt;/h3&gt;

&lt;p&gt;While we’re on the topic of sending messages and files over the network, it’s also worth noting that the Linux Bash terminal actually has a built-in capability for this using &lt;code&gt;/dev/tcp/&lt;/code&gt; and &lt;code&gt;/dev/udp/&lt;/code&gt;. Simply add an IP address and port and send your data to this directory, and Linux will handle the rest.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Hello. I'm sending this from the Linux terminal.&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;192.168&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;234.128&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;824&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In this example, I’ve used &lt;code&gt;echo&lt;/code&gt; to send a text message over the network, but I could equally have used &lt;code&gt;cat&lt;/code&gt; and a filename to send a file like we did with &lt;code&gt;netcat&lt;/code&gt;. A &lt;code&gt;netcat&lt;/code&gt; listener on the recipient’s system receives the message and outputs it to the terminal as in our first &lt;code&gt;netcat&lt;/code&gt; example.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2020/09/bash_1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Did you notice the small difference in the output above? When all the data has been received, the TCP connection is closed and the listener stops. If we were expecting to receive more data later, we could use &lt;code&gt;netcat&lt;/code&gt;’s &lt;code&gt;-k&lt;/code&gt; flag, which would keep the listener active. The TCP connection is still closed when the transfer is complete, but the listener persists and allows the creation of additional TCP connections as they are needed in response to connection requests from clients.&lt;/p&gt;

&lt;p&gt;Another difference to using &lt;code&gt;netcat&lt;/code&gt; to send data was that we did not need to use &lt;code&gt;sudo&lt;/code&gt; to send data from a TCP port using &lt;code&gt;/dev/tcp/&lt;/code&gt; - useful to know for scenarios where there are restrictions in place.&lt;/p&gt;

&lt;h3 id=&quot;scapy&quot;&gt;Scapy&lt;/h3&gt;

&lt;p&gt;Now let’s move down a level and take a look at some tools for manually crafting packets. &lt;a href=&quot;https://scapy.net/&quot; target=&quot;_blank&quot;&gt;Scapy&lt;/a&gt; is a Python tool that allows you to send custom packets, individually manipulating fields and flags via a series of commands and options to send exactly the data you want - very useful for testing.&lt;/p&gt;

&lt;p&gt;Let’s say I want to check &lt;a href=&quot;http://mattcasmith.net/2020/02/15/pi-hole-samsung-smart-tv/&quot;&gt;my Pi-hole DNS server&lt;/a&gt; is functioning correctly and returning responses when it receives queries. I can manually craft a DNS request for my domain, &lt;code&gt;www.mattcasmith.net&lt;/code&gt;, using the following command within Scapy to specify fields like the destination address/port and DNS query.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dst&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{pihole_ip}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UDP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dport&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;53&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DNS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DNSQR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qname&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;www.mattcasmith.net&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;I specify the Pi-hole’s IP address and the UDP port 53 (used for DNS) as the destination, and provide the domain to be queried with the &lt;code&gt;qname&lt;/code&gt; parameter. If all goes well, Scapy will send the packet to the DNS server and print the packet it receives in response, as below. If we wanted to review this data in a nicer format, we could also have run Wireshark or &lt;code&gt;tcpdump&lt;/code&gt; at the same time to generate a PCAP.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2020/09/scapy_1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By reviewing the response above, we can see that the Pi-hole has successfully returned a response. Between the &lt;code&gt;DNSRR&lt;/code&gt; tags we can see that the DNS server has returned several IP addresses associated with the domain, as we’d see in the output of a tool like &lt;code&gt;nslookup&lt;/code&gt; or &lt;code&gt;dig&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;sendip&quot;&gt;SendIp&lt;/h3&gt;

&lt;p&gt;Another means to the same end is &lt;a href=&quot;https://www-x.antd.nist.gov/ipv6/sendip.html&quot; target=&quot;_blank&quot;&gt;SendIp&lt;/a&gt; - a Linux command line tool that enables you to craft custom packets in a similar way to Scapy. Rather than sending another DNS query, let’s go back to an earlier example and set up a &lt;code&gt;netcat&lt;/code&gt; listener on UDP port 904 to receive a text message.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sendip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ipv4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;192.168&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;234.130&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;udp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;us&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5070&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ud&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;904&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;This is a datagram sent with sendip from the command line.&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;192.168&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;234.128&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The length of the SendIp command reflects the additional flexibility we have compared to when we sent a similar message with &lt;code&gt;netcat&lt;/code&gt;. The &lt;code&gt;-p&lt;/code&gt; flag sets our protocols - in this case IPv4 at the network layer and UDP at the transport layer - and after each we specify the contents of various header fields.&lt;/p&gt;

&lt;p&gt;Notice that SendIP allows us to set the IP source address manually with &lt;code&gt;-is&lt;/code&gt; as well as the UDP source port with &lt;code&gt;-us&lt;/code&gt;. In testing scenarios, including penetration tests, this would allow us to spoof packets from different machines, with much greater control over the packet and header contents than with &lt;code&gt;netcat&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finally, the &lt;code&gt;-d&lt;/code&gt; flag declares the contents of the packet payload - in this case the text message, which we can see in the output on the recipient’s system is received by the &lt;code&gt;netcat&lt;/code&gt; listener.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2020/09/sendip_1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;All of the tools and examples above represent different ways of sending data over a network from the Linux command line. While I used simple text and file exchanges to demonstrate how they work, these tools have many different features and options and can be used in much more complex scenarios. Hopefully this post has given you some idea of how they work and what they are capable of, and given you enough of a starting point to try some experiments of your own.&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">The problem with taking leave during a pandemic is that there are very few places you can go that don’t present an unnecessary risk. For me at least, the thought of taking a plane abroad wasn’t appealing, and neither were the Tube journeys that would be necessary to go out and about in London. To save myself from three weeks of Football Manager and repeats of The Office, I stocked up on cyber security books. I finally completed my first runthrough of the mammoth CISSP study guide, brushed up on my PowerShell with PowerShell for Sysadmins, and took some time to improve my Wireshark skills with Practical Packet Analysis. The latter was certainly my favourite - Chris Sanders’ book serves as both a very practical exploration of packet analysis and just about the best introduction to networking fundamentals I’ve come across. It’s certainly one I’ll be recommending to my colleagues when I return to the (virtual) office. When I finished Practical Packet Analysis, I decided I wanted to increase my familiarity not only with Wireshark and analysis techniques, but also how the packets are generated in the first place. It’s easy enough to run ping or nslookup and observe the results, but when it came to manually crafting packets I was only vaguely aware of where to start, and it seemed like a useful area to explore. So I fired up a couple of Linux virtual machines, opened Google, and got searching for tools and tutorials to help me. Below is a brief overview of some of the tools I experimented with. Some of them allow you to create packets, others TCP connections - but all of them are worth spending some time with to send some packets, generate some PCAPs, and learn more about how your network functions. Contents 1. netcat 2. Bash 3. Scapy 4. SendIp netcat Let’s start with a classic. A favourite among cyber security professionals, netcat is a tool that can be used to send and receive data between systems, either via TCP or via UDP with the -u flag. A listener is set up using the -l flag - for example, the command below would start a listener on UDP port 372. sudo nc -lu 372 With this system listening for UDP datagrams on port 372, we can now use netcat to send data from a second system. The command to begin sending data is quite similar to the one for setting up the listener - just this time we remove the -l flag and specify the IP address we want to transmit the data to. sudo nc -u 192.168.234.128 372 Any text that is sent over the new connection is displayed almost immediately in the terminal of the receiving machine, which is fine if you’re sitting watching the Linux terminal and waiting for quite a short message, as in the example communication between the two VMs below. The beauty of Linux, however, is that we can easily chain together commands to achieve more than they could individually. Let’s say we want to send an image file via netcat, for example. We can use &amp;gt; and | to read the contents of the file on the sending machine and save it on the receiving machine. To receive the image via a TCP connection on port 555, the recipient would use the following command. sudo nc -l 555 &amp;gt; received_image.png The sender would do almost the exact opposite, piping the contents of the original image file to the netcat connection in order to transmit it over the network to the recipient’s system. cat image_to_send.png | sudo nc 192.168.234.128 555 cat reads the contents of the image file, which are piped to our netcat connection. The receiving system sends the data received over the connection to its own image file, and when we check the file itself via the GUI we can see that the image data was successfully transmitted and saved to disk. These examples only scratch the surface of what is possible with netcat, and I’d encourage you to play around with it to see what it is capable of. SANS has a particularly handy cheat sheet with common commands and accepted flags and options to help you get started. Bash While we’re on the topic of sending messages and files over the network, it’s also worth noting that the Linux Bash terminal actually has a built-in capability for this using /dev/tcp/ and /dev/udp/. Simply add an IP address and port and send your data to this directory, and Linux will handle the rest. echo &quot;Hello. I'm sending this from the Linux terminal.&quot; &amp;gt; /dev/tcp/192.168.234.128/824 In this example, I’ve used echo to send a text message over the network, but I could equally have used cat and a filename to send a file like we did with netcat. A netcat listener on the recipient’s system receives the message and outputs it to the terminal as in our first netcat example. Did you notice the small difference in the output above? When all the data has been received, the TCP connection is closed and the listener stops. If we were expecting to receive more data later, we could use netcat’s -k flag, which would keep the listener active. The TCP connection is still closed when the transfer is complete, but the listener persists and allows the creation of additional TCP connections as they are needed in response to connection requests from clients. Another difference to using netcat to send data was that we did not need to use sudo to send data from a TCP port using /dev/tcp/ - useful to know for scenarios where there are restrictions in place. Scapy Now let’s move down a level and take a look at some tools for manually crafting packets. Scapy is a Python tool that allows you to send custom packets, individually manipulating fields and flags via a series of commands and options to send exactly the data you want - very useful for testing. Let’s say I want to check my Pi-hole DNS server is functioning correctly and returning responses when it receives queries. I can manually craft a DNS request for my domain, www.mattcasmith.net, using the following command within Scapy to specify fields like the destination address/port and DNS query. (dst=&quot;{pihole_ip}&quot;)/UDP(dport=53)/DNS(rd=1,qd=DNSQR(qname=&quot;www.mattcasmith.net&quot;)),verbose=0) I specify the Pi-hole’s IP address and the UDP port 53 (used for DNS) as the destination, and provide the domain to be queried with the qname parameter. If all goes well, Scapy will send the packet to the DNS server and print the packet it receives in response, as below. If we wanted to review this data in a nicer format, we could also have run Wireshark or tcpdump at the same time to generate a PCAP. By reviewing the response above, we can see that the Pi-hole has successfully returned a response. Between the DNSRR tags we can see that the DNS server has returned several IP addresses associated with the domain, as we’d see in the output of a tool like nslookup or dig. SendIp Another means to the same end is SendIp - a Linux command line tool that enables you to craft custom packets in a similar way to Scapy. Rather than sending another DNS query, let’s go back to an earlier example and set up a netcat listener on UDP port 904 to receive a text message. sudo sendip -p ipv4 -is 192.168.234.130 -p udp -us 5070 -ud 904 -d &quot;This is a datagram sent with sendip from the command line.&quot; -v 192.168.234.128 The length of the SendIp command reflects the additional flexibility we have compared to when we sent a similar message with netcat. The -p flag sets our protocols - in this case IPv4 at the network layer and UDP at the transport layer - and after each we specify the contents of various header fields. Notice that SendIP allows us to set the IP source address manually with -is as well as the UDP source port with -us. In testing scenarios, including penetration tests, this would allow us to spoof packets from different machines, with much greater control over the packet and header contents than with netcat. Finally, the -d flag declares the contents of the packet payload - in this case the text message, which we can see in the output on the recipient’s system is received by the netcat listener. All of the tools and examples above represent different ways of sending data over a network from the Linux command line. While I used simple text and file exchanges to demonstrate how they work, these tools have many different features and options and can be used in much more complex scenarios. Hopefully this post has given you some idea of how they work and what they are capable of, and given you enough of a starting point to try some experiments of your own.</summary></entry></feed>