<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.0">Jekyll</generator><link href="https://mattcasmith.net/feed.xml" rel="self" type="application/atom+xml" /><link href="https://mattcasmith.net/" rel="alternate" type="text/html" /><updated>2022-03-01T14:47:15+00:00</updated><id>https://mattcasmith.net/feed.xml</id><title type="html">MattCASmith</title><subtitle>A blog about cyber security and technology</subtitle><entry><title type="html">Linux .bash_history: Basics, behaviours, and forensics</title><link href="https://mattcasmith.net/2022/02/22/bash-history-basics-behaviours-forensics" rel="alternate" type="text/html" title="Linux .bash&amp;#95;history&amp;#58; Basics, behaviours, and forensics" /><published>2022-02-22T00:00:00+00:00</published><updated>2022-02-22T00:00:00+00:00</updated><id>https://mattcasmith.net/2022/02/22/bash-history-basics-behaviours-forensics</id><content type="html" xml:base="https://mattcasmith.net/2022/02/22/bash-history-basics-behaviours-forensics">&lt;p&gt;During any incident investigation on a Linux system, one of the most valuable things for responders and forensicators to establish is which commands were run. This is key to finding out what an attacker or malicious user was attempting to do, and what remediation activities are required.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;.bash_history&lt;/code&gt; file, located in each user’s home directory, is usually the investigator’s first stop for this information. This file contains a list of Bash commands recently run by the user and may appear relatively simple at first glance, but there are many ins and outs to its behaviours.&lt;/p&gt;

&lt;p&gt;Most importantly, commands entered in a running Bash terminal are stored in memory and are only written to the file when the terminal is closed. As you can imagine, this means many caveats can be applied to the file’s contents, and in this post I aim to cover some of the most common scenarios, how they affect &lt;code&gt;.bash_history&lt;/code&gt;, and alternatives when it does not contain the activity you are looking for.&lt;/p&gt;

&lt;h3 id=&quot;contents&quot;&gt;Contents&lt;/h3&gt;

&lt;p&gt;1. &lt;a href=&quot;#bash_history-behaviours&quot;&gt;&lt;code&gt;.bash_history&lt;/code&gt; behaviours&lt;/a&gt;&lt;br /&gt;
          a. &lt;a href=&quot;#first-a-note-on-the-histcontrol-variable&quot;&gt;The &lt;code&gt;$HISTCONTROL&lt;/code&gt; variable&lt;/a&gt;&lt;br /&gt;
          b. &lt;a href=&quot;#the-history-command-outputs-a-combined-history-from-bash_history-and-memory&quot;&gt;The &lt;code&gt;history&lt;/code&gt; command&lt;/a&gt;&lt;br /&gt;
          c. &lt;a href=&quot;#commands-are-written-to-bash_history-in-the-order-terminals-are-closed-not-the-order-they-are-run&quot;&gt;Terminal windows and command order&lt;/a&gt;&lt;br /&gt;
          d. &lt;a href=&quot;#commands-run-with-logical-and--appear-as-a-single-entry-in-bash_history&quot;&gt;Logical AND (&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;)&lt;/a&gt;&lt;br /&gt;
          e. &lt;a href=&quot;#commands-run-within-scripts-are-not-reflected-in-bash_history&quot;&gt;Commands within scripts&lt;/a&gt;&lt;br /&gt;
          f. &lt;a href=&quot;#if-a-terminal-is-stopped-with-the-kill-command-commands-are-still-written-to-bash_history&quot;&gt;The &lt;code&gt;kill&lt;/code&gt; command&lt;/a&gt;&lt;br /&gt;
          g. &lt;a href=&quot;#however-if-the--sigkill-switch-is-used-commands-are-not-written-to-bash_history&quot;&gt;The &lt;code&gt;-SIGKILL&lt;/code&gt; switch&lt;/a&gt;&lt;br /&gt;
          h. &lt;a href=&quot;#ssh-sessions-behave-in-a-similar-way-to-standard-terminals&quot;&gt;SSH sessions&lt;/a&gt;&lt;br /&gt;
2. &lt;a href=&quot;#finding-bash-commands-in-memory&quot;&gt;Finding Bash commands in memory&lt;/a&gt;&lt;br /&gt;
3. &lt;a href=&quot;#further-reading&quot;&gt;Further reading&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;bash_history-behaviours&quot;&gt;&lt;code&gt;.bash_history&lt;/code&gt; behaviours&lt;/h3&gt;

&lt;p&gt;To test exactly when commands are and aren’t recorded to the user’s &lt;code&gt;.bash_history&lt;/code&gt; file, I ran a series of tests covering common scenarios in which commands might be run. All of these tests were run on a clean installation of CentOS. Here’s what I found…&lt;/p&gt;

&lt;h4 id=&quot;first-a-note-on-the-histcontrol-variable&quot;&gt;First, a note on the &lt;code&gt;$HISTCONTROL&lt;/code&gt; variable…&lt;/h4&gt;

&lt;p&gt;It is important to check the contents of the &lt;code&gt;$HISTCONTROL&lt;/code&gt; variable on the subject system (after taking images and preserving evidence, so you’re not overwriting &lt;code&gt;.bash_history&lt;/code&gt; with your own commands) as this has a potentially significant bearing on which commands will be written to the file.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;echo $HISTCONTROL&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The command above will return one of the following strings. The table below details which commands &lt;code&gt;.bash_history&lt;/code&gt; will ignore when each of the possible values is present.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;$HISTCONTROL value&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ignorespace&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Excludes commands with a preceding space&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ignoredups&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Excludes subsequent duplicate commands&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ignorespace:ignoredups&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Excludes both commands with a preceding space and subsequent duplicates&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ignoreboth&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Excludes both commands with a preceding space and subsequent duplicates&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Therefore if the system has one of these values in the &lt;code&gt;$HISTCONTROL&lt;/code&gt; variable, there may be commands missing from &lt;code&gt;.bash_history&lt;/code&gt;. If it is set to &lt;code&gt;ignorespace&lt;/code&gt; and the attacker is aware, they could even just slip in a space before each of their commands and write nothing to the history file at all!&lt;/p&gt;

&lt;p&gt;The CentOS system where I performed the testing for this post had &lt;code&gt;$HISTCONTROL&lt;/code&gt; set to &lt;code&gt;ignoredups&lt;/code&gt;. However, I also performed checks across other Linux distros I had available (namely Kali, Tsurugi, and Windows Subsystem for Linux) and it was set to &lt;code&gt;ignoreboth&lt;/code&gt; for them all. So it’s important to check what you’re dealing with on the particular system you’re investigating.&lt;/p&gt;

&lt;p&gt;It’s also trivial to change the contents of the &lt;code&gt;$HISTCONTROL&lt;/code&gt; variable to exclude more commands from &lt;code&gt;.bash_history&lt;/code&gt;, so watch out for evidence it has changed, which will look something like the following:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;export HISTCONTROL=ignoreboth&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;the-history-command-outputs-a-combined-history-from-bash_history-and-memory&quot;&gt;The &lt;code&gt;history&lt;/code&gt; command outputs a combined history from &lt;code&gt;.bash_history&lt;/code&gt; and memory.&lt;/h4&gt;

&lt;p&gt;On a live Linux system, you might run the &lt;code&gt;history&lt;/code&gt; command in a terminal window to review recently executed commands. The output of this command combines both the contents of &lt;code&gt;.bash_history&lt;/code&gt; and any commands held in memory from the current session.&lt;/p&gt;

&lt;p&gt;In the example below, we can see that the test command &lt;code&gt;doesthiscommandshowinhistory?&lt;/code&gt; is returned by the &lt;code&gt;history&lt;/code&gt; command on the left, but has not yet been written to disk as shown on the right.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/02/bash-history_0.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bear in mind that &lt;code&gt;history&lt;/code&gt; will show commands from the memory of only the current Bash terminal - it won’t have access to those belonging to other processes. Since you won’t have access to the attacker’s terminal (unless the incident response team directly interrupted a hands-on-keyboard session), this means it won’t usually be much more useful than &lt;code&gt;.bash_history&lt;/code&gt; itself.&lt;/p&gt;

&lt;h4 id=&quot;commands-are-written-to-bash_history-in-the-order-terminals-are-closed-not-the-order-they-are-run&quot;&gt;Commands are written to &lt;code&gt;.bash_history&lt;/code&gt; in the order terminals are closed, not the order they are run.&lt;/h4&gt;

&lt;p&gt;For this test, I opened two Bash terminals and ran example commands containing text stating which terminal they belonged to and in which order they were run. As I closed the first terminal before the second, you can see that both of its commands appear first in &lt;code&gt;.bash_history&lt;/code&gt;, even though some of the second terminal’s commands were run first. It is the order the Bash processes end that matters.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/02/bash-history_1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As was discussed before, note that if we had run the &lt;code&gt;history&lt;/code&gt; command in one of these terminals, it would only have returned its own commands because each process has its own space in memory.&lt;/p&gt;

&lt;h4 id=&quot;commands-run-with-logical-and--appear-as-a-single-entry-in-bash_history&quot;&gt;Commands run with logical AND (&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;) appear as a single entry in &lt;code&gt;.bash_history&lt;/code&gt;.&lt;/h4&gt;

&lt;p&gt;If we string several commands together using &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; - a technique often used to save time (or in less innocent circumstances to avoid an attacker having to submit commands multiple times and risk attracting unwanted attention) - they will still only appear as one line in &lt;code&gt;.bash_history&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/02/bash-history_1.1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is an important detail as investigators and forensicators, as it preserves the original context in which these commands were run. If they were run back-to-back in this manner, this invites the question: “Why?”&lt;/p&gt;

&lt;h4 id=&quot;commands-run-within-scripts-are-not-reflected-in-bash_history&quot;&gt;Commands run within scripts are not reflected in &lt;code&gt;.bash_history&lt;/code&gt;.&lt;/h4&gt;

&lt;p&gt;To see whether commands in scripts are added to &lt;code&gt;.bash_history&lt;/code&gt; or not, I created a script called &lt;code&gt;folder_test.sh&lt;/code&gt; that would create a new folder, navigate into it, list the contents, navigate back up a level, and then delete the folder. You can see its contents in the left-hand window below.&lt;/p&gt;

&lt;p&gt;On the right, I ran the script with &lt;code&gt;./folder_test.sh&lt;/code&gt;. We know it executed successfully because the contents (i.e. nothing) are printed to the terminal. However, when we check the output of the &lt;code&gt;history&lt;/code&gt; command we can see that although the command that &lt;em&gt;ran&lt;/em&gt; the script was recorded, commands run &lt;em&gt;within&lt;/em&gt; the script were not - an important distinction to remember when investigating Linux systems.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/02/bash-history_1.2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This means that our attacker could potentially download or create a script named something innocuous like &lt;code&gt;file_cleanup.sh&lt;/code&gt;, execute it, and we would be none the wiser as to what it did based on the contents of &lt;code&gt;.bash_history&lt;/code&gt;. There would likely be artefacts elsewhere on the system to give us a clue as to what they were trying to do, but that’s a different topic for another day.&lt;/p&gt;

&lt;h4 id=&quot;if-a-terminal-is-stopped-with-the-kill-command-commands-are-still-written-to-bash_history&quot;&gt;If a terminal is stopped with the &lt;code&gt;kill&lt;/code&gt; command, commands are still written to &lt;code&gt;.bash_history&lt;/code&gt;.&lt;/h4&gt;

&lt;p&gt;This one was simple enough to test. I ran the command &lt;code&gt;doesthiscommandappearinhistory?&lt;/code&gt; in a terminal, then opened a new terminal and killed it using its process ID, then checked &lt;code&gt;.bash_history&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As you can see below, despite the Bash instance being killed, the command was still written to the log (the following &lt;code&gt;echo $$&lt;/code&gt; command was the one I used to identify the process ID of that terminal).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/02/bash-history_2.png&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;however-if-the--sigkill-switch-is-used-commands-are-not-written-to-bash_history&quot;&gt;However, if the &lt;code&gt;-SIGKILL&lt;/code&gt; switch is used, commands are not written to &lt;code&gt;.bash_history&lt;/code&gt;.&lt;/h4&gt;

&lt;p&gt;More useful for red teamers - and something to bear in mind for blue teamers - is that &lt;code&gt;.bash_history&lt;/code&gt; is not written if the terminal is killed with the &lt;code&gt;-SIGKILL&lt;/code&gt; switch. Repeating the same experiment as above but with the additional switch meant no commands were written to disk.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/02/bash-history_3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is because by default &lt;code&gt;kill&lt;/code&gt; sends the &lt;code&gt;SIGTERM&lt;/code&gt; signal, which gracefully kills the process and allows Bash to write to &lt;code&gt;.bash_history&lt;/code&gt; as it is closing down. &lt;code&gt;SIGKILL&lt;/code&gt;, on the other hand, kills the process immediately before the commands can be written to the file.&lt;/p&gt;

&lt;h4 id=&quot;ssh-sessions-behave-in-a-similar-way-to-standard-terminals&quot;&gt;SSH sessions behave in a similar way to standard terminals.&lt;/h4&gt;

&lt;p&gt;If we run commands within an SSH session and then quit with &lt;code&gt;exit&lt;/code&gt; or by closing the window, &lt;code&gt;.bash_history&lt;/code&gt; is written in much the same way as it usually would be. The same also applies if we close the Command Prompt running the SSH session with Task Manager - presumably because Windows gives it time to tear down the connection in the background.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/02/bash-history_4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Interestingly, if connecting via SSH from a Linux terminal, running a standard &lt;code&gt;kill&lt;/code&gt; command against the terminal will not close it until the user has ended the SSH session themselves. Using the &lt;code&gt;-SIGKILL&lt;/code&gt; switch ends the terminal process and SSH session immediately, but still writes the commands run during the session to the user’s &lt;code&gt;.bash_history&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;This is likely because this is the work of &lt;code&gt;sshd&lt;/code&gt;, which sees its client kill the connection but itself closes gracefully. If we use &lt;code&gt;kill&lt;/code&gt; directly on &lt;code&gt;sshd&lt;/code&gt; itself - even with the &lt;code&gt;-SIGKILL&lt;/code&gt; switch - commands are still written to the file, which makes me wonder whether there is actually a way around this for SSH sessions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/02/bash-history_5.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;finding-bash-commands-in-memory&quot;&gt;Finding Bash commands in memory&lt;/h3&gt;

&lt;p&gt;All the tests above showed when commands are and are not written to the on-disk &lt;code&gt;.bash_history&lt;/code&gt; file. Now let’s see what we can do when an attacker has an active session (and therefore commands in memory) but either &lt;code&gt;.bash_history&lt;/code&gt; has not been written yet or they have cleared its contents. This may also work for recently closed Bash terminal sessions that did not end gracefully.&lt;/p&gt;

&lt;p&gt;The first thing we need to do is take a memory image of the Linux system - for this we can use a tool called &lt;a href=&quot;https://github.com/504ensicsLabs/LiME&quot; target=&quot;_blank&quot;&gt;Linux Memory Extractor (LiME)&lt;/a&gt;. In a live incident scenario there are mechanisms to push the image to external storage or across the network, but I ran a basic command to create it locally.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;sudo insmod lime-4.18.0-240.22.1.el8_3.x86_64.ko “path=/mem-img.mem format=lime”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To analyse our new memory image, we’ll use &lt;a href=&quot;https://www.volatilityfoundation.org/&quot; target=&quot;_blank&quot;&gt;Volatility&lt;/a&gt;, which is currently considered the pinnacle of memory forensics toolkits. It’s not quite as simple to run Volatility against Linux memory images as it is for Windows images. I won’t go into the full process here, but you need to &lt;a href=&quot;https://github.com/volatilityfoundation/volatility/wiki/Linux&quot; target=&quot;_blank&quot;&gt;create your own profile&lt;/a&gt; for the specific Linux distro and kernel version from which the image was captured.&lt;/p&gt;

&lt;p&gt;Once the profile is in place and ready to go, we can run Volatility’s &lt;code&gt;linux_bash&lt;/code&gt; module with the following command to search for Bash commands held in memory.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;python2 vol.py -f mem-img.mem –profile=LinuxCentOSx64 linux_bash&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the screenshot below, you can see a small extract from the results, are ordered first by the process ID of the Bash terminal to which the commands belonged and then by the time they were run. This includes commands that were run too recently to have been written to &lt;code&gt;.bash_history&lt;/code&gt;, and towards the bottom you’ll even see the commands I ran to set up and run LiME.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/02/bash-history_6.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One small word of warning regarding timestamps… Everything from the &lt;code&gt;ls&lt;/code&gt; command at &lt;code&gt;15:59:18&lt;/code&gt; downwards appears to be correct, but you’ll probably notice that all the commands above that allegedly ran at exactly the same second, which is obviously not right. Further investigation is needed to work out why exactly that is, but it’s likely that there is some limit to the number of Bash command timestamps stored in memory, or that Volatility cannot always read them accurately.&lt;/p&gt;

&lt;h3 id=&quot;further-reading&quot;&gt;Further reading&lt;/h3&gt;

&lt;p&gt;The links below lead to pages that either inspired this post or provided useful information to compile it, including some more in-depth technical information on various features discussed above.&lt;/p&gt;

&lt;p&gt;• &lt;a href=&quot;https://www.gnu.org/software/bash/manual/html_node/Bash-History-Facilities.html&quot; target=&quot;_blank&quot;&gt;Bash history manual page&lt;/a&gt; (gnu.org)&lt;br /&gt;
• &lt;a href=&quot;https://www.geeksforgeeks.org/histcontrol-command-in-linux-with-examples/&quot; target=&quot;_blank&quot;&gt;$HISTCONTROL command in Linux with examples&lt;/a&gt; (geeksforgeeks.org)&lt;br /&gt;
• &lt;a href=&quot;https://linuxhandbook.com/sigterm-vs-sigkill/&quot; target=&quot;_blank&quot;&gt;SIGINT vs SIGKILL&lt;/a&gt; (linuxhandbook.com)&lt;br /&gt;
• &lt;a href=&quot;https://github.com/volatilityfoundation/volatility/wiki/Linux&quot; target=&quot;_blank&quot;&gt;Volatility Linux documentation&lt;/a&gt; (github.com)&lt;br /&gt;
• &lt;a href=&quot;https://www.crowdstrike.com/blog/how-to-extract-memory-information-to-spot-linux-malware/&quot; target=&quot;_blank&quot;&gt;How to extract memory information to spot Linux malware&lt;/a&gt; (crowdstrike.com)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Updated 25/02/2022 to add section on the $HISTCONTROL variable.&lt;/em&gt;&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">During any incident investigation on a Linux system, one of the most valuable things for responders and forensicators to establish is which commands were run. This is key to finding out what an attacker or malicious user was attempting to do, and what remediation activities are required. The .bash_history file, located in each user’s home directory, is usually the investigator’s first stop for this information. This file contains a list of Bash commands recently run by the user and may appear relatively simple at first glance, but there are many ins and outs to its behaviours. Most importantly, commands entered in a running Bash terminal are stored in memory and are only written to the file when the terminal is closed. As you can imagine, this means many caveats can be applied to the file’s contents, and in this post I aim to cover some of the most common scenarios, how they affect .bash_history, and alternatives when it does not contain the activity you are looking for. Contents 1. .bash_history behaviours           a. The $HISTCONTROL variable           b. The history command           c. Terminal windows and command order           d. Logical AND (&amp;amp;&amp;amp;)           e. Commands within scripts           f. The kill command           g. The -SIGKILL switch           h. SSH sessions 2. Finding Bash commands in memory 3. Further reading .bash_history behaviours To test exactly when commands are and aren’t recorded to the user’s .bash_history file, I ran a series of tests covering common scenarios in which commands might be run. All of these tests were run on a clean installation of CentOS. Here’s what I found… First, a note on the $HISTCONTROL variable… It is important to check the contents of the $HISTCONTROL variable on the subject system (after taking images and preserving evidence, so you’re not overwriting .bash_history with your own commands) as this has a potentially significant bearing on which commands will be written to the file. echo $HISTCONTROL The command above will return one of the following strings. The table below details which commands .bash_history will ignore when each of the possible values is present. $HISTCONTROL value Description ignorespace Excludes commands with a preceding space ignoredups Excludes subsequent duplicate commands ignorespace:ignoredups Excludes both commands with a preceding space and subsequent duplicates ignoreboth Excludes both commands with a preceding space and subsequent duplicates Therefore if the system has one of these values in the $HISTCONTROL variable, there may be commands missing from .bash_history. If it is set to ignorespace and the attacker is aware, they could even just slip in a space before each of their commands and write nothing to the history file at all! The CentOS system where I performed the testing for this post had $HISTCONTROL set to ignoredups. However, I also performed checks across other Linux distros I had available (namely Kali, Tsurugi, and Windows Subsystem for Linux) and it was set to ignoreboth for them all. So it’s important to check what you’re dealing with on the particular system you’re investigating. It’s also trivial to change the contents of the $HISTCONTROL variable to exclude more commands from .bash_history, so watch out for evidence it has changed, which will look something like the following: export HISTCONTROL=ignoreboth The history command outputs a combined history from .bash_history and memory. On a live Linux system, you might run the history command in a terminal window to review recently executed commands. The output of this command combines both the contents of .bash_history and any commands held in memory from the current session. In the example below, we can see that the test command doesthiscommandshowinhistory? is returned by the history command on the left, but has not yet been written to disk as shown on the right. Bear in mind that history will show commands from the memory of only the current Bash terminal - it won’t have access to those belonging to other processes. Since you won’t have access to the attacker’s terminal (unless the incident response team directly interrupted a hands-on-keyboard session), this means it won’t usually be much more useful than .bash_history itself. Commands are written to .bash_history in the order terminals are closed, not the order they are run. For this test, I opened two Bash terminals and ran example commands containing text stating which terminal they belonged to and in which order they were run. As I closed the first terminal before the second, you can see that both of its commands appear first in .bash_history, even though some of the second terminal’s commands were run first. It is the order the Bash processes end that matters. As was discussed before, note that if we had run the history command in one of these terminals, it would only have returned its own commands because each process has its own space in memory. Commands run with logical AND (&amp;amp;&amp;amp;) appear as a single entry in .bash_history. If we string several commands together using &amp;amp;&amp;amp; - a technique often used to save time (or in less innocent circumstances to avoid an attacker having to submit commands multiple times and risk attracting unwanted attention) - they will still only appear as one line in .bash_history. This is an important detail as investigators and forensicators, as it preserves the original context in which these commands were run. If they were run back-to-back in this manner, this invites the question: “Why?” Commands run within scripts are not reflected in .bash_history. To see whether commands in scripts are added to .bash_history or not, I created a script called folder_test.sh that would create a new folder, navigate into it, list the contents, navigate back up a level, and then delete the folder. You can see its contents in the left-hand window below. On the right, I ran the script with ./folder_test.sh. We know it executed successfully because the contents (i.e. nothing) are printed to the terminal. However, when we check the output of the history command we can see that although the command that ran the script was recorded, commands run within the script were not - an important distinction to remember when investigating Linux systems. This means that our attacker could potentially download or create a script named something innocuous like file_cleanup.sh, execute it, and we would be none the wiser as to what it did based on the contents of .bash_history. There would likely be artefacts elsewhere on the system to give us a clue as to what they were trying to do, but that’s a different topic for another day. If a terminal is stopped with the kill command, commands are still written to .bash_history. This one was simple enough to test. I ran the command doesthiscommandappearinhistory? in a terminal, then opened a new terminal and killed it using its process ID, then checked .bash_history. As you can see below, despite the Bash instance being killed, the command was still written to the log (the following echo $$ command was the one I used to identify the process ID of that terminal). However, if the -SIGKILL switch is used, commands are not written to .bash_history. More useful for red teamers - and something to bear in mind for blue teamers - is that .bash_history is not written if the terminal is killed with the -SIGKILL switch. Repeating the same experiment as above but with the additional switch meant no commands were written to disk. This is because by default kill sends the SIGTERM signal, which gracefully kills the process and allows Bash to write to .bash_history as it is closing down. SIGKILL, on the other hand, kills the process immediately before the commands can be written to the file. SSH sessions behave in a similar way to standard terminals. If we run commands within an SSH session and then quit with exit or by closing the window, .bash_history is written in much the same way as it usually would be. The same also applies if we close the Command Prompt running the SSH session with Task Manager - presumably because Windows gives it time to tear down the connection in the background. Interestingly, if connecting via SSH from a Linux terminal, running a standard kill command against the terminal will not close it until the user has ended the SSH session themselves. Using the -SIGKILL switch ends the terminal process and SSH session immediately, but still writes the commands run during the session to the user’s .bash_history file. This is likely because this is the work of sshd, which sees its client kill the connection but itself closes gracefully. If we use kill directly on sshd itself - even with the -SIGKILL switch - commands are still written to the file, which makes me wonder whether there is actually a way around this for SSH sessions. Finding Bash commands in memory All the tests above showed when commands are and are not written to the on-disk .bash_history file. Now let’s see what we can do when an attacker has an active session (and therefore commands in memory) but either .bash_history has not been written yet or they have cleared its contents. This may also work for recently closed Bash terminal sessions that did not end gracefully. The first thing we need to do is take a memory image of the Linux system - for this we can use a tool called Linux Memory Extractor (LiME). In a live incident scenario there are mechanisms to push the image to external storage or across the network, but I ran a basic command to create it locally. sudo insmod lime-4.18.0-240.22.1.el8_3.x86_64.ko “path=/mem-img.mem format=lime” To analyse our new memory image, we’ll use Volatility, which is currently considered the pinnacle of memory forensics toolkits. It’s not quite as simple to run Volatility against Linux memory images as it is for Windows images. I won’t go into the full process here, but you need to create your own profile for the specific Linux distro and kernel version from which the image was captured. Once the profile is in place and ready to go, we can run Volatility’s linux_bash module with the following command to search for Bash commands held in memory. python2 vol.py -f mem-img.mem –profile=LinuxCentOSx64 linux_bash In the screenshot below, you can see a small extract from the results, are ordered first by the process ID of the Bash terminal to which the commands belonged and then by the time they were run. This includes commands that were run too recently to have been written to .bash_history, and towards the bottom you’ll even see the commands I ran to set up and run LiME. One small word of warning regarding timestamps… Everything from the ls command at 15:59:18 downwards appears to be correct, but you’ll probably notice that all the commands above that allegedly ran at exactly the same second, which is obviously not right. Further investigation is needed to work out why exactly that is, but it’s likely that there is some limit to the number of Bash command timestamps stored in memory, or that Volatility cannot always read them accurately. Further reading The links below lead to pages that either inspired this post or provided useful information to compile it, including some more in-depth technical information on various features discussed above. • Bash history manual page (gnu.org) • $HISTCONTROL command in Linux with examples (geeksforgeeks.org) • SIGINT vs SIGKILL (linuxhandbook.com) • Volatility Linux documentation (github.com) • How to extract memory information to spot Linux malware (crowdstrike.com) Updated 25/02/2022 to add section on the $HISTCONTROL variable.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mattcasmith.net/wp-content/uploads/2022/02/bash-history_1.png" /><media:content medium="image" url="https://mattcasmith.net/wp-content/uploads/2022/02/bash-history_1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SANS Holiday Hack Challenge 2021: Slot machine walkthrough</title><link href="https://mattcasmith.net/2022/01/08/sans-holiday-hack-2021-slot-machine" rel="alternate" type="text/html" title="SANS Holiday Hack Challenge 2021&amp;#58; Slot machine walkthrough" /><published>2022-01-08T00:01:00+00:00</published><updated>2022-01-08T00:01:00+00:00</updated><id>https://mattcasmith.net/2022/01/08/sans-holiday-hack-2021-slot-machine</id><content type="html" xml:base="https://mattcasmith.net/2022/01/08/sans-holiday-hack-2021-slot-machine">&lt;p&gt;Here’s one more writeup from the &lt;a href=&quot;https://www.sans.org/mlp/holiday-hack-challenge/&quot; target=&quot;_blank&quot;&gt;SANS Holiday Hack Challenge&lt;/a&gt;! The slot machine hack was one of the showpiece challenges this year, so I thought I’d put together a quick blog post to guide you through the process of identifying and exploiting a vulnerability in the game.&lt;/p&gt;

&lt;h3 id=&quot;the-challenge&quot;&gt;The challenge&lt;/h3&gt;

&lt;p&gt;Our task is clear enough. We’re given a link to an online slot machine and the following request:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Test the security of Jack Frost’s slot machines. What does the Jack Frost Tower casino security team threaten to do when your coin total exceeds 1,000? Submit the string in the server &lt;code&gt;data.response&lt;/code&gt; element.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let’s follow the link and take a look at the slot machine. It’s fairly typical for such games (at least as per my understanding following &lt;a href=&quot;/2019/08/26/im-back-def-con-inspired-hacking/&quot;&gt;a short trip to Las Vegas for Black Hat and Def Con&lt;/a&gt;). You can choose various options before placing a bet by hitting the Spin button. Your credit is updated based on the outcome.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/01/sans-holiday-challenge-slots_1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since our objective is to hit more than 1,000 credit, we &lt;em&gt;could&lt;/em&gt; just play the slots for a long time to try to get lucky. But that would take forever. Instead, let’s take under the hood to see what we can hack…&lt;/p&gt;

&lt;h3 id=&quot;observing-requests&quot;&gt;Observing requests&lt;/h3&gt;

&lt;p&gt;Like any web application, the slot machine must be communicating with a server behind the scenes to make playing the game possible. By using the Burp Suite proxy’s intercept function, we can hold requests and responses between the web browser and the server to give us time to review their contents and see what’s going on when we place a bet. Here’s what is sent when we hit the Spin button…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/01/sans-holiday-challenge-slots_2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This looks like a pretty standard web request. We can see that this is a &lt;code&gt;POST&lt;/code&gt; request, which means that the client is transmitting information to the server (&lt;code&gt;slots.jackfrosttower.com&lt;/code&gt;). We can see that the request provides some cookie information, details about the page we’re on and our user agent, and so on. This is all pretty normal for this kind of request. But what about that last line?&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;betamount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Comparing these variable names to the GUI, we can ascertain that this is our browser telling the server how much we’re betting, the number of lines we’re betting on, and the cost per line. Interesting. By tweaking these values, we might be able to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Change the bet amount to a bigger stake than we can actually afford&lt;/li&gt;
  &lt;li&gt;Bet on more lines than actually exist on the slot machine&lt;/li&gt;
  &lt;li&gt;Adjust the cost per line to alter the slot machine’s financial calculations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From here, it’s a matter of trial and error. Attempting the first two potential exploits returns a &lt;code&gt;Server Error&lt;/code&gt; response. That leaves only one option - but how can we use the &lt;code&gt;cpl&lt;/code&gt; variable to our advantage?&lt;/p&gt;

&lt;h3 id=&quot;crafting-a-malicious-request&quot;&gt;Crafting a malicious request&lt;/h3&gt;

&lt;p&gt;So here’s the plan: We’re going to set &lt;code&gt;cpl&lt;/code&gt; to a negative value to confuse the game if this input isn’t validated. For example, if we set it to &lt;code&gt;-100&lt;/code&gt; and there are 20 lines to play, that spin would cost us -2,000 credits to play and therefore actually &lt;em&gt;increase&lt;/em&gt; our credit by 2,000 (and any winnings from the spin).&lt;/p&gt;

&lt;p&gt;To do this, we hit the Spin button on the slot machine’s GUI. Burp Suite intercepts the request. We must very quickly change the &lt;code&gt;cpl&lt;/code&gt; value to &lt;code&gt;-100&lt;/code&gt; and hit the Forward button before the spin times out.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/01/sans-holiday-challenge-slots_3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It looks like the server doesn’t check that &lt;code&gt;cpl&lt;/code&gt; is set to a valid value before executing the spin. All goes to plan and when the play is complete our credit is now in the thousands! Mission complete - almost.&lt;/p&gt;

&lt;p&gt;Our task was actually to find a value in the JSON response once our credit exceeds 1,000. To do this, we can take a look for the relevant request and response on Burp Suite’s HTTP history page. Sure enough, in the &lt;code&gt;data.response&lt;/code&gt; field, we can see the following message from casino security:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/01/sans-holiday-challenge-slots_4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And that’s our flag: &lt;code&gt;I'm going to have some bouncer trolls bounce you right out of this casino!&lt;/code&gt; Hopefully we have enough time to submit it and take the credit for our hacking first.&lt;/p&gt;

&lt;p&gt;If you found this walkthrough interesting, please consider &lt;a href=&quot;https://twitter.com/mattcasmith&quot; target=&quot;_blank&quot;&gt;following me on Twitter&lt;/a&gt;, and don’t forget to sign up for the next SANS Holiday Hack Challenge circa December 2022!&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">Here’s one more writeup from the SANS Holiday Hack Challenge! The slot machine hack was one of the showpiece challenges this year, so I thought I’d put together a quick blog post to guide you through the process of identifying and exploiting a vulnerability in the game. The challenge Our task is clear enough. We’re given a link to an online slot machine and the following request: Test the security of Jack Frost’s slot machines. What does the Jack Frost Tower casino security team threaten to do when your coin total exceeds 1,000? Submit the string in the server data.response element. Let’s follow the link and take a look at the slot machine. It’s fairly typical for such games (at least as per my understanding following a short trip to Las Vegas for Black Hat and Def Con). You can choose various options before placing a bet by hitting the Spin button. Your credit is updated based on the outcome. Since our objective is to hit more than 1,000 credit, we could just play the slots for a long time to try to get lucky. But that would take forever. Instead, let’s take under the hood to see what we can hack… Observing requests Like any web application, the slot machine must be communicating with a server behind the scenes to make playing the game possible. By using the Burp Suite proxy’s intercept function, we can hold requests and responses between the web browser and the server to give us time to review their contents and see what’s going on when we place a bet. Here’s what is sent when we hit the Spin button… This looks like a pretty standard web request. We can see that this is a POST request, which means that the client is transmitting information to the server (slots.jackfrosttower.com). We can see that the request provides some cookie information, details about the page we’re on and our user agent, and so on. This is all pretty normal for this kind of request. But what about that last line? betamount=1&amp;amp;numline=20&amp;amp;cpl=0.1 Comparing these variable names to the GUI, we can ascertain that this is our browser telling the server how much we’re betting, the number of lines we’re betting on, and the cost per line. Interesting. By tweaking these values, we might be able to: Change the bet amount to a bigger stake than we can actually afford Bet on more lines than actually exist on the slot machine Adjust the cost per line to alter the slot machine’s financial calculations From here, it’s a matter of trial and error. Attempting the first two potential exploits returns a Server Error response. That leaves only one option - but how can we use the cpl variable to our advantage? Crafting a malicious request So here’s the plan: We’re going to set cpl to a negative value to confuse the game if this input isn’t validated. For example, if we set it to -100 and there are 20 lines to play, that spin would cost us -2,000 credits to play and therefore actually increase our credit by 2,000 (and any winnings from the spin). To do this, we hit the Spin button on the slot machine’s GUI. Burp Suite intercepts the request. We must very quickly change the cpl value to -100 and hit the Forward button before the spin times out. It looks like the server doesn’t check that cpl is set to a valid value before executing the spin. All goes to plan and when the play is complete our credit is now in the thousands! Mission complete - almost. Our task was actually to find a value in the JSON response once our credit exceeds 1,000. To do this, we can take a look for the relevant request and response on Burp Suite’s HTTP history page. Sure enough, in the data.response field, we can see the following message from casino security: And that’s our flag: I'm going to have some bouncer trolls bounce you right out of this casino! Hopefully we have enough time to submit it and take the credit for our hacking first. If you found this walkthrough interesting, please consider following me on Twitter, and don’t forget to sign up for the next SANS Holiday Hack Challenge circa December 2022!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mattcasmith.net/wp-content/uploads/2022/01/sans-holiday-challenge-slots_1.png" /><media:content medium="image" url="https://mattcasmith.net/wp-content/uploads/2022/01/sans-holiday-challenge-slots_1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SANS Holiday Hack Challenge 2021: Yara rule analysis walkthrough</title><link href="https://mattcasmith.net/2022/01/08/sans-holiday-hack-2021-yara-rules" rel="alternate" type="text/html" title="SANS Holiday Hack Challenge 2021&amp;#58; Yara rule analysis walkthrough" /><published>2022-01-08T00:00:00+00:00</published><updated>2022-01-08T00:00:00+00:00</updated><id>https://mattcasmith.net/2022/01/08/sans-holiday-hack-2021-yara-rules</id><content type="html" xml:base="https://mattcasmith.net/2022/01/08/sans-holiday-hack-2021-yara-rules">&lt;p&gt;Over the Christmas break I took part in an annual tradition - the &lt;a href=&quot;https://www.sans.org/mlp/holiday-hack-challenge/&quot; target=&quot;_blank&quot;&gt;SANS Holiday Hack Challenge&lt;/a&gt;! For 2021 the team had put together a fresh set of challenges for this festive CTF, and now that the deadline for submissions and subsequent embargo has passed, I thought I’d share a write-up of a challenge that appeared - from the in-game chat window at least - to catch a few people out.&lt;/p&gt;

&lt;p&gt;This challenge is all about Yara rules, which detect malware based on a series of conditions set out in a rule file. As set out in the screenshot below, Santa’s elves are having problems running an application that measures the sweetness levels of the candy they’re manufacturing. It’s heavily hinted that it’s being blocked because it’s matching a Yara rule, and that we might be able to change the program to bypass it.&lt;/p&gt;

&lt;p&gt;To start, let’s try running the app using &lt;code&gt;./the_critical_elf_app&lt;/code&gt; to see what we get…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/01/sans-holiday-challenge-yara_0.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What exactly are we seeing here? Well, our app has been blocked from executing because it has matched a Yara rule - &lt;code&gt;rule_135&lt;/code&gt;, to be precise. To get around this rule-based protection we have two options:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Change the Yara rule so that it no longer matches on the program&lt;/li&gt;
  &lt;li&gt;Change the program so that it no longer matches the Yara rule&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this challenge our user account does not have the privileges to amend Yara rules, so the second option is our only choice. Our first step will be to work out why the program is matching the rule.&lt;/p&gt;

&lt;h3 id=&quot;rule_135&quot;&gt;rule_135&lt;/h3&gt;

&lt;p&gt;If we &lt;code&gt;cd&lt;/code&gt; into a folder called &lt;code&gt;yara_rules&lt;/code&gt;, we can see a file called &lt;code&gt;rules.yar&lt;/code&gt; that contains all the Yara rules in use. Viewing its contents with &lt;code&gt;nano&lt;/code&gt; or &lt;code&gt;cat&lt;/code&gt; would take an age to scroll through, so instead we can use a &lt;code&gt;grep&lt;/code&gt; command that will return the 30 lines following a match on &lt;code&gt;rule_135&lt;/code&gt;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rule_135&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/01/sans-holiday-challenge-yara_1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What does this show us? It turns out &lt;code&gt;rule_135&lt;/code&gt; is looking for a single string in the file: &lt;code&gt;candycane&lt;/code&gt; (in this case the &lt;code&gt;condition&lt;/code&gt; clause means it’s searching for this string only - we’ll see some more complex examples later on in the challenge). Now we know what we’re looking for, we can use &lt;code&gt;emacs&lt;/code&gt; to open &lt;code&gt;the_critical_elf_app&lt;/code&gt; and search for the offending string.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/01/sans-holiday-challenge-yara_2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With a little patience and scrolling, we can see that the string &lt;code&gt;candycane&lt;/code&gt; appears in the file. You might be wondering why in the screenshot above I’ve changed it to &lt;code&gt;candycand&lt;/code&gt; rather than deleting it entirely. That’s because we should try to alter the program as little as possible - if we change the number of characters, it is possible that references to sections later in the file may become inaccurate and the program may fail to run at all. Code is fragile stuff (something easily learnt from experience).&lt;/p&gt;

&lt;p&gt;Let’s save our changes and try running &lt;code&gt;the_critical_elf_app&lt;/code&gt; again…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/01/sans-holiday-challenge-yara_3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Oh no! While our adjustment has allowed execution to progress beyond the check for &lt;code&gt;rule_135&lt;/code&gt;, it’s now being blocked because it matches &lt;code&gt;rule_1056&lt;/code&gt;. I guess it’s back to &lt;code&gt;rules.yar&lt;/code&gt; for us…&lt;/p&gt;

&lt;h3 id=&quot;rule_1056&quot;&gt;rule_1056&lt;/h3&gt;

&lt;p&gt;Running another extended &lt;code&gt;grep&lt;/code&gt; on the Yara rule file shows us that &lt;code&gt;rule_1056&lt;/code&gt; requires a match on two strings, represented by the hex &lt;code&gt;6c 6962 632e 736f 2e36&lt;/code&gt; and &lt;code&gt;726f 6772 616d 2121&lt;/code&gt;. This is trickier, because those two don’t appear in their current form in &lt;code&gt;the_critical_elf_app&lt;/code&gt;…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/01/sans-holiday-challenge-yara_4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To find what’s matching the rule, we first need to use &lt;a href=&quot;https://gchq.github.io/CyberChef/&quot; target=&quot;_blank&quot;&gt;GCHQ’s CyberChef tool&lt;/a&gt; to convert the hex from &lt;code&gt;rules.yar&lt;/code&gt; to ASCII text. As shown below, after being passed through the &lt;code&gt;from hex&lt;/code&gt; filter they come out as &lt;code&gt;libc.so.6&lt;/code&gt; and &lt;code&gt;rogram!!&lt;/code&gt;. If Yara finds both of these strings, execution will be blocked.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/01/sans-holiday-challenge-yara_5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The important thing to note here is that &lt;em&gt;both&lt;/em&gt; strings need to match for Yara to block anything. One is not enough. This works in our favour when we return to &lt;code&gt;emacs&lt;/code&gt; and examine the file, because &lt;code&gt;libc.so.6&lt;/code&gt; looks like it’ll be required for the program to run.&lt;/p&gt;

&lt;p&gt;Meanwhile, &lt;code&gt;This is critical for the execution of this program!!&lt;/code&gt; looks like a comment, so as before, let’s try changing a single character and see what happens.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/01/sans-holiday-challenge-yara_6.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Well, we’re a step closer to a solution, at least. When we run the file with &lt;code&gt;./the_critical_elf_app&lt;/code&gt;, we now receive an error because the program has matched Yara’s &lt;code&gt;rule_1732&lt;/code&gt;. Another rule to investigate…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/01/sans-holiday-challenge-yara_7.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;rule_1732&quot;&gt;rule_1732&lt;/h3&gt;

&lt;p&gt;When we &lt;code&gt;grep&lt;/code&gt; for the contents of &lt;code&gt;rule_1732&lt;/code&gt;, we can see that this rule is a little more complex than those we’ve seen before. First of all, our eyes are drawn to the 20 different strings this rule looks for. The &lt;code&gt;condition&lt;/code&gt; clause says the program must match ten of them to be blocked. But there are more conditions that must be met - including that the file size is less than 50kb.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/01/sans-holiday-challenge-yara_8.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;My first thought was to take a similar approach to the previous rules and to change enough of the file to avoid matching ten of the strings, but this proved more challenging than I thought. While strings like &lt;code&gt;its_a_holly_jolly_variable&lt;/code&gt; might be easy enough to substitute out, terms like &lt;code&gt;GLIBC_2.2.5&lt;/code&gt; are more likely to be essential for the program to run. All my attempts either didn’t change enough to evade detection by Yara, or changed too much and broke the app.&lt;/p&gt;

&lt;p&gt;I then turned my attention to the other conditions. The file size was initially about 16kb - could I bump it up to more than 50kb? I decided the best way to do this was to add some padding to the file, but encoding issues meant I couldn’t &lt;code&gt;nano&lt;/code&gt; or &lt;code&gt;echo&lt;/code&gt; in a bunch of nonsense without breaking the program.&lt;/p&gt;

&lt;p&gt;I obviously needed a more creative approach… It was clear I needed to tackle the problem using &lt;code&gt;emacs&lt;/code&gt;, but after some Googling regarding the clunky Linux terminal copy and paste options I was no closer to a solution and running out of time. Sure of the answer but not the method, I took some inspiration from an old episode of &lt;em&gt;The Simpsons&lt;/em&gt; to come to an outside-the-box but very practical solution…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/01/sans-holiday-challenge-yara_9.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That’s right - I just fired up &lt;code&gt;emacs&lt;/code&gt;, scrolled to the end of the document, and left my water bottle resting on the space bar until enough characters had been added to push the file over the 50kb minimum. It took a while (in truth I went and handled some other stuff while it was running), but after saving the file and using &lt;code&gt;ls -l&lt;/code&gt; to check it was now more than 50kb, I ran &lt;code&gt;the_critical_elf_app&lt;/code&gt; one last time and…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2022/01/sans-holiday-challenge-yara_10.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Mission accomplished! I’m sure there were more graceful ways to reach the same outcome, but I’ll put this one down as more proof that a decent understanding of the problem and creative thinking can be just as important in cyber security as any deep technical knowledge in many situations.&lt;/p&gt;

&lt;p&gt;And if you’re wondering what &lt;em&gt;that&lt;/em&gt; hex means, it comes out as &lt;code&gt;Jolly Enough, Overtime Approved&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you found this walkthrough interesting, please consider &lt;a href=&quot;https://twitter.com/mattcasmith&quot; target=&quot;_blank&quot;&gt;following me on Twitter&lt;/a&gt;, and don’t forget to sign up for the next SANS Holiday Hack Challenge circa December 2022!&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">Over the Christmas break I took part in an annual tradition - the SANS Holiday Hack Challenge! For 2021 the team had put together a fresh set of challenges for this festive CTF, and now that the deadline for submissions and subsequent embargo has passed, I thought I’d share a write-up of a challenge that appeared - from the in-game chat window at least - to catch a few people out. This challenge is all about Yara rules, which detect malware based on a series of conditions set out in a rule file. As set out in the screenshot below, Santa’s elves are having problems running an application that measures the sweetness levels of the candy they’re manufacturing. It’s heavily hinted that it’s being blocked because it’s matching a Yara rule, and that we might be able to change the program to bypass it. To start, let’s try running the app using ./the_critical_elf_app to see what we get… What exactly are we seeing here? Well, our app has been blocked from executing because it has matched a Yara rule - rule_135, to be precise. To get around this rule-based protection we have two options: Change the Yara rule so that it no longer matches on the program Change the program so that it no longer matches the Yara rule In this challenge our user account does not have the privileges to amend Yara rules, so the second option is our only choice. Our first step will be to work out why the program is matching the rule. rule_135 If we cd into a folder called yara_rules, we can see a file called rules.yar that contains all the Yara rules in use. Viewing its contents with nano or cat would take an age to scroll through, so instead we can use a grep command that will return the 30 lines following a match on rule_135: grep rule_135 rules.yar -A 30 What does this show us? It turns out rule_135 is looking for a single string in the file: candycane (in this case the condition clause means it’s searching for this string only - we’ll see some more complex examples later on in the challenge). Now we know what we’re looking for, we can use emacs to open the_critical_elf_app and search for the offending string. With a little patience and scrolling, we can see that the string candycane appears in the file. You might be wondering why in the screenshot above I’ve changed it to candycand rather than deleting it entirely. That’s because we should try to alter the program as little as possible - if we change the number of characters, it is possible that references to sections later in the file may become inaccurate and the program may fail to run at all. Code is fragile stuff (something easily learnt from experience). Let’s save our changes and try running the_critical_elf_app again… Oh no! While our adjustment has allowed execution to progress beyond the check for rule_135, it’s now being blocked because it matches rule_1056. I guess it’s back to rules.yar for us… rule_1056 Running another extended grep on the Yara rule file shows us that rule_1056 requires a match on two strings, represented by the hex 6c 6962 632e 736f 2e36 and 726f 6772 616d 2121. This is trickier, because those two don’t appear in their current form in the_critical_elf_app… To find what’s matching the rule, we first need to use GCHQ’s CyberChef tool to convert the hex from rules.yar to ASCII text. As shown below, after being passed through the from hex filter they come out as libc.so.6 and rogram!!. If Yara finds both of these strings, execution will be blocked. The important thing to note here is that both strings need to match for Yara to block anything. One is not enough. This works in our favour when we return to emacs and examine the file, because libc.so.6 looks like it’ll be required for the program to run. Meanwhile, This is critical for the execution of this program!! looks like a comment, so as before, let’s try changing a single character and see what happens. Well, we’re a step closer to a solution, at least. When we run the file with ./the_critical_elf_app, we now receive an error because the program has matched Yara’s rule_1732. Another rule to investigate… rule_1732 When we grep for the contents of rule_1732, we can see that this rule is a little more complex than those we’ve seen before. First of all, our eyes are drawn to the 20 different strings this rule looks for. The condition clause says the program must match ten of them to be blocked. But there are more conditions that must be met - including that the file size is less than 50kb. My first thought was to take a similar approach to the previous rules and to change enough of the file to avoid matching ten of the strings, but this proved more challenging than I thought. While strings like its_a_holly_jolly_variable might be easy enough to substitute out, terms like GLIBC_2.2.5 are more likely to be essential for the program to run. All my attempts either didn’t change enough to evade detection by Yara, or changed too much and broke the app. I then turned my attention to the other conditions. The file size was initially about 16kb - could I bump it up to more than 50kb? I decided the best way to do this was to add some padding to the file, but encoding issues meant I couldn’t nano or echo in a bunch of nonsense without breaking the program. I obviously needed a more creative approach… It was clear I needed to tackle the problem using emacs, but after some Googling regarding the clunky Linux terminal copy and paste options I was no closer to a solution and running out of time. Sure of the answer but not the method, I took some inspiration from an old episode of The Simpsons to come to an outside-the-box but very practical solution… That’s right - I just fired up emacs, scrolled to the end of the document, and left my water bottle resting on the space bar until enough characters had been added to push the file over the 50kb minimum. It took a while (in truth I went and handled some other stuff while it was running), but after saving the file and using ls -l to check it was now more than 50kb, I ran the_critical_elf_app one last time and… Mission accomplished! I’m sure there were more graceful ways to reach the same outcome, but I’ll put this one down as more proof that a decent understanding of the problem and creative thinking can be just as important in cyber security as any deep technical knowledge in many situations. And if you’re wondering what that hex means, it comes out as Jolly Enough, Overtime Approved. If you found this walkthrough interesting, please consider following me on Twitter, and don’t forget to sign up for the next SANS Holiday Hack Challenge circa December 2022!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mattcasmith.net/wp-content/uploads/2022/01/sans-holiday-challenge-yara_9.png" /><media:content medium="image" url="https://mattcasmith.net/wp-content/uploads/2022/01/sans-holiday-challenge-yara_9.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Backutil development: Implementing multiprocessing in Python</title><link href="https://mattcasmith.net/2021/05/03/multiprocessing-python" rel="alternate" type="text/html" title="Backutil development&amp;#58; Implementing multiprocessing in Python" /><published>2021-05-03T01:00:00+01:00</published><updated>2021-05-03T01:00:00+01:00</updated><id>https://mattcasmith.net/2021/05/03/implementing-multiprocessing-python</id><content type="html" xml:base="https://mattcasmith.net/2021/05/03/multiprocessing-python">&lt;p&gt;I’m still hard at work on &lt;a href=&quot;/2021/01/01/backutil-windows-backup-utility&quot;&gt;Backutil&lt;/a&gt;, my simple Windows backup utility with automatic rotation features, fitting in little tweaks and improvements around my daily schedule. The latest of these - and perhaps the most impactful in terms of performance - involves the implementation of multiprocessing for several parts of the code, which I thought was significant and interesting enough to warrant a write-up.&lt;/p&gt;

&lt;h3 id=&quot;why-implement-multiprocessing&quot;&gt;Why implement multiprocessing?&lt;/h3&gt;

&lt;p&gt;By default, Python scripts execute in a single processor thread. This is the equivalent of a road with a single lane of traffic. This is fine for most parts of the code, but consider some of the operations involved in Backutil - specifically the section that generates hashes for all the files in the backup directories (also the bit that copies files, but we’ll stick to one example in this post to keep things simple).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/04/multiprocessing_before.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There’s a clear bottleneck in this scenario. That one processor core is being worked hard, tasked with generating &lt;em&gt;all&lt;/em&gt; the hashes (yes, there may be other factors at play, like disk read speed, but we’ll ignore that for the purpose of this walkthrough). Even if my computer has six processor cores, we’re only using one. This inefficiency means the script takes longer to run over large batches of files.&lt;/p&gt;

&lt;p&gt;But what if we could use multiple cores at the same time? This would be akin to a road with many lanes of traffic - like &lt;a href=&quot;https://old.reddit.com/r/formula1/comments/c9v83x/baku_turn_1_on_a_non_f1_time/&quot; target=&quot;_blank&quot;&gt;turn one at the Baku Formula 1 street circuit on a normal day&lt;/a&gt; (just more efficient). Let’s try breaking our lists of files into chunks - three, for the sake of argument - and handling them separately.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/04/multiprocessing_after.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That’s better! Now we’ve split the list into parts and assigned each to its own process, we can use more of the computer’s potential and - &lt;em&gt;in theory&lt;/em&gt;, at least - complete the task in a third of the time it would have taken using our previous process. Then we recombine the results for use in the rest of the program.&lt;/p&gt;

&lt;h3 id=&quot;writing-a-function-to-generate-hashes&quot;&gt;Writing a function to generate hashes&lt;/h3&gt;

&lt;p&gt;To spawn separate processes to handle the hash generation, we need to define a function to do the dirty work. Each time we start an additional process, we will invoke this function and supply it with a subset of the data. For the purposes of generating file hashes, that will look like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_hashes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;procnum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backup_files_thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;colorama&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;return_dict_process&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backup_files_thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;sha256_hash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hashlib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sha256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;byte_block&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;65535&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;b&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;sha256_hash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;byte_block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;hash_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sha256_hash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hexdigest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;return_dict_process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hash_output&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Couldn't generate hash for &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Warning&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;return_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;procnum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_dict_process&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;I won’t go into detail on the hash generation itself - that’s for a different post - but I will point out some important elements as far as multiprocessing is concerned.&lt;/p&gt;

&lt;p&gt;First, take a look at the arguments that &lt;code&gt;generate_hashes&lt;/code&gt; takes as input. The variable &lt;code&gt;procnum&lt;/code&gt; will let us assign each subprocess an identifier, &lt;code&gt;backup_files_thread&lt;/code&gt; will be used to supply the data that the subprocess will work on, and &lt;code&gt;version&lt;/code&gt; is just the Backutil version number for logging purposes.&lt;/p&gt;

&lt;p&gt;The special dictionary &lt;code&gt;return_dict&lt;/code&gt; will combine output from our subprocesses - more on that later. All we need to note now is that the final line of our function adds function output (&lt;code&gt;return_dict_process&lt;/code&gt;) as an entry in this dictionary, using the subprocess identifier (&lt;code&gt;procnum&lt;/code&gt;) as a key.&lt;/p&gt;

&lt;h3 id=&quot;splitting-the-dataset-into-chunks&quot;&gt;Splitting the dataset into chunks&lt;/h3&gt;

&lt;p&gt;But we can’t simply tell Python, or the &lt;code&gt;multiprocessing&lt;/code&gt; library, to take our list of files to generate hashes for and split it between &lt;code&gt;x&lt;/code&gt; processes. That would be too easy. We need to do the hard work ourselves and split the data (a list called &lt;code&gt;backup_files&lt;/code&gt;, full of file paths) into equal chunks.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;split_backup_files&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backup_files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_threads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_threads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;When this line of code has done its thing, we’ll be left with a new list called &lt;code&gt;split_backup_files&lt;/code&gt;, within which will be a number of equal lists equal to the &lt;code&gt;config.max_threads&lt;/code&gt; variable, which the user should configure to be equal to the number of processor cores they want Backutil to use.&lt;/p&gt;

&lt;h3 id=&quot;assigning-each-chunk-to-a-subprocess&quot;&gt;Assigning each chunk to a subprocess&lt;/h3&gt;

&lt;p&gt;We’re nearly ready to get our subprocesses up and running, but before we do so there’s some housekeeping to take care of to keep track of processes and their output.&lt;/p&gt;

&lt;p&gt;In the first few lines of setup, &lt;code&gt;process_container&lt;/code&gt; will be used to track all the processes we create, while &lt;code&gt;manager&lt;/code&gt; - and specifically &lt;code&gt;manager.dict()&lt;/code&gt; as &lt;code&gt;return_dict&lt;/code&gt; - will provide a kind of magic dictionary that will take the output from each process (we saw the line in the function for this earlier). The &lt;code&gt;x&lt;/code&gt; value will track process IDs as we create them, iterating by one each time.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;process_container&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;manager&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multiprocessing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Manager&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;return_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;manager&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backup_files_thread&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;split_backup_files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Process-&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;process&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multiprocessing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generate_hashes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backup_files_thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;process_container&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;process&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;process_container&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now for the real action. We iterate over &lt;code&gt;split_backup_files&lt;/code&gt; (you’ll remember the number of lists inside is equal to the number of threads) and for each item we use &lt;code&gt;multiprocessing.Process&lt;/code&gt; to create &lt;code&gt;Process X&lt;/code&gt;. The &lt;code&gt;target&lt;/code&gt; is our function, the &lt;code&gt;name&lt;/code&gt; comes from the variable we just created, and we supply all our function’s arguments in &lt;code&gt;args&lt;/code&gt;. The process is appended to &lt;code&gt;process_container&lt;/code&gt; for safekeeping, and with &lt;code&gt;process.start()&lt;/code&gt; we’re officially multiprocessing!&lt;/p&gt;

&lt;p&gt;One more important action for each process in the container here is &lt;code&gt;process.join()&lt;/code&gt;. This ensures that our parent process waits for each child process to finish before proceeding, ensuring we don’t start trying to use our hashes before they’ve all been generated and returned.&lt;/p&gt;

&lt;h3 id=&quot;checking-subprocess-exit-state&quot;&gt;Checking subprocess exit state&lt;/h3&gt;

&lt;p&gt;Time to build in a bit of resilience. The &lt;code&gt;generate_hashes&lt;/code&gt; function won’t fail if a single hash generation fails, but in the unlikely event that one of our subprocesses fails, we don’t want to proceed and complete a partial backup without the user knowing. Instead, we’ll check they succeed and interrupt Backutil if not.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;process&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;process_container&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exitcode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hash generation thread failed.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Failure&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;exit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To achieve this, we iterate over the processes in &lt;code&gt;process_container&lt;/code&gt; and check the value of &lt;code&gt;process.exitcode&lt;/code&gt; for each. If it’s &lt;code&gt;0&lt;/code&gt; (success), and if it’s not then we log the error before calling &lt;code&gt;sys.exit()&lt;/code&gt; to end the program. Backutil will automatically clean up any temporary files on its way out.&lt;/p&gt;

&lt;h3 id=&quot;recombining-the-dataset&quot;&gt;Recombining the dataset&lt;/h3&gt;

&lt;p&gt;So we split our data, we processed it, and each subprocess has returned its output dictionary as an entry in the dictionary &lt;code&gt;return_dict&lt;/code&gt; with the process number as the key. As the final step, we want to recombine the returned hashes so they can be used elsewhere in the program.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;combined_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;combined_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This is simple enough: We establish the dictionary &lt;code&gt;combined_dict&lt;/code&gt; for our output and begin to iterate over the values in &lt;code&gt;return_dict&lt;/code&gt;. Each key-value pair in each process’s returned dictionary is appended to &lt;code&gt;combined_dict&lt;/code&gt;. After all the entries have been processed, we’ve recombined our data!&lt;/p&gt;

&lt;h3 id=&quot;multiprocessing-in-action&quot;&gt;Multiprocessing in action&lt;/h3&gt;

&lt;p&gt;One of the nice (and convenient) things about multiprocessing is that - unlike most Python structures and implementations - we can see it in action directly in the operating system, rather than using variable explorers or debug &lt;code&gt;print&lt;/code&gt; statements. On Windows, we can simply check the Task Manager.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/04/multiprocessing_taskmanager.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the Details tab, we can see all of our currently running processes. By sorting them by name and finding all instances of &lt;code&gt;backutil.exe&lt;/code&gt;, we can see a number of processes equal to the &lt;code&gt;config.max_threads&lt;/code&gt; variable we saw earlier, plus the parent process - so we know multiprocessing is working properly.&lt;/p&gt;

&lt;h3 id=&quot;multiprocessing-on-windows-with-pyinstaller&quot;&gt;Multiprocessing on Windows with PyInstaller&lt;/h3&gt;

&lt;p&gt;I do have one more small note if you’re building a Python program for Windows using &lt;code&gt;multiprocessing&lt;/code&gt; and you’re planning on converting it to an &lt;code&gt;.exe&lt;/code&gt; file using &lt;a href=&quot;https://www.pyinstaller.org/&quot; target=&quot;_blank&quot;&gt;PyInstaller&lt;/a&gt;. In this case, you could do everything described above perfectly, and when you run your executable you’ll receive an error.&lt;/p&gt;

&lt;p&gt;Luckily, you need only one line of code to fix this.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;multiprocessing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;freeze_support&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You must put &lt;code&gt;multiprocessing.freeze_support()&lt;/code&gt; at the start of this section of your code, but if you do this and recompile your executable with PyInstaller then everything should work again.&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">I’m still hard at work on Backutil, my simple Windows backup utility with automatic rotation features, fitting in little tweaks and improvements around my daily schedule. The latest of these - and perhaps the most impactful in terms of performance - involves the implementation of multiprocessing for several parts of the code, which I thought was significant and interesting enough to warrant a write-up. Why implement multiprocessing? By default, Python scripts execute in a single processor thread. This is the equivalent of a road with a single lane of traffic. This is fine for most parts of the code, but consider some of the operations involved in Backutil - specifically the section that generates hashes for all the files in the backup directories (also the bit that copies files, but we’ll stick to one example in this post to keep things simple). There’s a clear bottleneck in this scenario. That one processor core is being worked hard, tasked with generating all the hashes (yes, there may be other factors at play, like disk read speed, but we’ll ignore that for the purpose of this walkthrough). Even if my computer has six processor cores, we’re only using one. This inefficiency means the script takes longer to run over large batches of files. But what if we could use multiple cores at the same time? This would be akin to a road with many lanes of traffic - like turn one at the Baku Formula 1 street circuit on a normal day (just more efficient). Let’s try breaking our lists of files into chunks - three, for the sake of argument - and handling them separately. That’s better! Now we’ve split the list into parts and assigned each to its own process, we can use more of the computer’s potential and - in theory, at least - complete the task in a third of the time it would have taken using our previous process. Then we recombine the results for use in the rest of the program. Writing a function to generate hashes To spawn separate processes to handle the hash generation, we need to define a function to do the dirty work. Each time we start an additional process, we will invoke this function and supply it with a subset of the data. For the purposes of generating file hashes, that will look like this: def generate_hashes(procnum, backup_files_thread, return_dict, version): colorama.init() return_dict_process = {} for filename in backup_files_thread: try: sha256_hash = hashlib.sha256() with open(filename,&quot;rb&quot;) as f: for byte_block in iter(lambda: f.read(65535),b&quot;&quot;): sha256_hash.update(byte_block) hash_output = (sha256_hash.hexdigest()) return_dict_process[filename] = hash_output except: msg = &quot;Couldn't generate hash for &quot; + filename log(msg, &quot;Warning&quot;, version) return_dict[procnum] = return_dict_process I won’t go into detail on the hash generation itself - that’s for a different post - but I will point out some important elements as far as multiprocessing is concerned. First, take a look at the arguments that generate_hashes takes as input. The variable procnum will let us assign each subprocess an identifier, backup_files_thread will be used to supply the data that the subprocess will work on, and version is just the Backutil version number for logging purposes. The special dictionary return_dict will combine output from our subprocesses - more on that later. All we need to note now is that the final line of our function adds function output (return_dict_process) as an entry in this dictionary, using the subprocess identifier (procnum) as a key. Splitting the dataset into chunks But we can’t simply tell Python, or the multiprocessing library, to take our list of files to generate hashes for and split it between x processes. That would be too easy. We need to do the hard work ourselves and split the data (a list called backup_files, full of file paths) into equal chunks. split_backup_files = (backup_files[i::config.max_threads] for i in range(config.max_threads)) When this line of code has done its thing, we’ll be left with a new list called split_backup_files, within which will be a number of equal lists equal to the config.max_threads variable, which the user should configure to be equal to the number of processor cores they want Backutil to use. Assigning each chunk to a subprocess We’re nearly ready to get our subprocesses up and running, but before we do so there’s some housekeeping to take care of to keep track of processes and their output. In the first few lines of setup, process_container will be used to track all the processes we create, while manager - and specifically manager.dict() as return_dict - will provide a kind of magic dictionary that will take the output from each process (we saw the line in the function for this earlier). The x value will track process IDs as we create them, iterating by one each time. process_container = [] manager = multiprocessing.Manager() return_dict = manager.dict() x = 1 for backup_files_thread in split_backup_files: name = &quot;Process-&quot; + str(x) process = multiprocessing.Process(target=generate_hashes, name=name, args=(x, backup_files_thread, return_dict, version,)) process_container.append(process) process.start() x += 1 for process in process_container: process.join() Now for the real action. We iterate over split_backup_files (you’ll remember the number of lists inside is equal to the number of threads) and for each item we use multiprocessing.Process to create Process X. The target is our function, the name comes from the variable we just created, and we supply all our function’s arguments in args. The process is appended to process_container for safekeeping, and with process.start() we’re officially multiprocessing! One more important action for each process in the container here is process.join(). This ensures that our parent process waits for each child process to finish before proceeding, ensuring we don’t start trying to use our hashes before they’ve all been generated and returned. Checking subprocess exit state Time to build in a bit of resilience. The generate_hashes function won’t fail if a single hash generation fails, but in the unlikely event that one of our subprocesses fails, we don’t want to proceed and complete a partial backup without the user knowing. Instead, we’ll check they succeed and interrupt Backutil if not. for process in process_container: if process.exitcode == 0: continue else: log(&quot;Hash generation thread failed.&quot;, &quot;Failure&quot;, version) sys.exit() To achieve this, we iterate over the processes in process_container and check the value of process.exitcode for each. If it’s 0 (success), and if it’s not then we log the error before calling sys.exit() to end the program. Backutil will automatically clean up any temporary files on its way out. Recombining the dataset So we split our data, we processed it, and each subprocess has returned its output dictionary as an entry in the dictionary return_dict with the process number as the key. As the final step, we want to recombine the returned hashes so they can be used elsewhere in the program. combined_dict = {} for dictionary in return_dict.values(): for key, value in dictionary.items(): combined_dict[key] = value This is simple enough: We establish the dictionary combined_dict for our output and begin to iterate over the values in return_dict. Each key-value pair in each process’s returned dictionary is appended to combined_dict. After all the entries have been processed, we’ve recombined our data! Multiprocessing in action One of the nice (and convenient) things about multiprocessing is that - unlike most Python structures and implementations - we can see it in action directly in the operating system, rather than using variable explorers or debug print statements. On Windows, we can simply check the Task Manager. In the Details tab, we can see all of our currently running processes. By sorting them by name and finding all instances of backutil.exe, we can see a number of processes equal to the config.max_threads variable we saw earlier, plus the parent process - so we know multiprocessing is working properly. Multiprocessing on Windows with PyInstaller I do have one more small note if you’re building a Python program for Windows using multiprocessing and you’re planning on converting it to an .exe file using PyInstaller. In this case, you could do everything described above perfectly, and when you run your executable you’ll receive an error. Luckily, you need only one line of code to fix this. if __name__ == &quot;__main__&quot;: multiprocessing.freeze_support() You must put multiprocessing.freeze_support() at the start of this section of your code, but if you do this and recompile your executable with PyInstaller then everything should work again.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mattcasmith.net/wp-content/uploads/2021/04/multiprocessing_after.png" /><media:content medium="image" url="https://mattcasmith.net/wp-content/uploads/2021/04/multiprocessing_after.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Installing Splunk Free in a virtual machine for log analysis</title><link href="https://mattcasmith.net/2021/05/01/installing-splunk-free-virtual-machine-log-analysis" rel="alternate" type="text/html" title="Installing Splunk Free in a virtual machine for log analysis" /><published>2021-05-01T01:00:00+01:00</published><updated>2021-05-01T01:00:00+01:00</updated><id>https://mattcasmith.net/2021/05/01/running-splunk-virtual-machine</id><content type="html" xml:base="https://mattcasmith.net/2021/05/01/installing-splunk-free-virtual-machine-log-analysis">&lt;p&gt;Splunk is considered the gold standard for analysis of event logs and other data, but unless you’re lucky enough to work for an organisation that pays for it, it can be difficult to get practical experience in how to run searches, build dashboards, and otherwise dissect data using its query language.&lt;/p&gt;

&lt;p&gt;Recently, when I wanted to take a look at some &lt;a href=&quot;/2021/01/01/backutil-windows-backup-utility&quot;&gt;Backutil&lt;/a&gt; logs to see how recent development had affected performance, I stumbled across &lt;a href=&quot;https://docs.splunk.com/Documentation/Splunk/8.1.3/Admin/MoreaboutSplunkFree&quot; target=&quot;_blank&quot;&gt;Splunk Free&lt;/a&gt; - a limited version of the logging platform - and discovered just how easy it is to install (and how surprisingly well it runs) in a virtual machine. Here’s how to do it…&lt;/p&gt;

&lt;h3 id=&quot;setting-up-a-virtual-machine&quot;&gt;Setting up a virtual machine&lt;/h3&gt;

&lt;p&gt;For the purposes of this post, I’ll assume you’re familiar with the basics of setting up a virtual machine in VirtualBox or VMware. I won’t go over the process of creating the machine, but I will point out some key steps to take to make sure Splunk works and is accessible from your host system.&lt;/p&gt;

&lt;p&gt;First, choose your Linux distro. I went for &lt;a href=&quot;https://www.centos.org/&quot; target=&quot;_blank&quot;&gt;CentOS&lt;/a&gt;, which is usually a safe bet to run a stable server with less of the fluff that comes with some other versions, with 8GB of RAM. Like most modern operating systems, installation is as easy as booting from the ISO and following the on-screen instructions.&lt;/p&gt;

&lt;p&gt;One extra (but critically important) step: You’ll want to check the virtual machine’s network connection is set to Bridged Adapter. When you boot it up, configure an IP address on the same subnet as your host machine. With these settings, the guest system will behave as though it is a completely separate machine on the network as your computer. This way, once we’ve made some firewall changes, we’ll be able to access Splunk in our browser at the virtual machine’s IP address.&lt;/p&gt;

&lt;h3 id=&quot;downloading-and-installing-splunk&quot;&gt;Downloading and installing Splunk&lt;/h3&gt;

&lt;p&gt;With our virtual machine set up, it’s time to install Splunk. Download Splunk Free for Linux (you can find it by clicking Free Splunk in the top right corner of &lt;a href=&quot;https://www.splunk.com/&quot; target=&quot;_blank&quot;&gt;the Splunk homepage&lt;/a&gt; and selecting Software Download). Either do this in your virtual machine, or do it on your host system and copy the file to your Linux machine.&lt;/p&gt;

&lt;p&gt;To install Splunk, we need to extract &lt;code&gt;splunk-x.x.x-x-Linux-x86_64.tgz&lt;/code&gt; to the &lt;code&gt;/opt&lt;/code&gt; directory. The easiest way to do this is to copy it to the correct directory and run the following command.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tar&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xvf&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;splunk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;8.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;63079&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c59e632&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linux&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x86_64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tgz&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You should now have a &lt;code&gt;/opt/splunk&lt;/code&gt; folder containing the archive’s contents, which are essentially a fully-fledged Splunk installation. There’s just one more command to run to get Splunk up and running.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;bin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splunk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accept&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;license&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The first time you run this file, it will launch the Splunk installer, which will give you an opportunity to set an administrator username and password before your instance is live. At this point, you should be able to access Splunk at &lt;code&gt;127.0.0.1:8000&lt;/code&gt; in your virtual machine’s web browser.&lt;/p&gt;

&lt;h3 id=&quot;configuring-the-linux-firewall&quot;&gt;Configuring the Linux firewall&lt;/h3&gt;

&lt;p&gt;But wouldn’t it be much simpler if we could access Splunk from our host system? To do that, we need to allow connections to our virtual machine on port 8000. The exact command you’ll need to run will depend on which Linux distro you chose for your virtual machine, but in CentOS it looks like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;firewall&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;permanent&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p style=&quot;float: left; width: 90%; padding: 5px; padding-left: 15px; padding-right: 5%; margin: 30px 0 10px 0; font-style: italic; border-left: 5px solid red; background: #ff9999; color: #333;&quot;&gt;Note that this firewall rule will open your virtual machine to all connections on TCP port 8000. If you're working with more sensitive data or in a less secure/private environment, you'll probably want to be more specific - for example, by specifying a source IP address.&lt;/p&gt;

&lt;p&gt;Now that this firewall rule has been added as a permanent rule, we need to use the &lt;code&gt;reload&lt;/code&gt; command to refresh the live instance of the firewall with the latest ruleset.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;firewall&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reload&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;That should do the trick, but if you want to make sure that you’ve done everything correctly, use the &lt;code&gt;list-all&lt;/code&gt; option to check that your new rule is present and correct.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;firewall&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Your output should look something like this - the important bit is that &lt;code&gt;8000/tcp&lt;/code&gt; is under &lt;code&gt;ports&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/05/splunk_vm_firewall.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If your firewall rule has been added successfully, you’re ready to start using Splunk!&lt;/p&gt;

&lt;h3 id=&quot;accessing-your-splunk-instance&quot;&gt;Accessing your Splunk instance&lt;/h3&gt;

&lt;p&gt;Open a web browser on your host machine and type in the IP address of your virtual machine followed by &lt;code&gt;:8000&lt;/code&gt;. With any luck, you’ll presented with a Splunk login page. Sign in with the administrator credentials you set a little earlier and there you have it - your own Splunk instance to tinker and experiment with.&lt;/p&gt;

&lt;p&gt;At this point, it’s probably a good idea to take a virtual machine snapshot before you start importing data or changing settings. That way, if you make any mistakes, or want to start fresh for any other reason, you can simply revert to the snapshot without doing all the setup again.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/05/splunk_vm_data.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And we’re done! Obviously Splunk won’t run quite as well on a virtual machine with 8GB of RAM as it does in the cloud or on a dedicated server cluster (and it will show warning messages to remind you of this), but I’ve found it surprisingly stable when analysing small datasets and learning its features.&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">Splunk is considered the gold standard for analysis of event logs and other data, but unless you’re lucky enough to work for an organisation that pays for it, it can be difficult to get practical experience in how to run searches, build dashboards, and otherwise dissect data using its query language. Recently, when I wanted to take a look at some Backutil logs to see how recent development had affected performance, I stumbled across Splunk Free - a limited version of the logging platform - and discovered just how easy it is to install (and how surprisingly well it runs) in a virtual machine. Here’s how to do it… Setting up a virtual machine For the purposes of this post, I’ll assume you’re familiar with the basics of setting up a virtual machine in VirtualBox or VMware. I won’t go over the process of creating the machine, but I will point out some key steps to take to make sure Splunk works and is accessible from your host system. First, choose your Linux distro. I went for CentOS, which is usually a safe bet to run a stable server with less of the fluff that comes with some other versions, with 8GB of RAM. Like most modern operating systems, installation is as easy as booting from the ISO and following the on-screen instructions. One extra (but critically important) step: You’ll want to check the virtual machine’s network connection is set to Bridged Adapter. When you boot it up, configure an IP address on the same subnet as your host machine. With these settings, the guest system will behave as though it is a completely separate machine on the network as your computer. This way, once we’ve made some firewall changes, we’ll be able to access Splunk in our browser at the virtual machine’s IP address. Downloading and installing Splunk With our virtual machine set up, it’s time to install Splunk. Download Splunk Free for Linux (you can find it by clicking Free Splunk in the top right corner of the Splunk homepage and selecting Software Download). Either do this in your virtual machine, or do it on your host system and copy the file to your Linux machine. To install Splunk, we need to extract splunk-x.x.x-x-Linux-x86_64.tgz to the /opt directory. The easiest way to do this is to copy it to the correct directory and run the following command. sudo tar xvf splunk-8.1.3-63079c59e632-Linux-x86_64.tgz You should now have a /opt/splunk folder containing the archive’s contents, which are essentially a fully-fledged Splunk installation. There’s just one more command to run to get Splunk up and running. sudo /opt/bin/splunk/start --accept-license The first time you run this file, it will launch the Splunk installer, which will give you an opportunity to set an administrator username and password before your instance is live. At this point, you should be able to access Splunk at 127.0.0.1:8000 in your virtual machine’s web browser. Configuring the Linux firewall But wouldn’t it be much simpler if we could access Splunk from our host system? To do that, we need to allow connections to our virtual machine on port 8000. The exact command you’ll need to run will depend on which Linux distro you chose for your virtual machine, but in CentOS it looks like this: sudo firewall-cmd --zone=public --add-port=8000/tcp --permanent Note that this firewall rule will open your virtual machine to all connections on TCP port 8000. If you're working with more sensitive data or in a less secure/private environment, you'll probably want to be more specific - for example, by specifying a source IP address. Now that this firewall rule has been added as a permanent rule, we need to use the reload command to refresh the live instance of the firewall with the latest ruleset. sudo firewall-cmd --reload That should do the trick, but if you want to make sure that you’ve done everything correctly, use the list-all option to check that your new rule is present and correct. sudo firewall-cmd --list-all Your output should look something like this - the important bit is that 8000/tcp is under ports. If your firewall rule has been added successfully, you’re ready to start using Splunk! Accessing your Splunk instance Open a web browser on your host machine and type in the IP address of your virtual machine followed by :8000. With any luck, you’ll presented with a Splunk login page. Sign in with the administrator credentials you set a little earlier and there you have it - your own Splunk instance to tinker and experiment with. At this point, it’s probably a good idea to take a virtual machine snapshot before you start importing data or changing settings. That way, if you make any mistakes, or want to start fresh for any other reason, you can simply revert to the snapshot without doing all the setup again. And we’re done! Obviously Splunk won’t run quite as well on a virtual machine with 8GB of RAM as it does in the cloud or on a dedicated server cluster (and it will show warning messages to remind you of this), but I’ve found it surprisingly stable when analysing small datasets and learning its features.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mattcasmith.net/wp-content/uploads/2021/05/splunk_vm_data.png" /><media:content medium="image" url="https://mattcasmith.net/wp-content/uploads/2021/05/splunk_vm_data.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">File carving: Recovering a deleted file from a Windows disk image</title><link href="https://mattcasmith.net/2021/04/02/file-carving-recovering-deleted-file-disk-image" rel="alternate" type="text/html" title="File carving&amp;#58; Recovering a deleted file from a Windows disk image" /><published>2021-04-02T01:00:00+01:00</published><updated>2021-04-02T01:00:00+01:00</updated><id>https://mattcasmith.net/2021/04/02/file-carving-recovering-deleted-file-disk-image</id><content type="html" xml:base="https://mattcasmith.net/2021/04/02/file-carving-recovering-deleted-file-disk-image">&lt;p&gt;Most computer users assume that when they delete a file and empty the Recycle Bin, it’s gone forever. After all, if Windows doesn’t show us a file, it doesn’t exist anymore, right? Wrong. With the right tools and knowledge, forensics experts can find fragments - or even complete versions - of deleted files that remain on the hard disk long after they disappear from Windows Explorer.&lt;/p&gt;

&lt;p&gt;In this post, I’ll run through how to take a disk image and recover deleted files using FTK Imager and a hex editor, explain why this works, and show how certain utilities let you overwrite this remnant data.&lt;/p&gt;

&lt;h3 id=&quot;what-happens-when-you-delete-a-file&quot;&gt;What happens when you delete a file?&lt;/h3&gt;

&lt;p&gt;To give myself something to find, I created the text file shown below on a 2GB hard drive partition. I saved it, then closed it, deleted it, and emptied the Recycle Bin to render it inaccessible to any normal Windows user. But what is actually happening in the background when you do this?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/04/deletedfile_file.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At the top of your hard drive is what’s known as the master file table (MFT). This is essentially an index - it contains filenames along with the addresses on the hard drive that these files can be found at. So when I create &lt;code&gt;test_file.txt&lt;/code&gt;, an MFT record is created containing that filename and the address where the data (i.e. the file contents) is stored - in this case that’s &lt;code&gt;40009D50h&lt;/code&gt;, but we don’t know that yet.&lt;/p&gt;

&lt;p&gt;When you delete a file, none of this information is removed from the disk. All deletion does is add a “deleted” flag to the MFT record, telling the computer that the information at the specified address is no longer needed and can be overwritten with new data if necessary - but nothing is actually deleted at this point and the file data is still present on the disk. You can probably guess where this is going…&lt;/p&gt;

&lt;h3 id=&quot;creating-a-disk-image-with-ftk-imager&quot;&gt;Creating a disk image with FTK Imager&lt;/h3&gt;

&lt;p&gt;With the right tools, we can access the remnant data. First, we need a physical disk image to work with. &lt;a href=&quot;https://accessdata.com/product-download&quot; target=&quot;_blank&quot;&gt;FTK Imager&lt;/a&gt; is a free tool that allows us to create one. We choose a few simple options (I’m generating an image in the &lt;code&gt;E01&lt;/code&gt; format) and set it to work. Depending on the size of the disk and your computer’s speed, it will take a little while to generate the image, which is an exact replica of the hard drive.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/04/deletedfile_ftk.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One thing to note is that the image cannot be generated on the same physical hard drive that is being imaged. For this reason, unless the system has multiple hard drives, it is necessary to choose to save the image to a directory on a removable drive or a network share, rather than on the subject computer itself.&lt;/p&gt;

&lt;h3 id=&quot;recovering-a-deleted-file-with-ftk-imager&quot;&gt;Recovering a deleted file with FTK Imager&lt;/h3&gt;

&lt;p&gt;FTK Imager can also be useful for the next step in our process - it can actually do the hard work for us and recover the deleted file. If we mount the physical image we created and open the correct partition, we can navigate through the file system to find deleted files, which will be marked with a red cross.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/04/deletedfile_ftkrecover.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As the last place the file was present was the Recycle Bin (read my post about &lt;a href=&quot;/2018/12/15/python-windows-forensics-recycle-bin-deleted-files/&quot;&gt;Python forensics for the Recycle Bin&lt;/a&gt; for more information about the file structure), its MFT entry points to that directory. Here, we can see that the file &lt;code&gt;$RHEYFYA.txt&lt;/code&gt; is our test file, and FTK Imager helpfully displays the contents.&lt;/p&gt;

&lt;h3 id=&quot;recovering-a-deleted-file-with-a-hex-editor&quot;&gt;Recovering a deleted file with a hex editor&lt;/h3&gt;

&lt;p&gt;But things won’t always be that easy, and we shouldn’t rely on tools to do our forensics for us without knowing how they work. We can find the same data manually by mounting the disk image and opening it in a hex editor - in this example, &lt;a href=&quot;https://www.sweetscape.com/010editor/&quot; target=&quot;_blank&quot;&gt;010 Editor&lt;/a&gt;. This displays the entire contents of the hard drive in hex format, and has a helpful feature that will search for strings amongst the data in the image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/04/deletedfile_hex.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, I was able to find the text file’s contents by searching for strings. If we were looking for a different type of file, we might search according to its file signature, which indicates what format the file is in (for example, JPEGs start with &lt;code&gt;FF D8 FF E0&lt;/code&gt;). Text files don’t normally have signatures, and in this case we can see the file data starts immediately at the address &lt;code&gt;40009D50h&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;overwriting-traces-of-deleted-files&quot;&gt;Overwriting traces of deleted files&lt;/h3&gt;

&lt;p&gt;So we’ve seen that it’s possible to recover deleted files, but is there any way to remove them  from the disk completely? In short: yes, there is. Tools like &lt;a href=&quot;https://www.ccleaner.com/&quot; target=&quot;_blank&quot;&gt;CCleaner&lt;/a&gt; can overwrite the free space on a hard drive with nonsense data. If I run CCleaner’s Drive Wiper feature on our partition and repeat the above process, we can see that the address formerly occupied by &lt;code&gt;test_file.txt&lt;/code&gt; is now full of zeroes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/04/deletedfile_ccleaner.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While this prevents a forensic investigator from recovering a file’s contents if done correctly, using some of these tools will arouse suspicion in itself. Some wipers will fill free space with ones, Zs, or random data, and this does not normally occur naturally. If this is observed on a hard drive then questions will be asked over whether the user was trying to hide something and a more thorough search may be conducted.&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">Most computer users assume that when they delete a file and empty the Recycle Bin, it’s gone forever. After all, if Windows doesn’t show us a file, it doesn’t exist anymore, right? Wrong. With the right tools and knowledge, forensics experts can find fragments - or even complete versions - of deleted files that remain on the hard disk long after they disappear from Windows Explorer. In this post, I’ll run through how to take a disk image and recover deleted files using FTK Imager and a hex editor, explain why this works, and show how certain utilities let you overwrite this remnant data. What happens when you delete a file? To give myself something to find, I created the text file shown below on a 2GB hard drive partition. I saved it, then closed it, deleted it, and emptied the Recycle Bin to render it inaccessible to any normal Windows user. But what is actually happening in the background when you do this? At the top of your hard drive is what’s known as the master file table (MFT). This is essentially an index - it contains filenames along with the addresses on the hard drive that these files can be found at. So when I create test_file.txt, an MFT record is created containing that filename and the address where the data (i.e. the file contents) is stored - in this case that’s 40009D50h, but we don’t know that yet. When you delete a file, none of this information is removed from the disk. All deletion does is add a “deleted” flag to the MFT record, telling the computer that the information at the specified address is no longer needed and can be overwritten with new data if necessary - but nothing is actually deleted at this point and the file data is still present on the disk. You can probably guess where this is going… Creating a disk image with FTK Imager With the right tools, we can access the remnant data. First, we need a physical disk image to work with. FTK Imager is a free tool that allows us to create one. We choose a few simple options (I’m generating an image in the E01 format) and set it to work. Depending on the size of the disk and your computer’s speed, it will take a little while to generate the image, which is an exact replica of the hard drive. One thing to note is that the image cannot be generated on the same physical hard drive that is being imaged. For this reason, unless the system has multiple hard drives, it is necessary to choose to save the image to a directory on a removable drive or a network share, rather than on the subject computer itself. Recovering a deleted file with FTK Imager FTK Imager can also be useful for the next step in our process - it can actually do the hard work for us and recover the deleted file. If we mount the physical image we created and open the correct partition, we can navigate through the file system to find deleted files, which will be marked with a red cross. As the last place the file was present was the Recycle Bin (read my post about Python forensics for the Recycle Bin for more information about the file structure), its MFT entry points to that directory. Here, we can see that the file $RHEYFYA.txt is our test file, and FTK Imager helpfully displays the contents. Recovering a deleted file with a hex editor But things won’t always be that easy, and we shouldn’t rely on tools to do our forensics for us without knowing how they work. We can find the same data manually by mounting the disk image and opening it in a hex editor - in this example, 010 Editor. This displays the entire contents of the hard drive in hex format, and has a helpful feature that will search for strings amongst the data in the image. As you can see, I was able to find the text file’s contents by searching for strings. If we were looking for a different type of file, we might search according to its file signature, which indicates what format the file is in (for example, JPEGs start with FF D8 FF E0). Text files don’t normally have signatures, and in this case we can see the file data starts immediately at the address 40009D50h. Overwriting traces of deleted files So we’ve seen that it’s possible to recover deleted files, but is there any way to remove them from the disk completely? In short: yes, there is. Tools like CCleaner can overwrite the free space on a hard drive with nonsense data. If I run CCleaner’s Drive Wiper feature on our partition and repeat the above process, we can see that the address formerly occupied by test_file.txt is now full of zeroes. While this prevents a forensic investigator from recovering a file’s contents if done correctly, using some of these tools will arouse suspicion in itself. Some wipers will fill free space with ones, Zs, or random data, and this does not normally occur naturally. If this is observed on a hard drive then questions will be asked over whether the user was trying to hide something and a more thorough search may be conducted.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mattcasmith.net/wp-content/uploads/2021/04/deletedfile_hex.png" /><media:content medium="image" url="https://mattcasmith.net/wp-content/uploads/2021/04/deletedfile_hex.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Backutil development: Building and JOINing SQLite databases in Python</title><link href="https://mattcasmith.net/2021/03/26/sqlite-databases-python" rel="alternate" type="text/html" title="Backutil development&amp;#58; Building and JOINing SQLite databases in Python" /><published>2021-03-26T00:00:00+00:00</published><updated>2021-03-26T00:00:00+00:00</updated><id>https://mattcasmith.net/2021/03/26/using-sqlite-python-protecting-sql-injection</id><content type="html" xml:base="https://mattcasmith.net/2021/03/26/sqlite-databases-python">&lt;p&gt;This post is something of a development diary for Backutil - &lt;a href=&quot;http://127.0.0.1:4000/2021/01/01/backutil-windows-backup-utility&quot;&gt;my Python-based utility for backing up files from Windows systems&lt;/a&gt;. I published the first version of Backutil (v0.51) at the beginning of 2021, and pushed a small update (v0.52) to fix some minor issues in February.&lt;/p&gt;

&lt;p&gt;As of v0.52, the utility still relied on text files full of file hashes to remember what it had backed up, and in-memory Pandas dataframes to compare the files currently on the system to the previous file hashes and work out what to back up. The full process looked something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/03/sqlite_before.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For v0.61, I wanted to streamline this somewhat, and hopefully speed up the process (this will be boosted by a multithreading implementation in a future update). The solution? SQLite - as the name would suggest, a lighter version of SQL that can be used to store databases either in memory or as files via the Python &lt;code&gt;sqlite3&lt;/code&gt; library. This enabled me to use &lt;a href=&quot;/2018/10/12/basic-sql-queries-select-from-where-operators/&quot;&gt;SQL functions like &lt;code&gt;INSERT&lt;/code&gt;, &lt;code&gt;DELETE&lt;/code&gt; and &lt;code&gt;JOIN&lt;/code&gt;&lt;/a&gt; to more efficiently manipulate the backup data. The new process looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/03/sqlite_after.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To demonstrate this, I’ll run through a few examples of how SQLite is used in the latest version.&lt;/p&gt;

&lt;h3 id=&quot;managing-the-database-connection&quot;&gt;Managing the database connection&lt;/h3&gt;

&lt;p&gt;Before we can use a SQLite database, we must connect to it. As this would happen several times through the code, I defined a function that takes the &lt;code&gt;config&lt;/code&gt; class and &lt;code&gt;action&lt;/code&gt; variable. If the &lt;code&gt;action&lt;/code&gt; is &lt;code&gt;open&lt;/code&gt;, we connect to the database and create the &lt;code&gt;backutil_previous&lt;/code&gt; table if it does not already exist.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;open&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db_conn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqlite3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db_cursor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db_conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db_cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;CREATE TABLE IF NOT EXISTS backutil_previous(date TEXT, hash TEXT);&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If the &lt;code&gt;action&lt;/code&gt; supplied to the function is &lt;code&gt;close&lt;/code&gt;, we close the connection to the database.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;close&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db_conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;With this function defined and actions taken according to the contents of the &lt;code&gt;action&lt;/code&gt; variable, we can open and close the database using a single line elsewhere in the code.&lt;/p&gt;

&lt;h3 id=&quot;getting-previous-backup-dates&quot;&gt;Getting previous backup dates&lt;/h3&gt;

&lt;p&gt;In order to work out whether the backups need to be rotated, Backutil previously counted the number of &lt;code&gt;.back&lt;/code&gt; text files in its directory. Now, however, we can use &lt;code&gt;SELECT DISTINCT&lt;/code&gt; to get a list of unique timestamps of previous backups from the SQLite database stored on disk.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;n&quot;&gt;backup_dates&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;db_dates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;previous_db_cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;SELECT DISTINCT date FROM backutil_previous ORDER BY date ASC;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;date&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db_dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;backup_dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This code iterates through the returned dates and adds them to a Python list named &lt;code&gt;backup_dates&lt;/code&gt;, the length of which is subsequently counted to determine whether there are more backups than required.&lt;/p&gt;

&lt;h3 id=&quot;deleting-data-from-old-backups&quot;&gt;Deleting data from old backups&lt;/h3&gt;

&lt;p&gt;If there are too many backups, the oldest backup must be deleted, along with its entries in the previous backups database. We can take the oldest date from &lt;code&gt;backup_dates&lt;/code&gt; and use the &lt;code&gt;DELETE&lt;/code&gt; operation. Using question marks in &lt;code&gt;sqlite3&lt;/code&gt; inserts items from the supplied variable - in this case &lt;code&gt;query_data&lt;/code&gt; - and is best practice to protect against SQL injection by interpreting strings only as literal values.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backup_dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backups_retained&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backups_rotated&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;True&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;query_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backup_dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;previous_db_cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;DELETE FROM backutil_previous WHERE date = ?;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;previous_db_conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;commit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Another important note here is that after an operation is executed that would change a SQLite database, it must be committed. If we did not commit our changes to the database, it would not be written to the file on disk. Once the changes are committed, we have deleted the oldest backup hash entries for good.&lt;/p&gt;

&lt;h3 id=&quot;checking-old-backup-hashes-against-current-files&quot;&gt;Checking old backup hashes against current files&lt;/h3&gt;

&lt;p&gt;Meanwhile, a separate database named &lt;code&gt;backutil_tracker&lt;/code&gt; has been built in memory, containing the hashes of all the files that currently reside in the backup directories. To work out which files are new and need to be backed up, we must compare them to the hashes in &lt;code&gt;backutil_previous&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We can use &lt;code&gt;ATTACH&lt;/code&gt; to enable cross-database operations, then perform a &lt;code&gt;LEFT JOIN&lt;/code&gt; (see &lt;a href=&quot;/2018/12/21/sql-joins-inner-left-right-outer/&quot;&gt;my previous post on SQL &lt;code&gt;JOIN&lt;/code&gt;s&lt;/a&gt; for all the details) to combine the data from the two tables based on the file hashes.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;n&quot;&gt;query_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;previous_db_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tracker_db_cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ATTACH ? as backutil_previous&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tracker_db_cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;SELECT backutil_tracker.file, backutil_tracker.hash, backutil_previous.date FROM backutil_tracker LEFT JOIN backutil_previous ON backutil_tracker.hash=backutil_previous.hash;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;files_to_back_up&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If a hash in &lt;code&gt;backutil_tracker&lt;/code&gt; matches a hash in &lt;code&gt;backutil_previous&lt;/code&gt;, the date that file was backed up is copied to the table. Once all hashes have been checked, we know that rows with &lt;code&gt;None&lt;/code&gt; in the date column do not exist in previous backups. These are copied to a Python list that will be supplied to the backup loop. This way, only new files are added to the new backup.&lt;/p&gt;

&lt;h3 id=&quot;writing-backed-up-files-to-the-database&quot;&gt;Writing backed up files to the database&lt;/h3&gt;

&lt;p&gt;As Backutil successfully copies files, it appends their rows to the &lt;code&gt;files_backed_up&lt;/code&gt; list. Once all files have been safely copied, compressed, and encrypted, we can &lt;code&gt;INSERT&lt;/code&gt; the contents of this list into the &lt;code&gt;backutil_previous&lt;/code&gt; table, which is stored on disk for future reference.&lt;/p&gt;

&lt;p&gt;We open the database connection again, execute the &lt;code&gt;INSERT&lt;/code&gt; operation, then commit the changes. The new hashes will be written to the database with the timestamp of the new backup.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;        &lt;span class=&quot;n&quot;&gt;manage_previous_db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;open&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backed_up_file&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;files_backed_up&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;query_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backup_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backed_up_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;previous_db_cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;INSERT INTO backutil_previous (date, hash) VALUES (?, ?);&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;previous_db_conn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;commit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;manage_previous_db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;close&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;With our files backed up and the new hashes written to &lt;code&gt;backutil_previous&lt;/code&gt;, we can close our database connection. The database will remain on disk until it is called upon during the next backup, at which time Backutil will not copy the same files again because their hashes are now in the database.&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">This post is something of a development diary for Backutil - my Python-based utility for backing up files from Windows systems. I published the first version of Backutil (v0.51) at the beginning of 2021, and pushed a small update (v0.52) to fix some minor issues in February. As of v0.52, the utility still relied on text files full of file hashes to remember what it had backed up, and in-memory Pandas dataframes to compare the files currently on the system to the previous file hashes and work out what to back up. The full process looked something like this: For v0.61, I wanted to streamline this somewhat, and hopefully speed up the process (this will be boosted by a multithreading implementation in a future update). The solution? SQLite - as the name would suggest, a lighter version of SQL that can be used to store databases either in memory or as files via the Python sqlite3 library. This enabled me to use SQL functions like INSERT, DELETE and JOIN to more efficiently manipulate the backup data. The new process looks like this: To demonstrate this, I’ll run through a few examples of how SQLite is used in the latest version. Managing the database connection Before we can use a SQLite database, we must connect to it. As this would happen several times through the code, I defined a function that takes the config class and action variable. If the action is open, we connect to the database and create the backutil_previous table if it does not already exist. if action == &quot;open&quot;: config.db_conn = sqlite3.connect(config.db_name) config.db_cursor = config.db_conn.cursor() config.db_cursor.execute(&quot;CREATE TABLE IF NOT EXISTS backutil_previous(date TEXT, hash TEXT);&quot;) If the action supplied to the function is close, we close the connection to the database. if action == &quot;close&quot;: config.db_conn.close() With this function defined and actions taken according to the contents of the action variable, we can open and close the database using a single line elsewhere in the code. Getting previous backup dates In order to work out whether the backups need to be rotated, Backutil previously counted the number of .back text files in its directory. Now, however, we can use SELECT DISTINCT to get a list of unique timestamps of previous backups from the SQLite database stored on disk. backup_dates=[] db_dates = config.previous_db_cursor.execute(&quot;SELECT DISTINCT date FROM backutil_previous ORDER BY date ASC;&quot;) for date in db_dates: backup_dates.append(date[0]) This code iterates through the returned dates and adds them to a Python list named backup_dates, the length of which is subsequently counted to determine whether there are more backups than required. Deleting data from old backups If there are too many backups, the oldest backup must be deleted, along with its entries in the previous backups database. We can take the oldest date from backup_dates and use the DELETE operation. Using question marks in sqlite3 inserts items from the supplied variable - in this case query_data - and is best practice to protect against SQL injection by interpreting strings only as literal values. if len(backup_dates) &amp;gt; (config.backups_retained - 1) and (config.backups_rotated == &quot;True&quot;): query_data = (str(backup_dates[0]),) config.previous_db_cursor.execute(&quot;DELETE FROM backutil_previous WHERE date = ?;&quot;, query_data) config.previous_db_conn.commit() Another important note here is that after an operation is executed that would change a SQLite database, it must be committed. If we did not commit our changes to the database, it would not be written to the file on disk. Once the changes are committed, we have deleted the oldest backup hash entries for good. Checking old backup hashes against current files Meanwhile, a separate database named backutil_tracker has been built in memory, containing the hashes of all the files that currently reside in the backup directories. To work out which files are new and need to be backed up, we must compare them to the hashes in backutil_previous. We can use ATTACH to enable cross-database operations, then perform a LEFT JOIN (see my previous post on SQL JOINs for all the details) to combine the data from the two tables based on the file hashes. query_data = (config.previous_db_name,) config.tracker_db_cursor.execute(&quot;ATTACH ? as backutil_previous&quot;, query_data) results = config.tracker_db_cursor.execute(&quot;SELECT backutil_tracker.file, backutil_tracker.hash, backutil_previous.date FROM backutil_tracker LEFT JOIN backutil_previous ON backutil_tracker.hash=backutil_previous.hash;&quot;) for line in results: if line[2] == None: config.files_to_back_up.append(line) If a hash in backutil_tracker matches a hash in backutil_previous, the date that file was backed up is copied to the table. Once all hashes have been checked, we know that rows with None in the date column do not exist in previous backups. These are copied to a Python list that will be supplied to the backup loop. This way, only new files are added to the new backup. Writing backed up files to the database As Backutil successfully copies files, it appends their rows to the files_backed_up list. Once all files have been safely copied, compressed, and encrypted, we can INSERT the contents of this list into the backutil_previous table, which is stored on disk for future reference. We open the database connection again, execute the INSERT operation, then commit the changes. The new hashes will be written to the database with the timestamp of the new backup. manage_previous_db(config, &quot;open&quot;) for backed_up_file in config.files_backed_up: query_data = (config.backup_time, backed_up_file) config.previous_db_cursor.execute(&quot;INSERT INTO backutil_previous (date, hash) VALUES (?, ?);&quot;, query_data) config.previous_db_conn.commit() manage_previous_db(config, &quot;close&quot;) With our files backed up and the new hashes written to backutil_previous, we can close our database connection. The database will remain on disk until it is called upon during the next backup, at which time Backutil will not copy the same files again because their hashes are now in the database.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mattcasmith.net/wp-content/uploads/2021/03/sqlite_after.png" /><media:content medium="image" url="https://mattcasmith.net/wp-content/uploads/2021/03/sqlite_after.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introducing Backutil: A Python‐based Windows backup utility</title><link href="https://mattcasmith.net/2021/01/01/backutil-windows-backup-utility" rel="alternate" type="text/html" title="Introducing Backutil&amp;#58; A Python&amp;#8208;based Windows backup utility" /><published>2021-01-01T00:00:00+00:00</published><updated>2021-01-01T00:00:00+00:00</updated><id>https://mattcasmith.net/2021/01/01/backutil-python-windows-backup-utility</id><content type="html" xml:base="https://mattcasmith.net/2021/01/01/backutil-windows-backup-utility">&lt;p&gt;Back in the spring, I decided that 2020 would be the year I would finally see a coding project through to completion. A recent work project shone a light on backup and recovery, and I realised that I should probably be a bit more consistent with my own backups from my personal PC. Wanting to avoid paying &lt;em&gt;another&lt;/em&gt; annual subscription, I decided to write a script myself. Thus Backutil was born - and the project only grew from there as I added more features along the way.&lt;/p&gt;

&lt;p&gt;I’m still not quite at the point when I’m ready to release a v1.0, but I told myself a few months ago that I wanted to put together a minimum viable product by the new year - so here it is! It has a few bugs and is missing a couple of features, but Backutil is now a functioning Python-based utility for backing up files on Windows systems, complete with options for incremental backups and backup rotation.&lt;/p&gt;

&lt;h3 id=&quot;contents&quot;&gt;Contents&lt;/h3&gt;

&lt;p&gt;1. &lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;br /&gt;
          a. &lt;a href=&quot;#testing-and-limitations&quot;&gt;Testing and limitations&lt;/a&gt;&lt;br /&gt;
2. &lt;a href=&quot;#configuration&quot;&gt;Configuration&lt;/a&gt;&lt;br /&gt;
          a. &lt;a href=&quot;#configuration-file&quot;&gt;Configuration file&lt;/a&gt;&lt;br /&gt;
          b. &lt;a href=&quot;#backup-list-file&quot;&gt;Backup list file&lt;/a&gt;&lt;br /&gt;
          a. &lt;a href=&quot;#command-line-options&quot;&gt;Command line options&lt;/a&gt;&lt;br /&gt;
3. &lt;a href=&quot;#download&quot;&gt;Download&lt;/a&gt;&lt;br /&gt;
4. &lt;a href=&quot;#changelog&quot;&gt;Changelog&lt;/a&gt;&lt;br /&gt;
5. &lt;a href=&quot;#future-development&quot;&gt;Future development&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Backutil is a simple, Python-based utility for backing up files from Windows systems to compressed, password-protected local archives. It has features for performing incremental backups and automatically rotating backup files. This is achieved using &lt;code&gt;robocopy&lt;/code&gt; and 7-Zip, which must be installed.&lt;/p&gt;

&lt;p style=&quot;float: left; width: 90%; padding: 5px; padding-left: 15px; padding-right: 5%; margin: 30px 0 10px 0; font-style: italic; border-left: 5px solid red; background: #ff9999; color: #333;&quot;&gt;Backutil is a learning/hobby project and some aspects of its code may not follow best practices. While you're welcome to use it, you do so at your own risk. Make sure you take a manual backup of your files before trying it out, and don't go relying on it to back up your production servers.&lt;/p&gt;

&lt;p&gt;To back up your files, simply ensure you have configured Backutil (see the sections below) and run &lt;code&gt;backutil.exe&lt;/code&gt; from the Command Prompt or PowerShell. The utility will report on its progress until the backup is successfully completed. More detail can also be found in &lt;code&gt;backutil_log.csv&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2021/05/backutil_v0_70.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When the utility is finished, you should find your complete backup files in your designated backup folder. The number and size of these backup files can be configured using the incremental backup and rotation settings, which are set in the configuration file or as command line options.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2020/12/backutil-2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As Backutil automatically manages your backup files, it can be configured to run automatically at the desired interval using the Windows Task Scheduler. Backutil’s features can be used to generate rolling full or incremental backups as required by your backup objectives and disk size.&lt;/p&gt;

&lt;h4 id=&quot;testing-and-limitations&quot;&gt;Testing and limitations&lt;/h4&gt;

&lt;p&gt;Aside from all the testing that comes naturally during the development process, I have been using Backutil to back up my personal files since the start of 2021 using a Windows scheduled task to run the utility on a weekly basis. My configuration performs incremental backups on a five-file rotation and so far has worked without a hitch, to a level where I occasionally even forgot it was running.&lt;/p&gt;

&lt;h3 id=&quot;configuration&quot;&gt;Configuration&lt;/h3&gt;

&lt;p&gt;Backutil can be configured via three main means: a configuration file, a file containing a list of directories to be backed up, and a series of command line options that override other settings. If a configuration file and backup list file are present, Backutil can be run using the following simple command.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;backutil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exe&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In terms of an installation directory, I put the executable and configuration files in &lt;code&gt;C:\backutil\bin\&lt;/code&gt; and use &lt;code&gt;C:\backutil\&lt;/code&gt; as the staging folder for the temporary files and records. However, you can put these files wherever you like as long as your settings are configured accordingly.&lt;/p&gt;

&lt;h4 id=&quot;configuration-file&quot;&gt;Configuration file&lt;/h4&gt;

&lt;p&gt;Backutil automatically loads settings from a file named &lt;code&gt;config.ini&lt;/code&gt;, including the location of the list of directories to back up, folders for backups and temporary files, and incremental backup and rotation options. The configuration file should be located in the same folder as &lt;code&gt;backutil.exe&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The contents of an example configuration file are shown below.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LOCAL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;computer_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pc&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;backup_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;txt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;staging_folder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;backutil&lt;/span&gt;\
&lt;span class=&quot;n&quot;&gt;archive_pass&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;supersecretpassword&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;incremental&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rotation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;retained&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;max_threads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SERVER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;server_directory&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;backups&lt;/span&gt;\&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The table below sets out what each option in the &lt;code&gt;config.ini&lt;/code&gt; configuration file does. Note that all directories supplied via the configuration file must include the trailing backslash.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Section&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Key&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Purpose&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCAL&lt;/td&gt;
      &lt;td&gt;computer_name&lt;/td&gt;
      &lt;td&gt;Sets backup folder/record name&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCAL&lt;/td&gt;
      &lt;td&gt;backup_list&lt;/td&gt;
      &lt;td&gt;Sets the backup list filename&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCAL&lt;/td&gt;
      &lt;td&gt;staging_folder&lt;/td&gt;
      &lt;td&gt;Sets folder for temporary file storage&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCAL&lt;/td&gt;
      &lt;td&gt;archive_pass&lt;/td&gt;
      &lt;td&gt;Sets 7-Zip backup file password&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCAL&lt;/td&gt;
      &lt;td&gt;incremental&lt;/td&gt;
      &lt;td&gt;Turns incremental backups on/off (True/False)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCAL&lt;/td&gt;
      &lt;td&gt;rotation&lt;/td&gt;
      &lt;td&gt;Turns backup rotation on/off (True/False)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCAL&lt;/td&gt;
      &lt;td&gt;retained&lt;/td&gt;
      &lt;td&gt;Sets number of backups to retain if rotation is on&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCAL&lt;/td&gt;
      &lt;td&gt;max_threads&lt;/td&gt;
      &lt;td&gt;Sets maximum number of threads for multiprocessing&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SERVER&lt;/td&gt;
      &lt;td&gt;server_directory&lt;/td&gt;
      &lt;td&gt;Sets folder for backup storage&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;backup-list-file&quot;&gt;Backup list file&lt;/h4&gt;

&lt;p&gt;The backup list file is a text file containing a list of directories. When Backutil is run, it will automatically generate a list of files to back up by scanning the contents of these directories and all subdirectories. The format of the backup list file should look something like the example below.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Desktop&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Downloads&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Music&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iTunes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iTunes&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Media&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Music&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pictures&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Matt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Videos&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;command-line-options&quot;&gt;Command line options&lt;/h4&gt;

&lt;p&gt;Backutil also supports several options if you wish to set certain configuration parameters manually from the Command Prompt or PowerShell. Note that any parameters set via command line options will override the respective parameters in the &lt;code&gt;config.ini&lt;/code&gt; configuration file.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Short&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Long&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Purpose&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-h&lt;/td&gt;
      &lt;td&gt;--help&lt;/td&gt;
      &lt;td&gt;Displays the help file&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-n &amp;lt;name&amp;gt;&lt;/td&gt;
      &lt;td&gt;--name &amp;lt;name&amp;gt;&lt;/td&gt;
      &lt;td&gt;Manually sets the backup folder/record name&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-l &amp;lt;file&amp;gt;&lt;/td&gt;
      &lt;td&gt;--list &amp;lt;file&amp;gt;&lt;/td&gt;
      &lt;td&gt;Manually sets the backup list file&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-i&lt;/td&gt;
      &lt;td&gt;--incremental&lt;/td&gt;
      &lt;td&gt;Manually turns on incremental backups&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-r &amp;lt;no&amp;gt;&lt;/td&gt;
      &lt;td&gt;--rotate &amp;lt;no&amp;gt;&lt;/td&gt;
      &lt;td&gt;Manually turns on backup rotation and sets number of backups&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-t &amp;lt;no&amp;gt;&lt;/td&gt;
      &lt;td&gt;--threads &amp;lt;no&amp;gt;&lt;/td&gt;
      &lt;td&gt;Manually sets max threads for multiprocessing&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The following command shows an example of how the command line options may be used.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;backutil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exe&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;locations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;txt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Running Backutil with the options above will save backup files to a folder called &lt;code&gt;matts-pc&lt;/code&gt; (note that this folder name is also how previous backups are tracked). The list of directories to back up files from will be retrieved from &lt;code&gt;locations.txt&lt;/code&gt;. Backups will be incremental (only changed files will be backed up each time Backutil runs) and five previous backups will be retained. A maximum of six threads will be used for hash generation and file copy operations.&lt;/p&gt;

&lt;h3 id=&quot;download&quot;&gt;Download&lt;/h3&gt;

&lt;p&gt;Use the link below to download Backutil. You’re free to run it for personal use - just please let me know if you encounter any bugs so I can work on fixing them in future realeases!&lt;/p&gt;

&lt;p style=&quot;float: left; width: 90%; padding: 5px; padding-left: 15px; padding-right: 5%; margin: 30px 0 10px 0; font-style: italic; border-left: 5px solid red; background: #ff9999; color: #333;&quot;&gt;Backutil is a learning/hobby project and some aspects of its code may not follow best practices. While you're welcome to use it, you do so at your own risk. Make sure you take a manual backup of your files before trying it out, and don't go relying on it to back up your production servers.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/mattcasmith/backutil/raw/main/dist/backutil_v0.70.zip&quot;&gt;&lt;img src=&quot;/assets/images/download.png&quot; style=&quot;width: 50px&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/mattcasmith/backutil/raw/main/dist/backutil_v0.70.zip&quot;&gt;Download Backutil v0.70&lt;/a&gt;&lt;br /&gt;7.2MB, ZIP&lt;/td&gt;
      &lt;td&gt;The downloadable archive contains &lt;code&gt;backutil.exe&lt;/code&gt; and an example &lt;code&gt;config.ini&lt;/code&gt; file. Interested in the source code? The full Python script is available in &lt;a target=&quot;_blank&quot; href=&quot;https://github.com/mattcasmith/backutil&quot;&gt;the Backutil GitHub repository&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;changelog&quot;&gt;Changelog&lt;/h3&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Date&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Version&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Changes&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;03/05/2021&lt;/td&gt;
      &lt;td&gt;v0.70&lt;/td&gt;
      &lt;td&gt;Implemented multiprocessing and other improvements:&lt;br /&gt;- Faster hash generation and file copying with multiprocessing&lt;br /&gt;- Improved terminal output and logging&lt;br /&gt;- General code improvements/tidying&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;26/03/2021&lt;/td&gt;
      &lt;td&gt;v0.61&lt;/td&gt;
      &lt;td&gt;Implemented SQLite and other speed improvements:&lt;br /&gt;- All data processed using SQLite&lt;br /&gt;- Hashes generated using bigger file chunks&lt;br /&gt;- File size cut by 80 per cent due to Pandas removal&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;19/02/2021&lt;/td&gt;
      &lt;td&gt;v0.52&lt;/td&gt;
      &lt;td&gt;Small bug fixes and improvements from v0.51:&lt;br /&gt;- 7-Zip file now generated directly in destination folder&lt;br /&gt;- Hash file now only generated after successful backup&lt;br /&gt;- Blank line at end of backup list file no longer required&lt;br /&gt;- Help page consistent with online documentation&lt;br /&gt;- Fixed –help and –incremental arguments&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;future-development&quot;&gt;Future development&lt;/h3&gt;

&lt;p&gt;My determination to build a minimum viable product before the end of 2020 means that I have a backlog of bug fixes and new features to add during 2021. These include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Remote backups&lt;/strong&gt; - You’ll notice that some parts of Backutil use terminology associated with remote backups (for example, the Server section in the configuration file). This is because Backutil could originally be configured to use WinSCP to send backup files to a remote server. This has been removed for the initial release, but I hope to reinstate it in a future version.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Graphical user interface (GUI)&lt;/strong&gt; - I’ve played around with Python GUIs a couple of times before, but have never had a script worth implementing one for. Depending on time limitations, I might develop a GUI for Backutil to increase ease of use for less experienced users.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Inconsistencies and bug fixes&lt;/strong&gt; - The more time you spend with a piece of code, the more flaws you find in it. I’m sure I’ll spot plenty to fix along the way.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Have you got more ideas for new Backutil features? Or have you found bugs that I haven’t? Please &lt;a href=&quot;mailto:mattcasmith@protonmail.com&quot;&gt;send me an email&lt;/a&gt; to let me know so I can add them to the development backlog.&lt;/p&gt;

&lt;p&gt;If you’re interested in the project, check back regularly for new releases. I’ll also announce any updates on &lt;a target=&quot;_blank&quot; href=&quot;https://twitter.com/mattcasmith&quot;&gt;my Twitter account&lt;/a&gt;, and may add some form of banner to &lt;a href=&quot;https://mattcasmith.net&quot;&gt;my site’s homepage&lt;/a&gt;.&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">Back in the spring, I decided that 2020 would be the year I would finally see a coding project through to completion. A recent work project shone a light on backup and recovery, and I realised that I should probably be a bit more consistent with my own backups from my personal PC. Wanting to avoid paying another annual subscription, I decided to write a script myself. Thus Backutil was born - and the project only grew from there as I added more features along the way. I’m still not quite at the point when I’m ready to release a v1.0, but I told myself a few months ago that I wanted to put together a minimum viable product by the new year - so here it is! It has a few bugs and is missing a couple of features, but Backutil is now a functioning Python-based utility for backing up files on Windows systems, complete with options for incremental backups and backup rotation. Contents 1. Introduction           a. Testing and limitations 2. Configuration           a. Configuration file           b. Backup list file           a. Command line options 3. Download 4. Changelog 5. Future development Introduction Backutil is a simple, Python-based utility for backing up files from Windows systems to compressed, password-protected local archives. It has features for performing incremental backups and automatically rotating backup files. This is achieved using robocopy and 7-Zip, which must be installed. Backutil is a learning/hobby project and some aspects of its code may not follow best practices. While you're welcome to use it, you do so at your own risk. Make sure you take a manual backup of your files before trying it out, and don't go relying on it to back up your production servers. To back up your files, simply ensure you have configured Backutil (see the sections below) and run backutil.exe from the Command Prompt or PowerShell. The utility will report on its progress until the backup is successfully completed. More detail can also be found in backutil_log.csv. When the utility is finished, you should find your complete backup files in your designated backup folder. The number and size of these backup files can be configured using the incremental backup and rotation settings, which are set in the configuration file or as command line options. As Backutil automatically manages your backup files, it can be configured to run automatically at the desired interval using the Windows Task Scheduler. Backutil’s features can be used to generate rolling full or incremental backups as required by your backup objectives and disk size. Testing and limitations Aside from all the testing that comes naturally during the development process, I have been using Backutil to back up my personal files since the start of 2021 using a Windows scheduled task to run the utility on a weekly basis. My configuration performs incremental backups on a five-file rotation and so far has worked without a hitch, to a level where I occasionally even forgot it was running. Configuration Backutil can be configured via three main means: a configuration file, a file containing a list of directories to be backed up, and a series of command line options that override other settings. If a configuration file and backup list file are present, Backutil can be run using the following simple command. .\backutil.exe In terms of an installation directory, I put the executable and configuration files in C:\backutil\bin\ and use C:\backutil\ as the staging folder for the temporary files and records. However, you can put these files wherever you like as long as your settings are configured accordingly. Configuration file Backutil automatically loads settings from a file named config.ini, including the location of the list of directories to back up, folders for backups and temporary files, and incremental backup and rotation options. The configuration file should be located in the same folder as backutil.exe. The contents of an example configuration file are shown below. [LOCAL] computer_name = matts-pc backup_list = backup-list.txt staging_folder = C:\backutil\ archive_pass = supersecretpassword incremental = True rotation = True retained = 5 max_threads = 6 [SERVER] server_directory = D:\backups\ The table below sets out what each option in the config.ini configuration file does. Note that all directories supplied via the configuration file must include the trailing backslash. Section Key Purpose LOCAL computer_name Sets backup folder/record name LOCAL backup_list Sets the backup list filename LOCAL staging_folder Sets folder for temporary file storage LOCAL archive_pass Sets 7-Zip backup file password LOCAL incremental Turns incremental backups on/off (True/False) LOCAL rotation Turns backup rotation on/off (True/False) LOCAL retained Sets number of backups to retain if rotation is on LOCAL max_threads Sets maximum number of threads for multiprocessing SERVER server_directory Sets folder for backup storage Backup list file The backup list file is a text file containing a list of directories. When Backutil is run, it will automatically generate a list of files to back up by scanning the contents of these directories and all subdirectories. The format of the backup list file should look something like the example below. C:/Users/Matt/Desktop C:/Users/Matt/Downloads C:/Users/Matt/Music/iTunes/iTunes Media/Music C:/Users/Matt/Pictures C:/Users/Matt/Videos Command line options Backutil also supports several options if you wish to set certain configuration parameters manually from the Command Prompt or PowerShell. Note that any parameters set via command line options will override the respective parameters in the config.ini configuration file. Short Long Purpose -h --help Displays the help file -n &amp;lt;name&amp;gt; --name &amp;lt;name&amp;gt; Manually sets the backup folder/record name -l &amp;lt;file&amp;gt; --list &amp;lt;file&amp;gt; Manually sets the backup list file -i --incremental Manually turns on incremental backups -r &amp;lt;no&amp;gt; --rotate &amp;lt;no&amp;gt; Manually turns on backup rotation and sets number of backups -t &amp;lt;no&amp;gt; --threads &amp;lt;no&amp;gt; Manually sets max threads for multiprocessing The following command shows an example of how the command line options may be used. .\backutil.exe -n matts-pc -l locations.txt -i -r 5 -t 6 Running Backutil with the options above will save backup files to a folder called matts-pc (note that this folder name is also how previous backups are tracked). The list of directories to back up files from will be retrieved from locations.txt. Backups will be incremental (only changed files will be backed up each time Backutil runs) and five previous backups will be retained. A maximum of six threads will be used for hash generation and file copy operations. Download Use the link below to download Backutil. You’re free to run it for personal use - just please let me know if you encounter any bugs so I can work on fixing them in future realeases! Backutil is a learning/hobby project and some aspects of its code may not follow best practices. While you're welcome to use it, you do so at your own risk. Make sure you take a manual backup of your files before trying it out, and don't go relying on it to back up your production servers. Download Backutil v0.707.2MB, ZIP The downloadable archive contains backutil.exe and an example config.ini file. Interested in the source code? The full Python script is available in the Backutil GitHub repository. Changelog Date Version Changes 03/05/2021 v0.70 Implemented multiprocessing and other improvements:- Faster hash generation and file copying with multiprocessing- Improved terminal output and logging- General code improvements/tidying 26/03/2021 v0.61 Implemented SQLite and other speed improvements:- All data processed using SQLite- Hashes generated using bigger file chunks- File size cut by 80 per cent due to Pandas removal 19/02/2021 v0.52 Small bug fixes and improvements from v0.51:- 7-Zip file now generated directly in destination folder- Hash file now only generated after successful backup- Blank line at end of backup list file no longer required- Help page consistent with online documentation- Fixed –help and –incremental arguments Future development My determination to build a minimum viable product before the end of 2020 means that I have a backlog of bug fixes and new features to add during 2021. These include: Remote backups - You’ll notice that some parts of Backutil use terminology associated with remote backups (for example, the Server section in the configuration file). This is because Backutil could originally be configured to use WinSCP to send backup files to a remote server. This has been removed for the initial release, but I hope to reinstate it in a future version. Graphical user interface (GUI) - I’ve played around with Python GUIs a couple of times before, but have never had a script worth implementing one for. Depending on time limitations, I might develop a GUI for Backutil to increase ease of use for less experienced users. Inconsistencies and bug fixes - The more time you spend with a piece of code, the more flaws you find in it. I’m sure I’ll spot plenty to fix along the way. Have you got more ideas for new Backutil features? Or have you found bugs that I haven’t? Please send me an email to let me know so I can add them to the development backlog. If you’re interested in the project, check back regularly for new releases. I’ll also announce any updates on my Twitter account, and may add some form of banner to my site’s homepage.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mattcasmith.net/wp-content/uploads/2021/05/backutil_v0_70.png" /><media:content medium="image" url="https://mattcasmith.net/wp-content/uploads/2021/05/backutil_v0_70.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The best cyber security and technology books I read during 2020</title><link href="https://mattcasmith.net/2020/12/22/best-cyber-security-tech-books-2020" rel="alternate" type="text/html" title="The best cyber security and technology books I read during 2020" /><published>2020-12-22T00:00:00+00:00</published><updated>2020-12-22T00:00:00+00:00</updated><id>https://mattcasmith.net/2020/12/22/best-cyber-security-tech-books-2020</id><content type="html" xml:base="https://mattcasmith.net/2020/12/22/best-cyber-security-tech-books-2020">&lt;p&gt;One of the few upsides of the whole 2020 situation is that I’ve had a lot more time to read. Periods that I would usually have spent commuting, out with friends, or cramming in chores between getting home and going to bed became downtime that I could devote to good books. It was a small silver lining to a year that became something of an endurance test in staying at home and finding ways to amuse myself.&lt;/p&gt;

&lt;p&gt;I read more books in 2020 than I have since I was at college or university, so naturally, some of them were more notable, enjoyable, or informative than others. With that in mind, I thought I’d close off the year by sharing some of my favourite cyber security and tech titles. Also, it doesn’t look like my schedule will change any time soon, so if you have any suggestions for my 2021 reading list, please let me know!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2020/12/cybersecuritytechbooks2020.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;ghost-in-the-wires&quot;&gt;Ghost in the Wires&lt;/h3&gt;

&lt;p&gt;I’ll start with a book that should require little introduction for many of you, but had somehow passed me by until now. &lt;em&gt;Ghost in the Wires&lt;/em&gt; is the memoir of Kevin Mitnick, possibly the most famous hacker of all time. It tells the story of his time hacking various people and corporations for the sheer thrill of it and evading the American authorities attempting to track him down. This had been on my list for years, and I narrowly missed out on picking up a signed copy &lt;a href=&quot;https://mattcasmith.net/2019/08/26/im-back-def-con-inspired-hacking/&quot;&gt;at Black Hat last year&lt;/a&gt; (I actually passed Mitnick in a corridor shortly afterwards), but I finally got my hands on a copy in the summer.&lt;/p&gt;

&lt;p&gt;You’re unlikely to learn much from the book in a technical sense, partly because that’s not the point and partly because Mitnick’s tale takes place in the late 1980s and early 1990s, so many of the technologies and attacks mentioned are outdated. But &lt;em&gt;Ghost in the Wires&lt;/em&gt; is a real page-turner akin to many a fictional action thriller. It is rightly held as a classic of the genre, and should serve as a powerful reminder of the damage a skilled social engineer can cause to an unprepared organisation.&lt;/p&gt;

&lt;h3 id=&quot;chaos-monkeys&quot;&gt;Chaos Monkeys&lt;/h3&gt;

&lt;p&gt;Antonio Garcia Martinez’s humourous and often shocking account of his time on the Silicon Valley start-up scene is far less cyber-focused, but no less thrilling. It would be easy to compare this book to the HBO sitcom &lt;em&gt;Silicon Valley&lt;/em&gt;, but the difference is that everything in &lt;em&gt;Chaos Monkeys&lt;/em&gt; is true. It also includes rare insights into personal dealings with some of the most famous faces in tech.&lt;/p&gt;

&lt;p&gt;Martinez covers his journey from Goldman Sachs to his own start-up and eventually to Facebook, with many intriguing details along the way. Despite his stories of some of the more scandalous behaviour in the California bubble, it’s nearly impossible to read this without becoming inspired by the big dreams and hard work of tech founders, and &lt;a href=&quot;https://mattcasmith.net/2020/06/30/new-website-new-philosophy/&quot;&gt;it was even the indirect inspiration for the latest iteration of this blog&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;countdown-to-zero-day&quot;&gt;Countdown to Zero Day&lt;/h3&gt;

&lt;p&gt;As a former cyber security journalist, I know just how hard it is to combine ground-level technical detail, organisational and industry fallout, and developments in the global geopolitical landscape into a compelling story. That’s why what Kim Zetter has done here is an even bigger triumph. She tells the story of Stuxnet in a way that will satisfy industry veterans and casual observers alike, framing the campaign with detail that provides valuable context for subsequent events such as the NotPetya incident.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Countdown to Zero Day&lt;/em&gt; does a fantastic job of providing an account of the Stuxnet campaign at all levels, from the corridors of the US government and Department of Defense to the Iranian nuclear facilities it targeted and the researchers at companies like Symantec racing to decipher how the malware worked. While it doesn’t touch the low-level technical details (understandably - that would fill a book all to itself), Zetter’s account breaks down the core functionality of Stuxnet - much of which also applies more broadly to other malware variants - in a way that both techies and non-techies will understand.&lt;/p&gt;

&lt;h3 id=&quot;code-the-hidden-language-of-computer-hardware-and-software&quot;&gt;Code: The Hidden Language of Computer Hardware and Software&lt;/h3&gt;

&lt;p&gt;Have you ever wondered how computers actually use all those ones and zeroes to allow us to send emails, edit Word documents, and write new programs in C++? Charles Petzold’s book covers more ground than just about any I’ve ever read, beginning with relatively simple systems like Morse code and, over the course of fewer than 400 pages, building a theoretical telegraph system, processor, RAM, and eventually a computer akin to those we use every day. It really is quite staggering to think about.&lt;/p&gt;

&lt;p&gt;While the detail presented in &lt;em&gt;Code&lt;/em&gt; isn’t something most of us will consider in the course of our daily lives, it does help to dispel the “magic box” effect surrounding PCs and smartphones. For my colleagues in the cyber security industry, it also provides a great overview of the relationship between binary and machine language, assembly, and high- and low-level programming languages, as well as where data is stored in CPU registers and RAM when code is run - fundamentals for malware analysts and bug hunters.&lt;/p&gt;

&lt;h3 id=&quot;practical-packet-analysis&quot;&gt;Practical Packet Analysis&lt;/h3&gt;

&lt;p&gt;No Starch Press is always a good bet for solid technology and cyber security books, and &lt;a href=&quot;https://nostarch.com/&quot; target=&quot;_blank&quot;&gt;the publisher’s website&lt;/a&gt; often serves as my starting point when I’m looking for something technical to read. I read a couple of their titles this year, but the highlight has to be &lt;em&gt;Practical Packet Analysis&lt;/em&gt;, which I worked through during some leave in the summer. At face value, the book is billed as a guide to using Wireshark to solve network problems, but it actually covers a lot more than that.&lt;/p&gt;

&lt;p&gt;For anybody new to networking, I think the opening chapters provide about the clearest explanation of the OSI model, TCP/IP, and common protocols that you’ll find anywhere. This is followed by an extensive rundown of Wireshark’s interface and features, including a guide on where and how to capture network traffic, as well as a series of scenarios where packet analysis helps to diagnose network problems, complete with downloadable PCAP files to follow along and get some practice in.&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">One of the few upsides of the whole 2020 situation is that I’ve had a lot more time to read. Periods that I would usually have spent commuting, out with friends, or cramming in chores between getting home and going to bed became downtime that I could devote to good books. It was a small silver lining to a year that became something of an endurance test in staying at home and finding ways to amuse myself. I read more books in 2020 than I have since I was at college or university, so naturally, some of them were more notable, enjoyable, or informative than others. With that in mind, I thought I’d close off the year by sharing some of my favourite cyber security and tech titles. Also, it doesn’t look like my schedule will change any time soon, so if you have any suggestions for my 2021 reading list, please let me know! Ghost in the Wires I’ll start with a book that should require little introduction for many of you, but had somehow passed me by until now. Ghost in the Wires is the memoir of Kevin Mitnick, possibly the most famous hacker of all time. It tells the story of his time hacking various people and corporations for the sheer thrill of it and evading the American authorities attempting to track him down. This had been on my list for years, and I narrowly missed out on picking up a signed copy at Black Hat last year (I actually passed Mitnick in a corridor shortly afterwards), but I finally got my hands on a copy in the summer. You’re unlikely to learn much from the book in a technical sense, partly because that’s not the point and partly because Mitnick’s tale takes place in the late 1980s and early 1990s, so many of the technologies and attacks mentioned are outdated. But Ghost in the Wires is a real page-turner akin to many a fictional action thriller. It is rightly held as a classic of the genre, and should serve as a powerful reminder of the damage a skilled social engineer can cause to an unprepared organisation. Chaos Monkeys Antonio Garcia Martinez’s humourous and often shocking account of his time on the Silicon Valley start-up scene is far less cyber-focused, but no less thrilling. It would be easy to compare this book to the HBO sitcom Silicon Valley, but the difference is that everything in Chaos Monkeys is true. It also includes rare insights into personal dealings with some of the most famous faces in tech. Martinez covers his journey from Goldman Sachs to his own start-up and eventually to Facebook, with many intriguing details along the way. Despite his stories of some of the more scandalous behaviour in the California bubble, it’s nearly impossible to read this without becoming inspired by the big dreams and hard work of tech founders, and it was even the indirect inspiration for the latest iteration of this blog. Countdown to Zero Day As a former cyber security journalist, I know just how hard it is to combine ground-level technical detail, organisational and industry fallout, and developments in the global geopolitical landscape into a compelling story. That’s why what Kim Zetter has done here is an even bigger triumph. She tells the story of Stuxnet in a way that will satisfy industry veterans and casual observers alike, framing the campaign with detail that provides valuable context for subsequent events such as the NotPetya incident. Countdown to Zero Day does a fantastic job of providing an account of the Stuxnet campaign at all levels, from the corridors of the US government and Department of Defense to the Iranian nuclear facilities it targeted and the researchers at companies like Symantec racing to decipher how the malware worked. While it doesn’t touch the low-level technical details (understandably - that would fill a book all to itself), Zetter’s account breaks down the core functionality of Stuxnet - much of which also applies more broadly to other malware variants - in a way that both techies and non-techies will understand. Code: The Hidden Language of Computer Hardware and Software Have you ever wondered how computers actually use all those ones and zeroes to allow us to send emails, edit Word documents, and write new programs in C++? Charles Petzold’s book covers more ground than just about any I’ve ever read, beginning with relatively simple systems like Morse code and, over the course of fewer than 400 pages, building a theoretical telegraph system, processor, RAM, and eventually a computer akin to those we use every day. It really is quite staggering to think about. While the detail presented in Code isn’t something most of us will consider in the course of our daily lives, it does help to dispel the “magic box” effect surrounding PCs and smartphones. For my colleagues in the cyber security industry, it also provides a great overview of the relationship between binary and machine language, assembly, and high- and low-level programming languages, as well as where data is stored in CPU registers and RAM when code is run - fundamentals for malware analysts and bug hunters. Practical Packet Analysis No Starch Press is always a good bet for solid technology and cyber security books, and the publisher’s website often serves as my starting point when I’m looking for something technical to read. I read a couple of their titles this year, but the highlight has to be Practical Packet Analysis, which I worked through during some leave in the summer. At face value, the book is billed as a guide to using Wireshark to solve network problems, but it actually covers a lot more than that. For anybody new to networking, I think the opening chapters provide about the clearest explanation of the OSI model, TCP/IP, and common protocols that you’ll find anywhere. This is followed by an extensive rundown of Wireshark’s interface and features, including a guide on where and how to capture network traffic, as well as a series of scenarios where packet analysis helps to diagnose network problems, complete with downloadable PCAP files to follow along and get some practice in.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mattcasmith.net/wp-content/uploads/2020/12/cybersecuritytechbooks2020.jpg" /><media:content medium="image" url="https://mattcasmith.net/wp-content/uploads/2020/12/cybersecuritytechbooks2020.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AWS: Deploying and connecting to a SQL database in the cloud</title><link href="https://mattcasmith.net/2020/11/15/aws-deploy-connect-sql-database-cloud" rel="alternate" type="text/html" title="AWS&amp;#58; Deploying and connecting to a SQL database in the cloud" /><published>2020-11-20T00:00:00+00:00</published><updated>2020-11-20T00:00:00+00:00</updated><id>https://mattcasmith.net/2020/11/15/aws-deploy-connecting-sql-database-cloud</id><content type="html" xml:base="https://mattcasmith.net/2020/11/15/aws-deploy-connect-sql-database-cloud">&lt;p&gt;My first Amazon Web Services (AWS) basics post covered the process of &lt;a href=&quot;https://mattcasmith.net/2020/11/15/aws-deploying-virtual-network-server-cloud&quot;&gt;setting up a Virtual Private Cloud (VPC) and a Windows Server 2019 EC2 instance&lt;/a&gt;. This time we’re going to build on this simple setup by deploying a Amazon Aurora SQL database and ensuring we can access it from our server.&lt;/p&gt;

&lt;h3 id=&quot;aws-basics-series&quot;&gt;AWS basics series&lt;/h3&gt;

&lt;p&gt;1. &lt;a href=&quot;https://mattcasmith.net/2020/11/15/aws-deploying-virtual-network-server-cloud&quot;&gt;Deploying a virtual network and server&lt;/a&gt;&lt;br /&gt;
2. Deploying and connecting to a SQL database&lt;/p&gt;

&lt;h3 id=&quot;databases-in-aws&quot;&gt;Databases in AWS&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/products/databases/&quot; target=&quot;_blank&quot;&gt;Databases in AWS&lt;/a&gt; generally come in three different flavours, which are all designed for different use cases, data volumes, and availability requirements. These are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Relational Database Service (RDS)&lt;/strong&gt; - Relational databases are typical, structured, table-based databases. AWS gives you the option to run MySQL, Oracle SQL, and Microsoft SQL Server among other established names, but also offers its own database engine called &lt;a href=&quot;https://aws.amazon.com/rds/aurora/&quot; target=&quot;_blank&quot;&gt;Amazon Aurora&lt;/a&gt;, which is optimised with a few extra features designed for the cloud.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;DynamoDB&lt;/strong&gt; - DynamoDB is a NoSQL database consisting of key-value pairs for less structured data. This can be a good option if the speed of queries is the most important factor.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Redshift&lt;/strong&gt; - Redshift is the AWS service for data warehouses. This is the best solution if you have petabytes of data to store, and is optimised for handling these large datasets.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/products/databases/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;\wp-content\uploads\2020\11\aws-sql-dbs.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To keep things simple, we’ll assume our data will fit nicely in an RDS database that needs to be accessed from the Windows Server 2019 EC2 instance we deployed in the last post. Deploying an RDS database will take three steps: creating the database, ensuring our server has the correct access permissions, and connecting to the database to use it. Let’s get started with the setup.&lt;/p&gt;

&lt;h3 id=&quot;deploying-an-aws-rds-database&quot;&gt;Deploying an AWS RDS database&lt;/h3&gt;

&lt;p&gt;Navigate to the RDS dashboard and look for the Create Database section, where there is also a Create Database button. Clicking this will take you to a form where you can choose the configuration of your new database, from the engine to which VPC it sits in. Click the Standard Create option to continue.&lt;/p&gt;

&lt;p&gt;Now let’s select an engine for our SQL database. You may instinctively reach for the familiar names like MySQL and Microsoft SQL Server, but I’m going to use Amazon Aurora (selecting the edition with MySQL compatibility). This is Amazon’s own database engine, which is optimised for use in the AWS cloud and can support higher throughput, auto-scaling, and replication across availability zones.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\wp-content\uploads\2020\11\aws-sql-engine.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ll choose Provisioned capacity, which means we manage the server instance the database sits on, and select the Dev/Test template to avoid any extra charges for the high availability and resilience aspects of the Production template. Next we can create a username and password for the master user - make a note of these details as we’ll need them to access the database later on.&lt;/p&gt;

&lt;p&gt;Scroll down and you have the opportunity to choose the VPC (basically the network) your database should sit in. We’ll choose the &lt;code&gt;server-deployment&lt;/code&gt; VPC we configured during the first blog post. This will make it easier to reach the database from the server and will do for our simple example, but it is not recommended to deploy production databases in VPCs with internet access - it would be best to tuck them away in their own VPC and configure rules for access from another.&lt;/p&gt;

&lt;p&gt;Let’s also create a new Security Group to allow access to the database. We’ll call it &lt;code&gt;database-access&lt;/code&gt; and configure it a bit later. Click Create Database and you’ll be taken back to the Databases Dashboard, where you’ll be able to see your new Aurora MySQL database is now being created.&lt;/p&gt;

&lt;h3 id=&quot;granting-access-to-the-ec2-server-instance&quot;&gt;Granting access to the EC2 server instance&lt;/h3&gt;

&lt;p&gt;But if we were to try to connect from our EC2 server to the database now, our connection would fail. Why? Because we haven’t configured the Security Group to allow the connection. We can rectify this by navigating to the VPC Dashboard and clicking on Security Groups. From there we can add a rule that allows access on port 3306 from the subnet our server sits in (&lt;code&gt;10.0.0.0/24&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\wp-content\uploads\2020\11\aws-sql-security-group.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That’s it - we’ve deployed a SQL database and given our server access to it. The next step is to log in and manipulate some data, just to prove that the database and connection both work.&lt;/p&gt;

&lt;h3 id=&quot;connecting-to-the-aws-rds-database&quot;&gt;Connecting to the AWS RDS database&lt;/h3&gt;

&lt;p&gt;If we RDP to our server, we can now connect to our SQL database - but first, you’ll need to download a client. I used Oracle’s &lt;a href=&quot;https://dev.mysql.com/downloads/&quot; target=&quot;_blank&quot;&gt;MySQL Shell&lt;/a&gt;, with which you can establish a connection with this command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;mysqlsh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exe&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endpoint&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3306&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;After entering our password, we are connected to the database and can interact with its contents using standard SQL commands to add, remove, merge, and filter data. Looks like it’s good to go! If you need any tips on how to manipulate data with SQL, see my previous posts on &lt;a href=&quot;https://mattcasmith.net/2018/10/12/basic-sql-queries-select-from-where-operators/&quot;&gt;basic SQL commands&lt;/a&gt;, &lt;a href=&quot;https://mattcasmith.net/2018/12/21/sql-joins-inner-left-right-outer/&quot;&gt;SQL JOINs&lt;/a&gt;, and &lt;a href=&quot;https://mattcasmith.net/2019/02/01/sql-alter-table-add-modify-drop-columns/&quot;&gt;SQL TABLE commands&lt;/a&gt;, which should help you to get started.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\wp-content\uploads\2020\11\aws-sql-mysql.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again, make sure you go back to the EC2 and RDS Dashboards when you’re finished and stop your EC2 instance and RDS database. This will help you to avoid any unexpected AWS charges.&lt;/p&gt;

&lt;p&gt;So now we have &lt;a href=&quot;https://mattcasmith.net/2020/11/15/aws-deploying-virtual-network-server-cloud&quot;&gt;a VPC and a Windows Server 2019 EC2 instance&lt;/a&gt;, and have deployed a simple RDS Aurora database that we can access from it. In my next and final AWS basics post, I’ll run through how to set up an S3 bucket to store some files in the cloud (and avoid sharing them with everyone).&lt;/p&gt;</content><author><name>mattcasmith</name></author><summary type="html">My first Amazon Web Services (AWS) basics post covered the process of setting up a Virtual Private Cloud (VPC) and a Windows Server 2019 EC2 instance. This time we’re going to build on this simple setup by deploying a Amazon Aurora SQL database and ensuring we can access it from our server. AWS basics series 1. Deploying a virtual network and server 2. Deploying and connecting to a SQL database Databases in AWS Databases in AWS generally come in three different flavours, which are all designed for different use cases, data volumes, and availability requirements. These are: Relational Database Service (RDS) - Relational databases are typical, structured, table-based databases. AWS gives you the option to run MySQL, Oracle SQL, and Microsoft SQL Server among other established names, but also offers its own database engine called Amazon Aurora, which is optimised with a few extra features designed for the cloud. DynamoDB - DynamoDB is a NoSQL database consisting of key-value pairs for less structured data. This can be a good option if the speed of queries is the most important factor. Redshift - Redshift is the AWS service for data warehouses. This is the best solution if you have petabytes of data to store, and is optimised for handling these large datasets. To keep things simple, we’ll assume our data will fit nicely in an RDS database that needs to be accessed from the Windows Server 2019 EC2 instance we deployed in the last post. Deploying an RDS database will take three steps: creating the database, ensuring our server has the correct access permissions, and connecting to the database to use it. Let’s get started with the setup. Deploying an AWS RDS database Navigate to the RDS dashboard and look for the Create Database section, where there is also a Create Database button. Clicking this will take you to a form where you can choose the configuration of your new database, from the engine to which VPC it sits in. Click the Standard Create option to continue. Now let’s select an engine for our SQL database. You may instinctively reach for the familiar names like MySQL and Microsoft SQL Server, but I’m going to use Amazon Aurora (selecting the edition with MySQL compatibility). This is Amazon’s own database engine, which is optimised for use in the AWS cloud and can support higher throughput, auto-scaling, and replication across availability zones. We’ll choose Provisioned capacity, which means we manage the server instance the database sits on, and select the Dev/Test template to avoid any extra charges for the high availability and resilience aspects of the Production template. Next we can create a username and password for the master user - make a note of these details as we’ll need them to access the database later on. Scroll down and you have the opportunity to choose the VPC (basically the network) your database should sit in. We’ll choose the server-deployment VPC we configured during the first blog post. This will make it easier to reach the database from the server and will do for our simple example, but it is not recommended to deploy production databases in VPCs with internet access - it would be best to tuck them away in their own VPC and configure rules for access from another. Let’s also create a new Security Group to allow access to the database. We’ll call it database-access and configure it a bit later. Click Create Database and you’ll be taken back to the Databases Dashboard, where you’ll be able to see your new Aurora MySQL database is now being created. Granting access to the EC2 server instance But if we were to try to connect from our EC2 server to the database now, our connection would fail. Why? Because we haven’t configured the Security Group to allow the connection. We can rectify this by navigating to the VPC Dashboard and clicking on Security Groups. From there we can add a rule that allows access on port 3306 from the subnet our server sits in (10.0.0.0/24). That’s it - we’ve deployed a SQL database and given our server access to it. The next step is to log in and manipulate some data, just to prove that the database and connection both work. Connecting to the AWS RDS database If we RDP to our server, we can now connect to our SQL database - but first, you’ll need to download a client. I used Oracle’s MySQL Shell, with which you can establish a connection with this command: .\mysqlsh.exe -h &amp;lt;insert database endpoint address&amp;gt; -P 3306 -u &amp;lt;username&amp;gt; -p After entering our password, we are connected to the database and can interact with its contents using standard SQL commands to add, remove, merge, and filter data. Looks like it’s good to go! If you need any tips on how to manipulate data with SQL, see my previous posts on basic SQL commands, SQL JOINs, and SQL TABLE commands, which should help you to get started. Again, make sure you go back to the EC2 and RDS Dashboards when you’re finished and stop your EC2 instance and RDS database. This will help you to avoid any unexpected AWS charges. So now we have a VPC and a Windows Server 2019 EC2 instance, and have deployed a simple RDS Aurora database that we can access from it. In my next and final AWS basics post, I’ll run through how to set up an S3 bucket to store some files in the cloud (and avoid sharing them with everyone).</summary></entry></feed>