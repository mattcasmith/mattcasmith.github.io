<!doctype html>
<head>
	<meta name="description" content="" />
	<meta name="robots" content="index, follow" />
	<meta name="googlebot" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1" />
	<meta name="bingbot" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1" />
	<link rel="canonical" href="https://mattcasmith.net/2021/05/03/multiprocessing-python" />
	<meta property="og:locale" content="en_GB" />
	<meta property="og:type" content="article" />
	<meta property="og:title" content="Backutil development&#58; Implementing multiprocessing in Python | MattCASmith" />
	<meta property="og:description" content="" />
	<meta property="og:url" content="https://mattcasmith.net/2021/05/03/multiprocessing-python" />
	<meta property="og:site_name" content="MattCASmith" />
	<meta property="article:published_time" content="2020-05-02T11:05:40+00:00" />
	<meta property="og:image" content="https://mattcasmith.net/wp-content/uploads/2021/04/multiprocessing_after.png" />
	<meta name="twitter:card" content="summary_large_image" />
	<meta name="twitter:creator" content="@mattcasmith" />
	<meta name="twitter:site" content="@mattcasmith" />
</head>
	<head>
		<style>
			html {
				display: none;
			}
		</style>
		<title>Backutil development&#58; Implementing multiprocessing in Python | MattCASmith</title>
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="shortcut icon" href="/assets/images/favicon.png">
		<link rel="stylesheet" href="/assets/css/styles.css">
		<link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
		
	</head>
	
	<body>
		<div id="site-container" class="post">
		<div id="header">
			<div id="header_left">
				<a href="/"><img class="site_logo light" src="/assets/images/mcas_site_logo.png"></a>
				<a href="/"><img class="site_logo dark" src="/assets/images/mcas_site_logo_dark.png"></a>
				<a href="/"><h2>MattCASmith</h2></a>
			</div>
			<div id="header_right">
				<a target="_blank" href="https://twitter.com/mattcasmith"><img class="link_icon_twitter" src="/assets/images/twitter_icon.png"></a>
				<a target="_blank" href="mailto:mattcasmith@protonmail.com"><img class="link_icon_email light" src="/assets/images/email_icon.png"></a>
				<a target="_blank" href="mailto:mattcasmith@protonmail.com"><img class="link_icon_email dark" src="/assets/images/email_icon_dark.png"></a>
			</div>
		</div>
		
		<div class="title">
			<h1 class="entry-title">Backutil development&#58; Implementing multiprocessing in Python</h1>
		</div>
		
		<div class="meta">
			<span class="year_label">2021-05-03</span>&nbsp; 
				
				
					<a href="/category/programming.html">Programming</a>
				
			
		</div>
		
		<p>I’m still hard at work on <a href="/2021/01/01/backutil-windows-backup-utility">Backutil</a>, my simple Windows backup utility with automatic rotation features, fitting in little tweaks and improvements around my daily schedule. The latest of these - and perhaps the most impactful in terms of performance - involves the implementation of multiprocessing for several parts of the code, which I thought was significant and interesting enough to warrant a write-up.</p>

<h3 id="why-implement-multiprocessing">Why implement multiprocessing?</h3>

<p>By default, Python scripts execute in a single processor thread. This is the equivalent of a road with a single lane of traffic. This is fine for most parts of the code, but consider some of the operations involved in Backutil - specifically the section that generates hashes for all the files in the backup directories (also the bit that copies files, but we’ll stick to one example in this post to keep things simple).</p>

<p><img src="/wp-content/uploads/2021/04/multiprocessing_before.png" /></p>

<p>There’s a clear bottleneck in this scenario. That one processor core is being worked hard, tasked with generating <em>all</em> the hashes (yes, there may be other factors at play, like disk read speed, but we’ll ignore that for the purpose of this walkthrough). Even if my computer has six processor cores, we’re only using one. This inefficiency means the script takes longer to run over large batches of files.</p>

<p>But what if we could use multiple cores at the same time? This would be akin to a road with many lanes of traffic - like <a href="https://old.reddit.com/r/formula1/comments/c9v83x/baku_turn_1_on_a_non_f1_time/" target="_blank">turn one at the Baku Formula 1 street circuit on a normal day</a> (just more efficient). Let’s try breaking our lists of files into chunks - three, for the sake of argument - and handling them separately.</p>

<p><img src="/wp-content/uploads/2021/04/multiprocessing_after.png" /></p>

<p>That’s better! Now we’ve split the list into parts and assigned each to its own process, we can use more of the computer’s potential and - <em>in theory</em>, at least - complete the task in a third of the time it would have taken using our previous process. Then we recombine the results for use in the rest of the program.</p>

<h3 id="writing-a-function-to-generate-hashes">Writing a function to generate hashes</h3>

<p>To spawn separate processes to handle the hash generation, we need to define a function to do the dirty work. Each time we start an additional process, we will invoke this function and supply it with a subset of the data. For the purposes of generating file hashes, that will look like this:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">generate_hashes</span><span class="p">(</span><span class="n">procnum</span><span class="p">,</span> <span class="n">backup_files_thread</span><span class="p">,</span> <span class="n">return_dict</span><span class="p">,</span> <span class="n">version</span><span class="p">):</span>
    <span class="n">colorama</span><span class="p">.</span><span class="n">init</span><span class="p">()</span>
    <span class="n">return_dict_process</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">backup_files_thread</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">sha256_hash</span> <span class="o">=</span> <span class="n">hashlib</span><span class="p">.</span><span class="n">sha256</span><span class="p">()</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span><span class="s">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">byte_block</span> <span class="ow">in</span> <span class="nb">iter</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="mi">65535</span><span class="p">),</span><span class="s">b""</span><span class="p">):</span>
                    <span class="n">sha256_hash</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">byte_block</span><span class="p">)</span>
                <span class="n">hash_output</span> <span class="o">=</span> <span class="p">(</span><span class="n">sha256_hash</span><span class="p">.</span><span class="n">hexdigest</span><span class="p">())</span>
                <span class="n">return_dict_process</span><span class="p">[</span><span class="n">filename</span><span class="p">]</span> <span class="o">=</span> <span class="n">hash_output</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="s">"Couldn't generate hash for "</span> <span class="o">+</span> <span class="n">filename</span>
            <span class="n">log</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="s">"Warning"</span><span class="p">,</span> <span class="n">version</span><span class="p">)</span>
    <span class="n">return_dict</span><span class="p">[</span><span class="n">procnum</span><span class="p">]</span> <span class="o">=</span> <span class="n">return_dict_process</span></code></pre></figure>

<p>I won’t go into detail on the hash generation itself - that’s for a different post - but I will point out some important elements as far as multiprocessing is concerned.</p>

<p>First, take a look at the arguments that <code>generate_hashes</code> takes as input. The variable <code>procnum</code> will let us assign each subprocess an identifier, <code>backup_files_thread</code> will be used to supply the data that the subprocess will work on, and <code>version</code> is just the Backutil version number for logging purposes.</p>

<p>The special dictionary <code>return_dict</code> will combine output from our subprocesses - more on that later. All we need to note now is that the final line of our function adds function output (<code>return_dict_process</code>) as an entry in this dictionary, using the subprocess identifier (<code>procnum</code>) as a key.</p>

<h3 id="splitting-the-dataset-into-chunks">Splitting the dataset into chunks</h3>

<p>But we can’t simply tell Python, or the <code>multiprocessing</code> library, to take our list of files to generate hashes for and split it between <code>x</code> processes. That would be too easy. We need to do the hard work ourselves and split the data (a list called <code>backup_files</code>, full of file paths) into equal chunks.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">split_backup_files</span> <span class="o">=</span> <span class="p">(</span><span class="n">backup_files</span><span class="p">[</span><span class="n">i</span><span class="p">::</span><span class="n">config</span><span class="p">.</span><span class="n">max_threads</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">max_threads</span><span class="p">))</span></code></pre></figure>

<p>When this line of code has done its thing, we’ll be left with a new list called <code>split_backup_files</code>, within which will be a number of equal lists equal to the <code>config.max_threads</code> variable, which the user should configure to be equal to the number of processor cores they want Backutil to use.</p>

<h3 id="assigning-each-chunk-to-a-subprocess">Assigning each chunk to a subprocess</h3>

<p>We’re nearly ready to get our subprocesses up and running, but before we do so there’s some housekeeping to take care of to keep track of processes and their output.</p>

<p>In the first few lines of setup, <code>process_container</code> will be used to track all the processes we create, while <code>manager</code> - and specifically <code>manager.dict()</code> as <code>return_dict</code> - will provide a kind of magic dictionary that will take the output from each process (we saw the line in the function for this earlier). The <code>x</code> value will track process IDs as we create them, iterating by one each time.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">process_container</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">manager</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="p">.</span><span class="n">Manager</span><span class="p">()</span>
<span class="n">return_dict</span> <span class="o">=</span> <span class="n">manager</span><span class="p">.</span><span class="nb">dict</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">backup_files_thread</span> <span class="ow">in</span> <span class="n">split_backup_files</span><span class="p">:</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">"Process-"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">process</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="p">.</span><span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">generate_hashes</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">backup_files_thread</span><span class="p">,</span> <span class="n">return_dict</span><span class="p">,</span> <span class="n">version</span><span class="p">,))</span>
    <span class="n">process_container</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">process</span><span class="p">)</span>
    <span class="n">process</span><span class="p">.</span><span class="n">start</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">process</span> <span class="ow">in</span> <span class="n">process_container</span><span class="p">:</span>
    <span class="n">process</span><span class="p">.</span><span class="n">join</span><span class="p">()</span></code></pre></figure>

<p>Now for the real action. We iterate over <code>split_backup_files</code> (you’ll remember the number of lists inside is equal to the number of threads) and for each item we use <code>multiprocessing.Process</code> to create <code>Process X</code>. The <code>target</code> is our function, the <code>name</code> comes from the variable we just created, and we supply all our function’s arguments in <code>args</code>. The process is appended to <code>process_container</code> for safekeeping, and with <code>process.start()</code> we’re officially multiprocessing!</p>

<p>One more important action for each process in the container here is <code>process.join()</code>. This ensures that our parent process waits for each child process to finish before proceeding, ensuring we don’t start trying to use our hashes before they’ve all been generated and returned.</p>

<h3 id="checking-subprocess-exit-state">Checking subprocess exit state</h3>

<p>Time to build in a bit of resilience. The <code>generate_hashes</code> function won’t fail if a single hash generation fails, but in the unlikely event that one of our subprocesses fails, we don’t want to proceed and complete a partial backup without the user knowing. Instead, we’ll check they succeed and interrupt Backutil if not.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">process</span> <span class="ow">in</span> <span class="n">process_container</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">process</span><span class="p">.</span><span class="n">exitcode</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">log</span><span class="p">(</span><span class="s">"Hash generation thread failed."</span><span class="p">,</span> <span class="s">"Failure"</span><span class="p">,</span> <span class="n">version</span><span class="p">)</span>
        <span class="n">sys</span><span class="p">.</span><span class="nb">exit</span><span class="p">()</span></code></pre></figure>

<p>To achieve this, we iterate over the processes in <code>process_container</code> and check the value of <code>process.exitcode</code> for each. If it’s <code>0</code> (success), and if it’s not then we log the error before calling <code>sys.exit()</code> to end the program. Backutil will automatically clean up any temporary files on its way out.</p>

<h3 id="recombining-the-dataset">Recombining the dataset</h3>

<p>So we split our data, we processed it, and each subprocess has returned its output dictionary as an entry in the dictionary <code>return_dict</code> with the process number as the key. As the final step, we want to recombine the returned hashes so they can be used elsewhere in the program.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">combined_dict</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">dictionary</span> <span class="ow">in</span> <span class="n">return_dict</span><span class="p">.</span><span class="n">values</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">combined_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span></code></pre></figure>

<p>This is simple enough: We establish the dictionary <code>combined_dict</code> for our output and begin to iterate over the values in <code>return_dict</code>. Each key-value pair in each process’s returned dictionary is appended to <code>combined_dict</code>. After all the entries have been processed, we’ve recombined our data!</p>

<h3 id="multiprocessing-in-action">Multiprocessing in action</h3>

<p>One of the nice (and convenient) things about multiprocessing is that - unlike most Python structures and implementations - we can see it in action directly in the operating system, rather than using variable explorers or debug <code>print</code> statements. On Windows, we can simply check the Task Manager.</p>

<p><img src="/wp-content/uploads/2021/04/multiprocessing_taskmanager.png" /></p>

<p>In the Details tab, we can see all of our currently running processes. By sorting them by name and finding all instances of <code>backutil.exe</code>, we can see a number of processes equal to the <code>config.max_threads</code> variable we saw earlier, plus the parent process - so we know multiprocessing is working properly.</p>

<h3 id="multiprocessing-on-windows-with-pyinstaller">Multiprocessing on Windows with PyInstaller</h3>

<p>I do have one more small note if you’re building a Python program for Windows using <code>multiprocessing</code> and you’re planning on converting it to an <code>.exe</code> file using <a href="https://www.pyinstaller.org/" target="_blank">PyInstaller</a>. In this case, you could do everything described above perfectly, and when you run your executable you’ll receive an error.</p>

<p>Luckily, you need only one line of code to fix this.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span> 
    <span class="n">multiprocessing</span><span class="p">.</span><span class="n">freeze_support</span><span class="p">()</span></code></pre></figure>

<p>You must put <code>multiprocessing.freeze_support()</code> at the start of this section of your code, but if you do this and recompile your executable with PyInstaller then everything should work again.</p>

		
		<div id="cta">
	Looking for the comments? My website doesn't have a comments section because it would take a fair amount of effort to maintain and wouldn't usually present much value to readers. However, if you have thoughts to share I'd love to hear from you - feel free to send me <a href="https://twitter.com/mattcasmith" target="_blank">a tweet</a> or <a href="mailto:mattcasmith@protonmail.com">an email</a>.
</div>
		
		<div id="prev_next">
			<div class="prev">
				    
						<a href="/2021/05/01/installing-splunk-free-virtual-machine-log-analysis">&laquo;&nbsp;Installing Splunk Free in a virtual machine for log analysis</a>
					
			</div>
			<div class="next">
				    
						<a href="/2022/01/08/sans-holiday-hack-2021-yara-rules">SANS Holiday Hack Challenge 2021&#58; Yara rule analysis walkthrough&nbsp;&raquo;</a>
					
			</div>
		</div>
		
		
				<div id="footer">
			&copy; 2016-22 MattCASmith
			<span class="home-link">Personal blog - does not reflect views of employers past or present</span>
		</div>
		
		<script data-goatcounter="https://mattcasmith.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>
		
		</div>
	
	</body>
</html>